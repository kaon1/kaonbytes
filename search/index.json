[{"content":"About Not long ago, I was asked by Kris Beevers, the CEO of NetBox Labs \u0026mdash; what actionable steps I recommend for individuals who are interested in a career in network engineering. My number one answer is to lab it up. Deploying a lab environment and getting hands on to solve a real (or made up) problem is the best way to accelerate your learning.\nThere are multiple ways to create network lab environments: if you have the resources to grab real physical gear go for it. However, this has limitations, there is a cost to this, power, cabling, and space requirements. Another option is to use an application such as EVE-NG or GNS3. These platforms let you deploy virtual machine based labs. However, the images are usually memory intensive and setting up the topology is not programmatically friendly.\nLuckily for us, in the last few years more and more vendors are providing containerized network operating systems. A network OS hosted on a docker container uses significantly less cpu and memory and can be deployed using infrastructure-as-code methods (IAC)\nSRL Labs has built an open source tool called ContainerLab which can orchestrate a routing topology for these network containers. In addition to network operating systems, ContainerLab allows us to deploy and virtually wire-up arbitrary linux containers to host applications. This can allow for full end-to-end testing of the application and network stack.\nContainerLab does have somewhat of a learning curve get labs up and running. I realize that not all network engineers want to learn a new platform or how docker containers work. For these cases, I built an Ansible wrapper which lets a user deploy their own pre-canned lab environment using Ansible Tower. The hope is that this limits the barrier to entry for using ContainerLab.\nWorkflow User Selects their Desired Pre-Canned Lab Topology User Selects their desired Router Image Action: Deploy - Start Lab from Scratch Inspect - Show information about existing already deployed lab Save - logs into all routers of lab and saves config (can also be done manually on cli) Shutdown - powers off lab routers but keeps the config on server (config persists on next lab deployment) Destroy - powers off lab routers and deletes all configs on server (all saved configs lost) After Job is complete. Lab Nodes can be accessed by following the output prompts Ansible Tower Prompts: Lab to be Deployed using Arista cEOS images: Results of Ansible Playbook: Deployed Docker Images: The unique lab names (per user) and unique port-mappings per container allow for multiple engineers to deploy the same lab on the same server.\nLogging into one of the routers: How it Works The full code and other lab topologies can be found here: https://github.com/kaon1/containerlab-selfservice\nCreating the ContainerLab Topology Files We can use a jinja2 template to dynamically create unique topology files per user and per lab. For example, here we have the jinja2 template for the spine-leaf-datacenter lab:\nname: {{ lab_title }}-{{ lab_username }} prefix: \u0026#34;\u0026#34; topology: nodes: {{ lab_title }}-{{ lab_username }}-spine01: kind: \u0026#34;{{ router_container_kind }}\u0026#34; image: \u0026#34;{{ router_container_kind }}:latest\u0026#34; ports: - \u0026#34;5{{ user_map[lab_username] }}01:22\u0026#34; {{ lab_title }}-{{ lab_username }}-spine02: kind: \u0026#34;{{ router_container_kind }}\u0026#34; image: \u0026#34;{{ router_container_kind }}:latest\u0026#34; ports: - \u0026#34;5{{ user_map[lab_username] }}02:22\u0026#34; {{ lab_title }}-{{ lab_username }}-leaf01: kind: \u0026#34;{{ router_container_kind }}\u0026#34; image: \u0026#34;{{ router_container_kind }}:latest\u0026#34; ports: - \u0026#34;5{{ user_map[lab_username] }}03:22\u0026#34; {{ lab_title }}-{{ lab_username }}-leaf02: kind: \u0026#34;{{ router_container_kind }}\u0026#34; image: \u0026#34;{{ router_container_kind }}:latest\u0026#34; ports: - \u0026#34;5{{ user_map[lab_username] }}04:22\u0026#34; {{ lab_title }}-{{ lab_username }}-leaf03: kind: \u0026#34;{{ router_container_kind }}\u0026#34; image: \u0026#34;{{ router_container_kind }}:latest\u0026#34; ports: - \u0026#34;5{{ user_map[lab_username] }}05:22\u0026#34; {{ lab_title }}-{{ lab_username }}-leaf04: kind: \u0026#34;{{ router_container_kind }}\u0026#34; image: \u0026#34;{{ router_container_kind }}:latest\u0026#34; ports: - \u0026#34;5{{ user_map[lab_username] }}06:22\u0026#34; {{ lab_title }}-{{ lab_username }}-client01: kind: \u0026#34;{{ host_container_kind }}\u0026#34; image: \u0026#34;{{ host_container_kind }}:latest\u0026#34; ports: - \u0026#34;5{{ user_map[lab_username] }}07:22\u0026#34; {{ lab_title }}-{{ lab_username }}-client02: kind: \u0026#34;{{ host_container_kind }}\u0026#34; image: \u0026#34;{{ host_container_kind }}:latest\u0026#34; ports: - \u0026#34;5{{ user_map[lab_username] }}08:22\u0026#34; links: {%for link in links %} - endpoints: [{{ lab_title }}-{{ lab_username }}-{{link[\u0026#39;endpoints\u0026#39;][0]}},{{ lab_title }}-{{ lab_username }}-{{link[\u0026#39;endpoints\u0026#39;][1]}}] {% endfor %} mgmt: network: \u0026#34;{{ lab_title }}-{{ lab_username }}-network\u0026#34; ipv4-subnet: \u0026#34;{{ ipv4_subnet | ansible.utils.ipaddr }}\u0026#34; The virtual wiring is handled by the endpoints file here:\n--- links: - endpoints: [\u0026#34;spine01:eth1\u0026#34;, \u0026#34;spine02:eth1\u0026#34;] - endpoints: [\u0026#34;spine01:eth2\u0026#34;, \u0026#34;leaf01:eth2\u0026#34;] - endpoints: [\u0026#34;spine01:eth3\u0026#34;, \u0026#34;leaf02:eth2\u0026#34;] - endpoints: [\u0026#34;spine01:eth4\u0026#34;, \u0026#34;leaf03:eth2\u0026#34;] - endpoints: [\u0026#34;spine01:eth5\u0026#34;, \u0026#34;leaf04:eth2\u0026#34;] - endpoints: [\u0026#34;spine01:eth6\u0026#34;, \u0026#34;spine02:eth6\u0026#34;] - endpoints: [\u0026#34;spine02:eth2\u0026#34;, \u0026#34;leaf01:eth3\u0026#34;] - endpoints: [\u0026#34;spine02:eth3\u0026#34;, \u0026#34;leaf02:eth3\u0026#34;] - endpoints: [\u0026#34;spine02:eth4\u0026#34;, \u0026#34;leaf03:eth3\u0026#34;] - endpoints: [\u0026#34;spine02:eth5\u0026#34;, \u0026#34;leaf04:eth3\u0026#34;] - endpoints: [\u0026#34;leaf01:eth1\u0026#34;, \u0026#34;leaf02:eth1\u0026#34;] - endpoints: [\u0026#34;leaf01:eth6\u0026#34;, \u0026#34;leaf02:eth6\u0026#34;] - endpoints: [\u0026#34;leaf03:eth1\u0026#34;, \u0026#34;leaf04:eth1\u0026#34;] - endpoints: [\u0026#34;leaf03:eth6\u0026#34;, \u0026#34;leaf04:eth6\u0026#34;] - endpoints: [\u0026#34;client01:eth1\u0026#34;, \u0026#34;leaf01:eth4\u0026#34;] - endpoints: [\u0026#34;client02:eth1\u0026#34;, \u0026#34;leaf04:eth4\u0026#34;] The other base vars are populated by this file:\n### Base vars to get over-written by Ansible Tower Survey (i.e. Extra Vars) at runtime --- lab_hosts: containerlab01.server lab_username: kaon lab_userid: \u0026#34;11\u0026#34; lab_title: \u0026#34;server-core-lab01\u0026#34; router_container_kind: ceos host_container_kind: linux wireshark_command: \u0026#34;ssh clab@containerlab01.server \u0026#39;sudo ip netns exec \u0026lt;container_name\u0026gt; tcpdump -U -nni \u0026lt;intf\u0026gt; -w -\u0026#39; | /mnt/c/Program\\\\ Files/Wireshark/wireshark.exe -k -i -\u0026#34; actions: - deploy user_map: jack: \u0026#34;00\u0026#34; jill: \u0026#34;01\u0026#34; john: \u0026#34;02\u0026#34; bob: \u0026#34;03\u0026#34; mary: \u0026#34;04\u0026#34; joe: \u0026#34;05\u0026#34; kaon: \u0026#34;11\u0026#34; Ansible Wrapper We can use Ansible to take in user input, render the jinja2 template and launch the ContainerLab binary. The ansible playbook can also handle tearing down and destroying the topology.\ncontainerlab_engine.yml:\n--- - name: Container Lab Engine for Self Service Labs gather_facts: false hosts: \u0026#34;{{ lab_hosts }}\u0026#34; become: yes vars_files: - \u0026#34;{{ lab_title }}/{{ lab_title }}-vars.yml\u0026#34; - base_vars.yml tasks: ### Create User and Lab specific directory on remote linux server hosting containerlab - name: Create User Specific Lab Directory file: path: \u0026#34;/opt/containerlab/labs/{{ lab_username }}/{{ lab_title }}\u0026#34; state: directory when: \u0026#39;\u0026#34;deploy\u0026#34; in actions\u0026#39; ### Declare unique Subnet Per user to be used later for Docker Container network - name: Set CLAB Docker IP Network for Each User set_fact: ipv4_subnet: 172.100.{{ user_map[lab_username] }}.0/24 changed_when: false when: \u0026#39;\u0026#34;deploy\u0026#34; in actions\u0026#39; ### Use local topology jinja2 file to build remote containerlab topology file with per user/lab variables - name: Build Lab Topology File ansible.builtin.template: src: \u0026#34;{{ lab_title }}/{{ lab_title }}.j2\u0026#34; dest: \u0026#34;/opt/containerlab/labs/{{ lab_username }}/{{ lab_title }}/{{lab_title}}-{{ lab_username }}.clab.yml\u0026#34; when: \u0026#39;\u0026#34;deploy\u0026#34; in actions\u0026#39; ### Run Container Lab Deploy to build lab from .clab.yaml topology file - name: Deploy Lab File ansible.builtin.command: chdir: /opt/containerlab/labs/{{ lab_username }}/{{ lab_title }} cmd: containerlab deploy when: \u0026#39;\u0026#34;deploy\u0026#34; in actions\u0026#39; ### Show All Docker Containers Running By current User - name: Show Docker Containers ansible.builtin.command: chdir: /opt/containerlab/labs/{{ lab_username }}/{{ lab_title }} cmd: docker container ls -f name={{lab_username}} register: docker_output when: (\u0026#34;deploy\u0026#34; in actions) or (\u0026#34;inspect\u0026#34; in actions) - name: Print Docker Output ansible.builtin.debug: msg: \u0026#34;{{ docker_output.stdout_lines }}\u0026#34; when: - docker_output is defined - (\u0026#34;deploy\u0026#34; in actions) or (\u0026#34;inspect\u0026#34; in actions) ### Inspect containerlab info for current user and lab - name: Inspect Lab Topology ansible.builtin.command: chdir: /opt/containerlab/labs/{{ lab_username }}/{{ lab_title }} cmd: containerlab inspect register: deploy_output when: (\u0026#34;deploy\u0026#34; in actions) or (\u0026#34;inspect\u0026#34; in actions) - name: Print Lab Topology Output ansible.builtin.debug: msg: \u0026#34;{{ deploy_output.stdout_lines }}\u0026#34; when: - deploy_output is defined - (\u0026#34;deploy\u0026#34; in actions) or (\u0026#34;inspect\u0026#34; in actions) ### Powerdown lab nodes while maintaining startup configuration - name: Shutdown Lab ansible.builtin.command: chdir: /opt/containerlab/labs/{{ lab_username }}/{{ lab_title }} cmd: containerlab destroy when: \u0026#39;\u0026#34;shutdown\u0026#34; in actions\u0026#39; ### Powerdown and delete all startup configuration of nodes - name: Destroy Lab ansible.builtin.command: chdir: /opt/containerlab/labs/{{ lab_username }}/{{ lab_title }} cmd: containerlab destroy -c when: \u0026#39;\u0026#34;destroy\u0026#34; in actions\u0026#39; ### Run containerlab save on all nodes to save startup configs locally - name: Save Lab ansible.builtin.command: chdir: /opt/containerlab/labs/{{ lab_username }}/{{ lab_title }} cmd: containerlab save when: \u0026#39;\u0026#34;save\u0026#34; in actions\u0026#39; - name: Print Instructions ansible.builtin.debug: msg: - \u0026#34;to connect to your Containers: ssh clab@{{inventory_hostname}}:5{{ user_map[lab_username] }}XX\u0026#34; - \u0026#34;to create IP interface on a linux container. First SSH then RUN command: sudo ifconfig \u0026lt;interface\u0026gt; \u0026lt;ip\u0026gt; netmask \u0026lt;mask\u0026gt;\u0026#34; - \u0026#34;to capture traffic and redirect locally to windows wireshark: {{ wireshark_command }}\u0026#34; when: - deploy_output is defined - (\u0026#34;deploy\u0026#34; in actions) or (\u0026#34;inspect\u0026#34; in actions) Garbage Collector If a user forgets to tear down their lab. We can run a periodic garbage collector playbook which checks for labs running longer than 7 days and powers them down.\ngarbage-collector.yml:\n--- - name: Service to Shutdown Long Running ContainerLabs gather_facts: false hosts: localhost become: yes tasks: - name: Grab List of All Running ContainerLab Docker Instances ansible.builtin.command: chdir: /opt/containerlab/ cmd: containerlab inspect -a --details -f json register: containerlab_inspect - name: Loop through and inspect each ID ansible.builtin.command: chdir: /opt/containerlab/ cmd: \u0026#34;docker inspect {{ item[\u0026#39;ShortID\u0026#39;]}}\u0026#34; register: docker_inspect_raw with_items: \u0026#34;{{containerlab_inspect.stdout}}\u0026#34; loop_control: label: \u0026#34;{{ item[\u0026#39;ShortID\u0026#39;]}}\u0026#34; - name: Create ID to Creation timestamp dictionary mapping set_fact: timestamps: \u0026#34;{{ timestamps | default({}) | combine( {item.item.Labels[\u0026#39;clab-topo-file\u0026#39;]: item.stdout | from_json | json_query(\u0026#39;[0].Created\u0026#39;)}) }}\u0026#34; with_items: \u0026#34;{{docker_inspect_raw.results}}\u0026#34; loop_control: label: \u0026#34;{{item.cmd[2]}}\u0026#34; - name: custom filter set_fact: list_to_shutdown: \u0026#34;{{ timestamps | timestamp_differ }}\u0026#34; - name: Loop through and shutdown long running labs ansible.builtin.command: chdir: /opt/containerlab/ cmd: \u0026#34;containerlab destroy -t {{ item }}\u0026#34; with_items: \u0026#34;{{list_to_shutdown}}\u0026#34; loop_control: label: \u0026#34;{{ item}}\u0026#34; Other Pre-Canned Labs Wan Lab 01 wan-lab-01 Server Core Lab 01 server-core Spine Leaf Lab 01 spine-leaf Linux Image and LACP Bonding In order to test real-world scenarios, we can spin up an alpine-linux container and connect it to our router images with LACP link bonding.\nWe build the docker image using this dockerfile and build script found here Once built, we can use the image in our topology files as clients or servers.\nWireshark and iperf Testing end-to-end application and network connectivty may require more than just ping and traceroute. We can do full wireshark packet captures on any of the links. We can also generate traffic with iperf from the pre-built alpine linux containers.\nTo capture traffic and redirect locally to windows wireshark:\nssh clab@containerlab01.server \u0026#39;sudo ip netns exec \u0026lt;container_name\u0026gt; tcpdump -U -nni \u0026lt;intf\u0026gt; -w -\u0026#39; | /mnt/c/Program\\\\ Files/Wireshark/wireshark.exe -k -i - To run an iperf server one of the alpine-linux containers:\ndocker exec -it \u0026lt;container-name\u0026gt; /bin/sh -c \u0026#34;sudo iperf3 -s -B \u0026lt;iperf-server-ip\u0026gt; -p 5201\u0026#34; To run an iperf client test:\ndocker exec -it \u0026lt;container-name\u0026gt; /bin/sh -c \u0026#34;sudo iperf3 -c \u0026lt;iperf-server-ip\u0026gt; -b 10Mbit -p 5201 -P 5 -t 60\u0026#34; End I hope a guide like this can help ease the barrier to entry for creating network lab environments. If you have any questions don\u0026rsquo;t hesitate to reach out to me on my socials. Thank you.\n","date":"2023-07-24T00:00:00Z","image":"https://kaonbytes.com/p/self-service-containerlab-deployment-with-ansible-tower/images/clab-ansible_hu900e690d7d00d8033c3afbeaf3ab65f6_51076_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/self-service-containerlab-deployment-with-ansible-tower/","title":"Self-Service ContainerLab Deployment with Ansible Tower"},{"content":"My First Podcast Interview A few weeks ago I was contacted by Kris Beevers to be a guest on the Netbox Heroes podcast. He discovered my blog and noticed I was posting some Netbox related content and thought I could give some useful insight. I happily agreed and had a great time doing it.\nHere is the link to the Netbox Labs Blog Post recapping the interview.\nYou can listen/watch the full video on youtube here\nThanks!\n","date":"2023-06-05T00:00:00Z","image":"https://kaonbytes.com/p/my-first-podcast-interview/images/cover_huc2b67065cc238bc9f15887ce24c9ad33_147103_120x120_fill_q75_box_smart1.jpg","permalink":"https://kaonbytes.com/p/my-first-podcast-interview/","title":"My First Podcast Interview"},{"content":"Problem A stakeholder posts a question to the Network Team: \u0026ldquo;Hi, what\u0026rsquo;s the IP Address range for some list of services or sites\nWithin a few minutes, a senior network greybeard emerges from his lair, dusts off his keyboard and starts typing away retrieving all the IPv4 addresses burned into his hippocampus due to years of on-call PTSD.\nThis is a perfect solution and we should never improve upon this. HOWEVER, for argument\u0026rsquo;s sake lets say we wanted a more structured and consistent way of providing this information to our stakeholders?\nSolution I built a Slack Application which can be invoked with a slash command and interacts with Netbox to retrieve IP Address Management (IPAM) information.\nThe application code is hosted on AWS Lambda Functions which are triggered by an API Gateway request. The AWS infrastructure is deployed using Infrastructure as Code (IaC) principles with a Github, Drone, Terraform pipeline.\nThis blog post is a guide on how you can do it too.\nWhy build it? As I mentioned above, some of the most common queries asked of Network Teams are IP Address questions from stakeholders within the organization. Generally relating to, allow list ranges, egress IPs, site and user specific IP addresses for identification. Some examples may include:\nWhat are the VPN Subnets? Can I get a list of all remote site\u0026rsquo;s Internet Egress addresses? What are the Wireless Subnets in Site X? Instead of relying on shared spreadsheets and/or tribal knowledge which may change based on which engineer is answering the question this week. If we focus our energy on maintaining an accurate single source of truth (SSoT), then we can relay this information back with programmatic efficiency.\nWhere to host it? It depends.\nSlack is in the public cloud therefore, slash commands, webhooks and bots need to make calls to public endpoints. If your SSoT (in this case Netbox) is not accessible on a public endpoint then what?\nWell, in my case, i\u0026rsquo;m hosting Netbox in a private AWS VPC so the best solution is to create a public AWS API Gateway Endpoint which integrates with an AWS Lambda Function. The lambda function acts as a buffer between the public internet and the Netbox API. Request comes in to the API Gateway, gets forwarded to the lambda, lambda python code performs the logic, talks to Netbox and returns a pretty result for the end user.\nWhat\u0026rsquo;s it look like? User types /ipam in Slack and a menu appears like below:\nExample 1: User chooses first radio button and gets back a list of prefixes with descriptions:\nExample 2: User enters a specific IP Address in the form and presses enter. They get back the full JSON result that Netbox has on that IP Address:\nExample 3: User enters an IP Address that is not known, however is part of a larger prefix. We return information of the parent prefixes:\nHigh Level Architecture Diagram The Code All code can be found on my GitHub here\nDeploying with a Terraform, Drone, GitHub Pipeline Why should we use Infrastructure as Code (IaC)?\nWe don\u0026rsquo;t want this project to turn into tech debt. IaC helps with this because:\nAllows for version controlled configuration changes Self-documents (for the most part) Creates an easy-to-follow process for updating and maintaining the application and system environment Example Pipeline Run:\nIn this post, I am not going to dive deep into the IaC pipeline. If you want to learn more about it, you can follow my previous post: bgpalerter-as-code-using-a-terraform-pipeline\nSystem Components Netbox IPAM and Tagging The key to this whole endeavor is to make sure we maintain accurate data in our IPAM. Much of this work will be manual, but it needs to be done somewhere, so its better that these updates are centralized.\nAnother way to supplement the IPAM data is to backfill Netbox with data directly from our devices. I have a blog post showing how we can do that here \u0026ndash; netbox-dynamic-inventory-for-ansible-as-a-feedback-loop\nIn addition to maintaining accurate data, we also need to tag the data that we want to relay to our stakeholders. For example, user clicks the VPN Subnets button on the slack app, then the data returned should be all prefixes tagged with vpn_subnets.\nExample of tagging prefixes in Netbox:\nSlack API Admin Building the Slack App requires access to https://api.slack.com/start/building. Once you have access, you can start building your app.\nSlash Commands First we can start by creating the /ipam slash command as shown here:\nInside the command, we enter our API Gateway Public Endpoint (we haven\u0026rsquo;t actually created that yet):\nThe response of this endpoint will be a JSON result which will show our initial menu.\nInteractivity \u0026amp; Shortcuts When a user interacts with the initial menu, a new webhook is sent to slack. The destination of this webhook can be configured under Slack \u0026ndash;\u0026gt; Interactivity. Here we enter our 2nd API Gateway endpoint which points to our second lambda (not configured yet)\nToken A verification token is sent along with all slash command payloads. Our Lambda function will parse this token and verify that it is coming from our workspace. This prevents unauthorized users from gaining access to the Netbox data.\nThe verification token can be found in your Slack App Admin \u0026ndash;\u0026gt; Basic Information \u0026ndash;\u0026gt; Verification Token\nBlock Kit Builder Block Kit builder is a visualization tool that slack provides to build your forms and user input fields. The JSON payload can be copied to our lambda function as the returning result of the /ipam slash command.\nAmazon AWS Components API Gateway Terraform Code:\nresource \u0026#34;aws_apigatewayv2_api\u0026#34; \u0026#34;ipam-helper-slack-api\u0026#34; { name = \u0026#34;ipam-helper-slack-api\u0026#34; protocol_type = \u0026#34;HTTP\u0026#34; } resource \u0026#34;aws_apigatewayv2_integration\u0026#34; \u0026#34;ipam-get-menu-integration\u0026#34; { api_id = aws_apigatewayv2_api.ipam-helper-slack-api.id integration_type = \u0026#34;AWS_PROXY\u0026#34; connection_type = \u0026#34;INTERNET\u0026#34; # content_handling_strategy = \u0026#34;CONVERT_TO_TEXT\u0026#34; description = \u0026#34;Ipam Helper Get Menu Integration\u0026#34; integration_method = \u0026#34;POST\u0026#34; integration_uri = aws_lambda_function.ipam_get_menu.invoke_arn passthrough_behavior = \u0026#34;WHEN_NO_MATCH\u0026#34; } resource \u0026#34;aws_apigatewayv2_integration\u0026#34; \u0026#34;ipam-return-action-integration\u0026#34; { api_id = aws_apigatewayv2_api.ipam-helper-slack-api.id integration_type = \u0026#34;AWS_PROXY\u0026#34; connection_type = \u0026#34;INTERNET\u0026#34; # content_handling_strategy = \u0026#34;CONVERT_TO_TEXT\u0026#34; description = \u0026#34;Ipam Helper Return Action Integration\u0026#34; integration_method = \u0026#34;POST\u0026#34; integration_uri = aws_lambda_function.ipam_return_action.invoke_arn passthrough_behavior = \u0026#34;WHEN_NO_MATCH\u0026#34; } resource \u0026#34;aws_apigatewayv2_route\u0026#34; \u0026#34;ipam-get-menu-route\u0026#34; { api_id = aws_apigatewayv2_api.ipam-helper-slack-api.id route_key = \u0026#34;ANY /ipam_get_menu\u0026#34; target = \u0026#34;integrations/${aws_apigatewayv2_integration.ipam-get-menu-integration.id}\u0026#34; } resource \u0026#34;aws_apigatewayv2_route\u0026#34; \u0026#34;ipam-return-action-route\u0026#34; { api_id = aws_apigatewayv2_api.ipam-helper-slack-api.id route_key = \u0026#34;ANY /ipam_return_action\u0026#34; target = \u0026#34;integrations/${aws_apigatewayv2_integration.ipam-return-action-integration.id}\u0026#34; } resource \u0026#34;aws_apigatewayv2_stage\u0026#34; \u0026#34;ipam-helper-slack-api-deploy\u0026#34; { api_id = aws_apigatewayv2_api.ipam-helper-slack-api.id name = \u0026#34;default\u0026#34; auto_deploy = true } API GW in GUI:\nAPI GW Integrations to Lambdas:\nAPI GW Routes:\nLambda 1 - Get Menu Terraform:\ndata \u0026#34;archive_file\u0026#34; \u0026#34;ipam_get_menu\u0026#34; { type = \u0026#34;zip\u0026#34; source_dir = abspath(\u0026#34;${path.root}/../../lambda_ipam_get_menu\u0026#34;) output_path = abspath(\u0026#34;${path.root}/../modules/ipam-helper/lambda_ipam_get_menu.zip\u0026#34;) } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;iam_for_ipam_get_menu\u0026#34; { name = \u0026#34;iam_for_ipam_get_menu\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } EOF } resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;ipam_get_menu\u0026#34; { filename = data.archive_file.ipam_get_menu.output_path function_name = \u0026#34;ipam_get_menu\u0026#34; role = aws_iam_role.iam_for_ipam_get_menu.arn handler = \u0026#34;lambda_function.lambda_handler\u0026#34; source_code_hash = filebase64sha256(data.archive_file.ipam_get_menu.output_path) runtime = \u0026#34;python3.8\u0026#34; timeout = 30 # Time out in second, default value is 3 memory_size = 128 # Default value in MB environment { variables = { kmsEncryptedToken = \u0026#34;replacelater\u0026#34; } } depends_on = [aws_cloudwatch_log_group.cw_ipam_get_menu] tags = var.tags } resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;cw_ipam_get_menu\u0026#34; { name = \u0026#34;/aws/lambda/ipam_get_menu\u0026#34; retention_in_days = 180 } # See also the following AWS managed policy: AWSLambdaBasicExecutionRole resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;ipam_get_menu_lambda_logging\u0026#34; { name = \u0026#34;ipam_get_menu_lambda_logging\u0026#34; path = \u0026#34;/\u0026#34; description = \u0026#34;IAM policy for logging from a lambda\u0026#34; policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } EOF } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;ipam_get_menu_lambda_logs\u0026#34; { role = aws_iam_role.iam_for_ipam_get_menu.name policy_arn = aws_iam_policy.ipam_get_menu_lambda_logging.arn } resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;apigw\u0026#34; { statement_id = \u0026#34;AllowAPIGatewayInvoke\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = aws_lambda_function.ipam_get_menu.function_name principal = \u0026#34;apigateway.amazonaws.com\u0026#34; # The /*/* portion grants access from any method on any resource # within the API Gateway \u0026#34;REST API\u0026#34;. source_arn = \u0026#34;${aws_apigatewayv2_api.ipam-helper-slack-api.execution_arn}/*/*/ipam_get_menu\u0026#34; } Python Code:\nimport boto3 import json import logging import os from base64 import b64decode from urllib.parse import parse_qs ENCRYPTED_EXPECTED_TOKEN = os.environ[\u0026#39;kmsEncryptedToken\u0026#39;] kms = boto3.client(\u0026#39;kms\u0026#39;) expected_token = kms.decrypt( CiphertextBlob=b64decode(ENCRYPTED_EXPECTED_TOKEN), EncryptionContext={\u0026#39;LambdaFunctionName\u0026#39;: os.environ[\u0026#39;AWS_LAMBDA_FUNCTION_NAME\u0026#39;]} )[\u0026#39;Plaintext\u0026#39;].decode(\u0026#39;utf-8\u0026#39;) logger = logging.getLogger() logger.setLevel(logging.INFO) def respond(err, res=None): return { \u0026#39;statusCode\u0026#39;: \u0026#39;400\u0026#39; if err else \u0026#39;200\u0026#39;, \u0026#39;body\u0026#39;: err.message if err else json.dumps(res).encode(\u0026#39;utf8\u0026#39;), \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, } def lambda_handler(event, context): # logger.info(event) decode_body = b64decode(str(event[\u0026#39;body\u0026#39;])).decode(\u0026#39;ascii\u0026#39;) params = parse_qs(decode_body) token = params[\u0026#39;token\u0026#39;][0] if token != expected_token: logger.error(\u0026#34;Request token (%s) does not match expected\u0026#34;, token) return respond(Exception(\u0026#39;Invalid request token\u0026#39;)) f = open(\u0026#39;response_menu.json\u0026#39;, \u0026#34;r\u0026#34;) slack_menu_json = json.loads(f.read()) return respond(None, slack_menu_json) Console:\nLambda 2 - Return Data Terraform:\ndata \u0026#34;archive_file\u0026#34; \u0026#34;ipam_return_action\u0026#34; { type = \u0026#34;zip\u0026#34; source_dir = abspath(\u0026#34;${path.root}/../../lambda_ipam_return_action\u0026#34;) output_path = abspath(\u0026#34;${path.root}/../modules/ipam-helper/lambda_ipam_return_action.zip\u0026#34;) } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;iam_for_ipam_return_action\u0026#34; { name = \u0026#34;iam_for_ipam_return_action\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } EOF } resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;ipam_return_action\u0026#34; { filename = data.archive_file.ipam_return_action.output_path function_name = \u0026#34;ipam_return_action\u0026#34; role = aws_iam_role.iam_for_ipam_return_action.arn handler = \u0026#34;lambda_function.lambda_handler\u0026#34; source_code_hash = filebase64sha256(data.archive_file.ipam_return_action.output_path) runtime = \u0026#34;python3.8\u0026#34; timeout = 30 # Time out in second, default value is 3 memory_size = 128 # Default value in MB vpc_config { subnet_ids = [\u0026#34;\u0026lt;subnet1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;subnet2\u0026gt;\u0026#34;] security_group_ids = [aws_security_group.lambda_slackbot_sg.id] } environment { variables = { kmsEncryptedToken = \u0026#34;replacelater\u0026#34; NETBOX_TOKEN = \u0026#34;replacelater\u0026#34; } } depends_on = [aws_cloudwatch_log_group.cw_ipam_return_action] tags = var.tags } resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;cw_ipam_return_action\u0026#34; { name = \u0026#34;/aws/lambda/ipam_return_action\u0026#34; retention_in_days = 180 } # See also the following AWS managed policy: AWSLambdaBasicExecutionRole resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;ipam_return_action_lambda_logging\u0026#34; { name = \u0026#34;ipam_return_action_lambda_logging\u0026#34; path = \u0026#34;/\u0026#34; description = \u0026#34;IAM policy for logging from a lambda\u0026#34; policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } EOF } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;ipam_return_action_lambda_logs\u0026#34; { role = aws_iam_role.iam_for_ipam_return_action.name policy_arn = aws_iam_policy.ipam_return_action_lambda_logging.arn } resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;apigw_2\u0026#34; { statement_id = \u0026#34;AllowAPIGatewayInvoke\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = aws_lambda_function.ipam_return_action.function_name principal = \u0026#34;apigateway.amazonaws.com\u0026#34; # The /*/* portion grants access from any method on any resource # within the API Gateway \u0026#34;REST API\u0026#34;. source_arn = \u0026#34;${aws_apigatewayv2_api.ipam-helper-slack-api.execution_arn}/*/*/ipam_return_action\u0026#34; } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;iam_role_policy_attachment_lambda_vpc_access_execution\u0026#34; { role = aws_iam_role.iam_for_ipam_return_action.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole\u0026#34; } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;lambda_slackbot_sg\u0026#34; { # provider = aws.us-east-1 name = \u0026#34;lambda_slackbot_sg\u0026#34; description = \u0026#34;Security Group to manage lambda_slackbot access\u0026#34; vpc_id = data.terraform_remote_state.account_base.outputs.vpc_east_id ingress { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;10.0.0.0/8\u0026#34;] } ingress { from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;10.0.0.0/8\u0026#34;] } ingress { from_port = -1 to_port = -1 protocol = \u0026#34;icmp\u0026#34; cidr_blocks = [\u0026#34;10.0.0.0/8\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } Python Code:\nimport boto3 import json import logging import os import requests import ipaddress import urllib.parse import pprint from base64 import b64decode from urllib.parse import parse_qs ENCRYPTED_EXPECTED_TOKEN = os.environ[\u0026#39;kmsEncryptedToken\u0026#39;] NETBOX_TOKEN = os.environ[\u0026#39;NETBOX_TOKEN\u0026#39;] netbox_url = \u0026#39;\u0026lt;redacted\u0026gt;\u0026#39; netbox_headers = {\u0026#39;Authorization\u0026#39;: \u0026#34;Token {}\u0026#34;.format(NETBOX_TOKEN)} kms = boto3.client(\u0026#39;kms\u0026#39;) expected_token = kms.decrypt( CiphertextBlob=b64decode(ENCRYPTED_EXPECTED_TOKEN), EncryptionContext={\u0026#39;LambdaFunctionName\u0026#39;: os.environ[\u0026#39;AWS_LAMBDA_FUNCTION_NAME\u0026#39;]} )[\u0026#39;Plaintext\u0026#39;].decode(\u0026#39;utf-8\u0026#39;) logger = logging.getLogger() logger.setLevel(logging.INFO) def respond(err, res=None): return { \u0026#39;statusCode\u0026#39;: \u0026#39;400\u0026#39; if err else \u0026#39;200\u0026#39;, \u0026#39;body\u0026#39;: err.message if err else json.dumps(res).encode(\u0026#39;utf8\u0026#39;), \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, } def netbox_return_prefixes(tag): ipam_api_call = requests.get(netbox_url+\u0026#34;/api/ipam/prefixes/?limit=0\u0026amp;tag=\u0026#34;+tag, headers=netbox_headers, verify=False).json() return ipam_api_call.get(\u0026#39;results\u0026#39;) def netbox_return_addresses(tag): ipam_api_call = requests.get(netbox_url+\u0026#34;/api/ipam/ip-addresses/?limit=0\u0026amp;tag=\u0026#34;+tag, headers=netbox_headers, verify=False).json() return ipam_api_call.get(\u0026#39;results\u0026#39;) def netbox_return_ranges(tag): ipam_api_call = requests.get(netbox_url+\u0026#34;/api/ipam/ip-ranges/?limit=0\u0026amp;tag=\u0026#34;+tag, headers=netbox_headers, verify=False).json() return ipam_api_call.get(\u0026#39;results\u0026#39;) def netbox_return_prefix_input(input): ipam_api_call = requests.get(netbox_url+\u0026#34;/api/ipam/prefixes/?limit=0\u0026amp;prefix=\u0026#34;+input, headers=netbox_headers, verify=False).json() return ipam_api_call.get(\u0026#39;results\u0026#39;) def netbox_return_address_input(input): ipam_api_call = requests.get(netbox_url+\u0026#34;/api/ipam/ip-addresses/?limit=0\u0026amp;address=\u0026#34;+input, headers=netbox_headers, verify=False).json() return ipam_api_call.get(\u0026#39;results\u0026#39;) def netbox_return_prefix_contains_input(input): ipam_api_call = requests.get(netbox_url+\u0026#34;/api/ipam/prefixes/?limit=0\u0026amp;contains=\u0026#34;+input, headers=netbox_headers, verify=False).json() return ipam_api_call.get(\u0026#39;results\u0026#39;) def send_ephemeral_message(response_url, data): ephemeral_headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} # logger.info(response_url) return requests.post(response_url, headers=ephemeral_headers, json=data) def action_get_public_nat(tag=\u0026#34;public_egress\u0026#34;): public_nat_prefixes = netbox_return_prefixes(tag) public_nat_ranges = netbox_return_ranges(tag) public_nat_addresses = netbox_return_addresses(tag) ephemeral_response_text = \u0026#34;\u0026#34; for prefix in public_nat_prefixes: ephemeral_response_text += \u0026#34;Prefix --\u0026gt; \u0026#34;+str(prefix.get(\u0026#39;prefix\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(prefix.get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; for range in public_nat_ranges: ephemeral_response_text += \u0026#34;Range --\u0026gt; \u0026#34;+str(range.get(\u0026#39;display\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(range.get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; for address in public_nat_addresses: ephemeral_response_text += \u0026#34;Address --\u0026gt; \u0026#34;+str(address.get(\u0026#39;address\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(address.get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; return ephemeral_response_text def action_get_trusted_user_subnet(tag=\u0026#34;wireless_user_subnet\u0026#34;): trusted_user_prefixes = netbox_return_prefixes(tag) trusted_user_ranges = netbox_return_ranges(tag) trusted_user_addresses = netbox_return_addresses(tag) ephemeral_response_text = \u0026#34;\u0026#34; for prefix in trusted_user_prefixes: ephemeral_response_text += \u0026#34;Prefix --\u0026gt; \u0026#34;+str(prefix.get(\u0026#39;prefix\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(prefix.get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; for range in trusted_user_ranges: ephemeral_response_text += \u0026#34;Range --\u0026gt; \u0026#34;+str(range.get(\u0026#39;display\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(range.get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; for address in trusted_user_addresses: ephemeral_response_text += \u0026#34;Address --\u0026gt; \u0026#34;+str(address.get(\u0026#39;address\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(address.get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; return ephemeral_response_text def action_get_tagged_subnet(tag): trusted_user_prefixes = netbox_return_prefixes(tag) trusted_user_ranges = netbox_return_ranges(tag) trusted_user_addresses = netbox_return_addresses(tag) ephemeral_response_text = \u0026#34;\u0026#34; for prefix in trusted_user_prefixes: ephemeral_response_text += \u0026#34;Prefix --\u0026gt; \u0026#34;+str(prefix.get(\u0026#39;prefix\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(prefix.get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; for range in trusted_user_ranges: ephemeral_response_text += \u0026#34;Range --\u0026gt; \u0026#34;+str(range.get(\u0026#39;display\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(range.get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; for address in trusted_user_addresses: ephemeral_response_text += \u0026#34;Address --\u0026gt; \u0026#34;+str(address.get(\u0026#39;address\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(address.get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; return ephemeral_response_text def action_get_more_info(user_input): prefix_info = netbox_return_prefix_input(user_input) address_info = netbox_return_address_input(user_input) prefix_contains_info = netbox_return_prefix_contains_input(user_input) # logger.info(user_input) # logger.info(prefix_info) # logger.info(address_info) ephemeral_response_text = \u0026#34;\u0026#34; if prefix_info is not None and len(prefix_info) \u0026gt; 0: # ephemeral_response_text += \u0026#34;Prefix --\u0026gt; \u0026#34;+str(prefix_info[0].get(\u0026#39;prefix\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(prefix_info[0].get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; ephemeral_response_text = pprint.pformat(prefix_info[0], indent=4) elif address_info is not None and len(address_info) \u0026gt; 0: # ephemeral_response_text += \u0026#34;Address --\u0026gt; \u0026#34;+str(address_info[0].get(\u0026#39;address\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(address_info[0].get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; ephemeral_response_text = pprint.pformat(address_info[0], indent=4) elif prefix_contains_info is not None and len(prefix_contains_info) \u0026gt; 0: ephemeral_response_text += \u0026#34;Specific entry not found but is contained in the following Prefixes:\\n\u0026#34; for item in prefix_contains_info: ephemeral_response_text += \u0026#34;Prefix --\u0026gt; \u0026#34;+str(item.get(\u0026#39;prefix\u0026#39;))+\u0026#34; | Description --\u0026gt; \u0026#34;+str(item.get(\u0026#39;description\u0026#39;))+\u0026#34;\\n\u0026#34; else: ephemeral_response_text = \u0026#34;No Information Found for This Entry --\u0026gt; \u0026#34;+user_input return ephemeral_response_text def lambda_handler(event, context): # logger.info(event) decode_body = b64decode(str(event[\u0026#39;body\u0026#39;])).decode(\u0026#39;ascii\u0026#39;) params = parse_qs(decode_body) payload_dict = json.loads(params[\u0026#39;payload\u0026#39;][0]) # logger.info(payload_dict) # logger.info(type(payload_dict)) token = payload_dict[\u0026#39;token\u0026#39;] if token != expected_token: logger.error(\u0026#34;Request token (%s) does not match expected\u0026#34;, token) return respond(Exception(\u0026#39;Invalid request token\u0026#39;)) # logger.info(payload_dict[\u0026#39;actions\u0026#39;][0][\u0026#39;value\u0026#39;]) if payload_dict[\u0026#39;actions\u0026#39;][0][\u0026#39;value\u0026#39;] == \u0026#34;public_egress\u0026#34;: ephemeral_response_body = {\u0026#34;text\u0026#34;: action_get_public_nat()} if payload_dict[\u0026#39;actions\u0026#39;][0][\u0026#39;value\u0026#39;] == \u0026#34;wireless_user_subnet\u0026#34;: ephemeral_response_body = {\u0026#34;text\u0026#34;: action_get_trusted_user_subnet()} if payload_dict[\u0026#39;actions\u0026#39;][0][\u0026#39;value\u0026#39;] == \u0026#34;vpn_subnet\u0026#34;: ephemeral_response_body = {\u0026#34;text\u0026#34;: action_get_tagged_subnet(payload_dict[\u0026#39;actions\u0026#39;][0][\u0026#39;value\u0026#39;])} if payload_dict[\u0026#39;actions\u0026#39;][0][\u0026#39;action_id\u0026#39;] == \u0026#34;plain_text_input-action\u0026#34;: user_input = payload_dict[\u0026#39;actions\u0026#39;][0][\u0026#39;value\u0026#39;] try: user_input_validated = ipaddress.ip_interface(user_input) user_input_encoded = urllib.parse.quote(str(user_input).encode(\u0026#39;utf8\u0026#39;)) ephemeral_response_body = {\u0026#34;text\u0026#34;: action_get_more_info(user_input_encoded)} except ValueError as e: ephemeral_response_body = {\u0026#34;text\u0026#34;: \u0026#34;You entered an Unrecognized IP or Prefix - Please Enter in the Format of 10.10.10.1 or 10.10.10.0/24\u0026#34;} send_msg = send_ephemeral_message(payload_dict[\u0026#39;response_url\u0026#39;], ephemeral_response_body) logger.info(send_msg) api_response = {\u0026#34;text\u0026#34;: \u0026#34;okay\u0026#34;} return respond(None, api_response) Console:\nEncrypting the Slack Token We can encrypt the Slack verification token using AWS KMS Encryption. To encrypt your token use the following steps:\nCreate or use an existing KMS Key - http://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html\nExpand \u0026ldquo;Encryption configuration\u0026rdquo; and click the \u0026ldquo;Enable helpers for encryption in transit\u0026rdquo; checkbox\nPaste \u0026lt;COMMAND_TOKEN\u0026gt; into the kmsEncryptedToken environment variable and click \u0026ldquo;Encrypt\u0026rdquo;\nGive your function\u0026rsquo;s role permission for the kms:Decrypt action using the provided policy template\nThe encrypted token will look something like this:\nConclusion - Next Steps? Future improvements for this app may include:\nAdditional inputs, create drop down lists instead of buttons Auto-responder (instead of requiring slash command input) Move away from ephemeral messages to persistent messages Thanks for following along, cheers!\n","date":"2023-05-10T00:00:00Z","image":"https://kaonbytes.com/p/slack-ipam-helper/images/ipam-helper-front_hu0b4c396984346b48c89ef05d3f4ab66d_78125_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/slack-ipam-helper/","title":"Slack IPAM Helper"},{"content":"About I put together a system of components to perform automated perimeter scanning using readily available open source tools. Here is a guide on how you can do the same thing.\nThe system can accomplish the following:\nActively port scan your IP address space (please do not scan IP space that you do not control!) Store results in a structured database which can provide a snapshot of the state of the perimeter. Perform programmatic checks comparing current state vs desired state and report on discrepancies. Why Port Scan? As network admins, we manage large pools of address space which host various services and expose ports for internal and external connections. In general, these available services are documented, firewalled and compliant to standards.\nHowever, human error, misconfigurations, or vendor patches may cause undesired services to be exposed on the network. Port scanning (or port knocking) is one way to determine if these unintended services exist.\nPort scanning is just one tool we can use to mitigate vulnerabilities. If we follow the swiss cheese model of layered security, adding this check is beneficial in overall network security posture.\nThe Tools Scanning Engine \u0026mdash; Nmap\nNmap is a free and open source utility for network discovery and security auditing. Framework \u0026mdash; IVRE\nIVRE provides the framework to store, view and manipulate the scan results from nmap Source of Truth \u0026mdash; Netbox\nNetbox is used to ingest targets to be scanned and as a reference for desired state The End Result Generated Report IVRE View Database Project Code Github Repo of the code can be found here\nSystem Implementation Execution Timeline Scheduled cronjob starts pscan_spawn_jobs.py and does the following: Makes an API call to netbox to grab all prefixes and IP addresses tagged for scanning Spawns two nmap jobs per result (one TCP scan and one UDP scan) NMAP Scans run in the background on Network Targets and output the data as .XML files saved locally pscan_db_job.sh is kicked off which does the following: Initializes the IVRE Web View Imports the newest scans into the IVRE Database Creates two new web views in IVRE - Last Week\u0026rsquo;s Scans and This Week\u0026rsquo;s Scans export_recent_changes.py and print_recent_scan.py kickoff which compare the scan data vs the desired data in netbox create_email_report.py ingests the data and generates an email report to be sent Nmap Options Explained pscan_spawn_jobs.py import ipaddress import requests import subprocess import time from datetime import datetime, timedelta from netutils.ip import netmask_to_cidr ### start by killing all nmap processes (if they are running) subprocess.Popen([\u0026#39;killall\u0026#39;, \u0026#39;-9\u0026#39;, \u0026#39;nmap\u0026#39;]) ## this is the tag used in netbox to classify prefixes and ips to scan nmap_tag = \u0026#34;nmap_scanning\u0026#34; netbox_url = \u0026#39;https://netbox\u0026#39; netbox_token = \u0026#39;token\u0026#39; netbox_headers = {\u0026#39;Authorization\u0026#39;: \u0026#34;Token {}\u0026#34;.format(netbox_token)} # writing date files date_parser = datetime.now() date_stamp = date_parser.strftime(\u0026#34;%Y-%m-%d\u0026#34;) prev_date_parser = datetime.now() + timedelta(days=-7) prev_date_stamp = prev_date_parser.strftime(\u0026#34;%Y-%m-%d\u0026#34;) current_scan_fh = open(\u0026#34;/opt/ivre/ivre-opt/current_scan_date.txt\u0026#34;, \u0026#34;w\u0026#34;) previous_scan_fh = open(\u0026#34;/opt/ivre/ivre-opt/previous_scan_date.txt\u0026#34;, \u0026#34;w\u0026#34;) current_scan_fh.write(date_stamp) current_scan_fh.close() previous_scan_fh.write(prev_date_stamp) previous_scan_fh.close() # end writing date files # nmap args to use for TCP Scans nmap_base_args = \u0026#34;nohup nmap --log-errors --open -T3 -Pn -sS --top-ports 2000 -oX /opt/ivre/ivre-share/\u0026#34;+date_stamp+\u0026#34;_tcp_\u0026#34; # nmap args to use for UDP Scans nmap_base_args_udp = \u0026#34;nohup nmap --log-errors --open -T3 -Pn -sU --top-ports 250 -oX /opt/ivre/ivre-share/\u0026#34;+date_stamp+\u0026#34;_udp_\u0026#34; # netbox api calls to grab all ip addresses and prefixes tagged with nmap_scanning tag netbox_api_ip_addr = requests.get(netbox_url+\u0026#34;/api/ipam/ip-addresses/?limit=0\u0026amp;tag=\u0026#34;+nmap_tag, headers=netbox_headers, verify=False).json() netbox_api_ip_prefix = requests.get(netbox_url+\u0026#34;/api/ipam/prefixes/?limit=0\u0026amp;tag=\u0026#34;+nmap_tag, headers=netbox_headers, verify=False).json() # function to take in address like 172.3.3.0/25 and return 172_3_3_0 and netmask 25 def explode_address_elements(address_to_scan): exploded_elements_dict = {} exploded_elements_dict[\u0026#39;address_to_scan_exploded\u0026#39;] = str( ipaddress.IPv4Interface(address_to_scan).ip).replace(\u0026#39;.\u0026#39;, \u0026#39;_\u0026#39;) exploded_elements_dict[\u0026#39;address_to_scan_mask\u0026#39;] = str( netmask_to_cidr((ipaddress.IPv4Interface(address_to_scan).netmask))) return exploded_elements_dict # function to spawn one nmap job per ingested prefix or ip address def spawn_nmap_jobs(address_to_scan, address_to_scan_exploded, address_to_scan_mask, is_prefix): if is_prefix: ## spawn tcp scans nmap_raw_cmd = nmap_base_args+address_to_scan_exploded+\u0026#34;_\u0026#34; + address_to_scan_mask+\u0026#34;.xml \u0026#34;+address_to_scan subprocess.Popen(nmap_raw_cmd.split()) ##spawn udp scans nmap_raw_cmd = nmap_base_args_udp+address_to_scan_exploded+\u0026#34;_\u0026#34; + address_to_scan_mask+\u0026#34;.xml \u0026#34;+address_to_scan subprocess.Popen(nmap_raw_cmd.split()) else: ## spawn tcp scans nmap_raw_cmd = nmap_base_args+address_to_scan_exploded+\u0026#34;_\u0026#34; + address_to_scan_mask+\u0026#34;.xml \u0026#34;+address_to_scan.split(\u0026#34;/\u0026#34;)[0] subprocess.Popen(nmap_raw_cmd.split()) ## spawn udp scans nmap_raw_cmd = nmap_base_args_udp+address_to_scan_exploded+\u0026#34;_\u0026#34; + address_to_scan_mask+\u0026#34;.xml \u0026#34;+address_to_scan.split(\u0026#34;/\u0026#34;)[0] subprocess.Popen(nmap_raw_cmd.split()) # iterate through netbox ip address query results and start the nmap job for ip_addr in netbox_api_ip_addr[\u0026#39;results\u0026#39;]: ip_elements = explode_address_elements(ip_addr[\u0026#39;address\u0026#39;]) spawn_nmap_jobs(ip_addr[\u0026#39;address\u0026#39;], ip_elements[\u0026#39;address_to_scan_exploded\u0026#39;], ip_elements[\u0026#39;address_to_scan_mask\u0026#39;], is_prefix=False) time.sleep(.3) # iterate through netbox prefix query results and start the nmap job for prefix in netbox_api_ip_prefix[\u0026#39;results\u0026#39;]: ip_elements = explode_address_elements(prefix[\u0026#39;prefix\u0026#39;]) spawn_nmap_jobs(prefix[\u0026#39;prefix\u0026#39;], ip_elements[\u0026#39;address_to_scan_exploded\u0026#39;], ip_elements[\u0026#39;address_to_scan_mask\u0026#39;], is_prefix=True) time.sleep(.3) # all jobs should be started now and script exits. With current params the jobs will complete in 24 hours # example output of psaux www ### ## nmap --log-errors --open -T4 -Pn -sS --top-ports 2000 -oX /opt/ivre/ivre-share/2023-02-08_tcp_172_1_1_0_23.xml 172.1.1.0/23 ## nmap --log-errors --open -T4 -Pn -sU --top-ports 250 -oX /opt/ivre/ivre-share/2023-02-08_udp_172_2_2_0_23.xml 172.2.2.0/23 ps auxww Example list of spawned nmap processes during the scanning window.\nxml results file Processing the Results Import Scan Results into IVRE DB #!/bin/bash # Bash file to run on ivre-client container with command # docker exec -i ivreclient /bin/bash /ivre-opt/pscan_db_job.sh # 1. Purge the view database (web gui view) to start fresh # 2. Import the latest scan into scan database # 3. Create two new views -- 1. View of previous scan 2. View of Newest Current Scan /bin/sh -c \u0026#34;echo yes | ivre view --purgedb\u0026#34; /bin/sh -c \u0026#34;ivre scan2db --open-ports -s SCAN_SERVER -c pscan_$(cat \u0026#34;/ivre-opt/current_scan_date.txt\u0026#34;) /ivre-share/$(cat \u0026#34;/ivre-opt/current_scan_date.txt\u0026#34;)*.xml\u0026#34; /bin/sh -c \u0026#34;ivre db2view --no-merge nmap --category pscan_$(cat \u0026#34;/ivre-opt/previous_scan_date.txt\u0026#34;)\u0026#34; /bin/sh -c \u0026#34;ivre db2view --no-merge nmap --category pscan_$(cat \u0026#34;/ivre-opt/current_scan_date.txt\u0026#34;)\u0026#34; Show most recent scan results from DB ### Script run on ivre client container to print out latest scan results from mongo DB ### Calls the mongo query: list(db.view.get_ips_ports(flt=db.view.searchcategory(\u0026#34;pscan_\u0026#34;+current_date))[0]) ### Converts results to a json dict exported to recent_scan_results.json so future script can read it for email generation from ivre.db import db import json file1 = open(\u0026#34;/ivre-opt/current_scan_date.txt\u0026#34;, \u0026#34;r\u0026#34;) current_date = file1.readline() file1.close result_list = [] # Data to be written scan_export = list(db.view.get_ips_ports(flt=db.view.searchcategory(\u0026#34;pscan_\u0026#34;+current_date))[0]) for result in scan_export: result_list.append({\u0026#34;ip_address\u0026#34;: result[\u0026#39;addr\u0026#39;], \u0026#34;ports\u0026#34;: result[\u0026#39;ports\u0026#39;]}) json_obj = json.dumps(result_list, indent=4) with open(\u0026#34;/ivre-opt/recent_scan_result.json\u0026#34;, \u0026#34;w\u0026#34;) as outfile: outfile.write(json_obj) Show what\u0026rsquo;s changed from last scan ### Script run on ivre client container to print out latest diff of changes from previous scan to current scan ### Calls the mongo query: list(db.view.diff_categories(category1=\u0026#34;1\u0026#34;,category2=\u0026#34;2\u0026#34;,include_both_open=False)) ### Converts results to a json dict exported to whats_changed.json so future script can read it for email generation from ivre.db import db import json file1 = open(\u0026#34;/ivre-opt/current_scan_date.txt\u0026#34;, \u0026#34;r\u0026#34;) current_date = file1.readline().strip() file1.close file2 = open(\u0026#34;/ivre-opt/previous_scan_date.txt\u0026#34;, \u0026#34;r\u0026#34;) previous_date = file2.readline().strip() file2.close result_list = [] value_map = {\u0026#34;-1\u0026#34;: \u0026#34;No Longer Detected\u0026#34;, \u0026#34;0\u0026#34;: \u0026#34;Still Open: Open in previous scan \u0026#34;+previous_date+\u0026#34; and open in current scan \u0026#34;+current_date, \u0026#34;1\u0026#34;: \u0026#34;Newly Detected\u0026#34;} # Data to be written scan_export = list(db.view.diff_categories(category1=\u0026#34;pscan_\u0026#34;+previous_date,category2=\u0026#34;pscan_\u0026#34;+current_date,include_both_open=False)) for result in scan_export: result_list.append({\u0026#34;ip_address\u0026#34;: result[\u0026#39;addr\u0026#39;], \u0026#34;protocol\u0026#34;: result[\u0026#39;proto\u0026#39;], \u0026#34;port\u0026#34;: result[\u0026#39;port\u0026#39;], \u0026#34;status\u0026#34;: value_map[str(result[\u0026#39;value\u0026#39;])]}) sorted_result_list = sorted(result_list, key=lambda d: d[\u0026#39;status\u0026#39;]) json_obj = json.dumps(sorted_result_list, indent=2) with open(\u0026#34;/ivre-opt/whats_changed.json\u0026#34;, \u0026#34;w\u0026#34;) as outfile: outfile.write(json_obj) Generating Report Data Comparing Results to Desired State We can create a custom field in Netbox to declare the known open ports per IP Address. Example: Creating the report import requests import json import datetime from jinja2 import Template import smtplib ### global vars netbox_url = \u0026#39;https://netbox\u0026#39; netbox_token = \u0026#39;token\u0026#39; netbox_headers = {\u0026#39;Authorization\u0026#39;: \u0026#34;Token {}\u0026#34;.format(netbox_token)} date_parser = datetime.datetime.now() date_stamp = date_parser.strftime(\u0026#34;%Y-%m-%d\u0026#34;) recent_scan_filename = \u0026#34;/opt/ivre/ivre-opt/recent_scan_result.json\u0026#34; whats_changed_filename = \u0026#34;/opt/ivre/ivre-opt/whats_changed.json\u0026#34; output_list_no_reason = [] output_list_all = [] ### functions ### Search if IP + Port Combo exists in netbox custom field called \u0026#39;known open ports ### example 172.1.1.5--\u0026gt;443 see if exists in data structure: # {\u0026#39;known_open_ports\u0026#39;: [ # {\u0026#39;port\u0026#39;: 443, \u0026#39;reason\u0026#39;: \u0026#39;web server\u0026#39;, \u0026#39;protocol\u0026#39;: \u0026#39;tcp\u0026#39; # } # ] # } def get_netbox_known_ports_justification(ip_address,port): ipam_api_call = requests.get(netbox_url+\u0026#34;/api/ipam/ip-addresses/?address=\u0026#34;+ip_address, headers=netbox_headers, verify=False).json() if len(ipam_api_call[\u0026#39;results\u0026#39;]) \u0026gt; 0: known_ports_list = ipam_api_call[\u0026#39;results\u0026#39;][0].get(\u0026#39;custom_fields\u0026#39;) if known_ports_list[\u0026#39;known_open_ports\u0026#39;] is not None: for known_port in known_ports_list[\u0026#39;known_open_ports\u0026#39;]: if known_port.get(\u0026#39;port\u0026#39;) == port: return known_port.get(\u0026#39;reason\u0026#39;) else: return \u0026#34;No Justification\u0026#34; else: return \u0026#34;No Justification\u0026#34; return \u0026#34;No Justification\u0026#34; def get_recent_scan(filename): f = open(filename) return json.load(f) ### Gather Data recent_scan_results_dict = get_recent_scan(recent_scan_filename) whats_changed_dict = get_recent_scan(whats_changed_filename) ### Iterate through every entry of most recent scan and compare each entry to netbox \u0026#34;known open port\u0026#34; field for entry in recent_scan_results_dict: for discovered_port in entry[\u0026#39;ports\u0026#39;]: ## ignore udp open/filtered state as its not truly open if discovered_port[\u0026#39;state_state\u0026#39;] == \u0026#39;open\u0026#39;: reason = get_netbox_known_ports_justification(entry[\u0026#39;ip_address\u0026#39;],discovered_port[\u0026#39;port\u0026#39;]) else: continue if reason == \u0026#34;No Justification\u0026#34;: output_list_no_reason.append({\u0026#34;IP Address\u0026#34;: entry[\u0026#39;ip_address\u0026#39;],\u0026#34;Open Port\u0026#34;: discovered_port[\u0026#39;port\u0026#39;], \u0026#34;Reason\u0026#34;: reason}) else: output_list_all.append({\u0026#34;IP Address\u0026#34;: entry[\u0026#39;ip_address\u0026#39;],\u0026#34;Open Port\u0026#34;: discovered_port[\u0026#39;port\u0026#39;], \u0026#34;Reason\u0026#34;: reason}) ### Build Email Template with open(\u0026#39;/opt/ivre/ivre-opt/email_template.j2\u0026#39;) as f: rendered = Template(f.read()).render(date_stamp=date_stamp,output_list_no_reason=output_list_no_reason,output_list_all=output_list_all,whats_changed_dict=whats_changed_dict) message = \u0026#39;Subject: {}\\n\\n{}\u0026#39;.format(\u0026#34;Network Perimeter Port Scanner Summary\u0026#34;, rendered) s = smtplib.SMTP(\u0026#39;smtp-server\u0026#39;) s.sendmail(\u0026#39;pscanner@domain\u0026#39;, [\u0026#39;notify@domain\u0026#39;], message) s.sendmail() s.quit() Email Template - Jinja2 Network Perimeter Port Scanner Summary Web View: https://pscanner/ IP Addresses and Prefixes Tagged for Scanning: https://netbox/ --------------------------------------------------------------------------------------------------- List of Open Services that are not justified: {% for item in output_list_no_reason %} IP --\u0026gt; {{ item[\u0026#39;IP Address\u0026#39;] }} | Open_Port --\u0026gt; {{ item[\u0026#39;Open Port\u0026#39;] }} | Reason --\u0026gt; {{ item[\u0026#39;Reason\u0026#39;] }} {% endfor %} List of Changes From Previous Scan to Current Scan: {% for item in whats_changed_dict %} IP --\u0026gt; {{ item[\u0026#39;ip_address\u0026#39;] }} | Proto --\u0026gt; {{ item[\u0026#39;protocol\u0026#39;] }} | Port --\u0026gt; {{ item[\u0026#39;port\u0026#39;] }} | Change --\u0026gt; {{ item[\u0026#39;status\u0026#39;] }} {% endfor %} List of Current Open Services with Justifications: {% for item in output_list_all %} IP --\u0026gt; {{ item[\u0026#39;IP Address\u0026#39;] }} | Open_Port --\u0026gt; {{ item[\u0026#39;Open Port\u0026#39;] }} | Reason --\u0026gt; {{ item[\u0026#39;Reason\u0026#39;] }} {% endfor %} --------------------------------------------------------------------------------------------------- Date: \u0026#34;{{ date_stamp }}\u0026#34; End I never know how to end these posts, so for the small percentage of readers who stuck it out til the end, heres a gif of a robot dancing. Thanks for reading!\n","date":"2023-05-05T00:00:00Z","image":"https://kaonbytes.com/p/perimeter-scanner/images/perimeter-scanner-front_huda0cd6a7d1bad9baa374d7c905f6581c_150841_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/perimeter-scanner/","title":"Perimeter Scanner"},{"content":"The first step in creating a self-healing network is to implement a golden configuration standard. Ensuring that your network devices are in compliance with intended parameters improves operational overhead, limits security risks and potential outages caused by human error.\nHowever, this is a complex problem to solve because a universal one-size-fits-all configuration is not common. Many factors cause slight changes in configurations such as:\ndevice hardware firmware region specific parameters non-idempotent secret keys \u0026hellip; just to name a few. So how can we do it?\nPurpose In this blog post, I will share a technical architecture that I have deployed in production on hundreds of network nodes across dozens of sites and global regions.\nThe Steps Populate Source of Truth (Netbox) with desired configuration context to be loaded as dynamic inventory host variables Use weighted config context to populate region or site wide specific configuration Use custom fields to populate device specific configuration (if needed) Populate Secret Passwords in Ansible Automation Platform Credentials library, Ansible Vault, HashiCorp Vault or some other secure method to be called later Build custom python module to create idempotent secret keys per host. Securely encrypted keys which can be re-produced on each host (i.e not change on every run) Render intended configurations with Ansible Jinja2 Templating Deploy desired configuration with Ansible In this case, we are deploying on Juniper Devices using the junos_config module with the replace argument Use the junos_config option commit_check to perform Compliance Checking. i.e. Dry Run of changes to be made The End Result The Code Github Repo can be found here - Golden Config Compliance Engine\nThe Juniper Configuration Template\nThe Ansible Playbook\nRunning the playbook Example of out-of-compliance run:\n(py38env_ansible) [root@net config-templates]# ansible-playbook -i netbox_inventory.yml --e \u0026#34;@extra-vars.yml\u0026#34; --diff junos-golden-config-engine.yml -u kaon -k SSH password: PLAY [Standardize System Params on Junos Config] *** TASK [Lookup Timezone Info by Device Site] *** ok: [TEST01 -\u0026gt; localhost] TASK [Set Time Zone Fact to be used in jinja2 template] *** ok: [TEST01] TASK [Grab switch software fw version from Netbox and set as integer Value] *** ok: [TEST01] TASK [get snmp info] *** ok: [TEST01] TASK [parse snmp engine id] *** ok: [TEST01] TASK [generate snmp_9key] *** ok: [TEST01] TASK [generate tacacs_key] *** ok: [TEST01] TASK [generate root login key per device] *** ok: [TEST01] TASK [Template Lookup and Config Generation] *** ok: [TEST01 -\u0026gt; localhost] TASK [Load Standard System Parameters to Juniper Device] *** [edit system login] - user bob-super-admin { - uid 2006; - class super-user; - authentication { - encrypted-password \u0026#34;$6$Hyk9Ve....\u0026#34;; ## SECRET-DATA - } - } [edit system services ssh] - root-login allow; + root-login deny; [edit system] + processes { + web-management disable; + } changed: [TEST01] TASK [debug] *** fatal: [TEST01]: FAILED! =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;Change detected, action needed\u0026#34; } PLAY RECAP *** TEST01 : ok=10 changed=1 unreachable=0 failed=1 skipped=1 rescued=0 ignored=0 Example of in-compliance run:\n(py38env_ansible) [root@net config-templates]# ansible-playbook -i netbox_inventory.yml --e \u0026#34;@extra-vars.yml\u0026#34; --diff junos-golden-config-engine.yml -u kaon -k SSH password: PLAY [Standardize System Params on Junos Config] *** TASK [Lookup Timezone Info by Device Site] *** ok: [TEST01 -\u0026gt; localhost] TASK [Set Time Zone Fact to be used in jinja2 template] *** ok: [TEST01] TASK [Grab switch software fw version from Netbox and set as integer Value] *** ok: [TEST01] TASK [get snmp info] **** ok: [TEST01] TASK [parse snmp engine id] *** ok: [TEST01] TASK [generate snmp_9key] *** ok: [TEST01] TASK [generate tacacs_key] *** ok: [TEST01] TASK [generate root login key per device] *** ok: [TEST01] TASK [Template Lookup and Config Generation] *** ok: [TEST01 -\u0026gt; localhost] TASK [Load Standard System Parameters to Juniper Device] *** ok: [TEST01] PLAY RECAP *** TTEST01 : ok=10 changed=0 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 (py38env_ansible) [root@server config-templates]# The Nuts and Bolts How-To Declaring Intended State In order to build a golden configuration we have to start somewhere. In this tutorial we are using Juniper Junos devices. Junos devices facilitate automation because:\nThey offer the ability to load all or parts of configuration as hierarchical text. Allows for replace functionality in which the operating system determines configuration differences and inserts/deletes lines as needed Has the ability to check commit which performs a dry run load of the intended configuration without making the change A simple Junos Configuration .txt file could look something like this:\nreplace: system { host-name \u0026lt;some_name\u0026gt;; services { ssh { \u0026lt;some_more config\u0026gt; } time-zone \u0026lt;some_tz\u0026gt;; name-server { \u0026lt;some_ns1\u0026gt; \u0026lt;some_ns2\u0026gt; \u0026lt;some_ns3\u0026gt; } syslog { \u0026lt;some_more config\u0026gt; } ntp { \u0026lt;some_server1\u0026gt; \u0026lt;some_server2\u0026gt; } login { \u0026lt;some_user_config\u0026gt; } } replace: snmp { description \u0026lt;some_description\u0026gt;; location \u0026lt;some_location\u0026gt;; contact \u0026lt;some_email\u0026gt;; v3 { \u0026lt;some_more config\u0026gt; } ... \u0026lt;some_more config\u0026gt; If we manually populate all the above values with correct Junos syntax and load it via the CLI then the OS will replace the system section and snmp section with the declared .txt file.\nBut how can we turn the above snippet into a golden config to apply across multiple devices?\nSource Of Truth - Netbox First we need to model our devices into a centralized location. Netbox is a good tool to use for this, but other products also exist such as Nautobot, Solarwinds, InfoBlox etc\nIn a previous blog post I discussed how we can use Netbox as our dynamic Ansible inventory. When we populate our Ansible Inventory from Netbox, we can also pull in important host variables. These variables can be used to populate our Juniper System Configuration text file.\nExample of Ansible hostvars from Netbox inventory:\n# ansible-inventory -i ../netbox/netbox_inventory.yml --host TEST01\n{ \u0026#34;ansible_connection\u0026#34;: \u0026#34;netconf\u0026#34;, \u0026#34;ansible_host\u0026#34;: \u0026#34;10.X.X.X\u0026#34;, \u0026#34;ansible_network_os\u0026#34;: \u0026#34;junos\u0026#34;, \u0026#34;custom_fields\u0026#34;: { \u0026#34;ansible_connection\u0026#34;: \u0026#34;netconf\u0026#34;, \u0026#34;code_version\u0026#34;: \u0026#34;20\u0026#34;, }, \u0026#34;device_roles\u0026#34;: [ \u0026#34;access_switch\u0026#34; ], \u0026#34;device_site\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;device_types\u0026#34;: [ \u0026#34;ex4300-48p\u0026#34; ], \u0026#34;is_virtual\u0026#34;: false, \u0026#34;local_context_data\u0026#34;: [ null ], \u0026#34;locations\u0026#34;: [], \u0026#34;manufacturers\u0026#34;: [ \u0026#34;juniper\u0026#34; ], \u0026#34;platforms\u0026#34;: [ \u0026#34;junos\u0026#34; ], \u0026#34;primary_ip4\u0026#34;: \u0026#34;10.x.x.x\u0026#34;, \u0026#34;regions\u0026#34;: [ \u0026#34;americas\u0026#34; ], \u0026#34;services\u0026#34;: [], \u0026#34;sites\u0026#34;: [ \u0026#34;xxx\u0026#34; ], \u0026#34;status\u0026#34;: { \u0026#34;label\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;active\u0026#34; }, \u0026#34;tags\u0026#34;: [ \u0026#34;3\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;1\u0026#34; ] } Config Context In addition to device specific variables, we can render large blocks of configuration with the Context Data feature of Netbox. Context data can be applied to a class of devices on different conditions such as site, region, role, etc\nFor example, all switches in Europe should use an EU set of NTP servers Weighting can be used to apply multiple layers of context data.\nFor example, all devices should use DNS server 10.10.10.10 (default weight of 1000) However, devices in SITE A should use DNS server 11.11.11.11 (set weight 2000) Example Config Context Declaration:\nExample of Rendered Content Per Device:\nIn the above screenshot, we have populated the following parameters for this individual device:\nDomain Name Name Servers NTP Servers TACACS Servers The rendered config can now be sent to the Ansible Inventory as variable called hostvars[inventory_hostname]['config_context']\nHandling Credentials And Secret Passwords Do not commit secret keys to github :)\nInstead, let\u0026rsquo;s inject our passwords as credentials in Ansible Automation Platform and call them as variables during runtime execution. As an alternative, we can also do this with Hashicorp Vault using the hashi_vault lookup module.\nExample Passwords to Store:\nExample of injecting passwords into your playbook build:\nPasswords can be called as variables inside your playbook. The variable name is determined by the credential_type that was created earlier. For example, the snmp credential type passes a variable called snmp_cred which can be referenced as such:\nvars: # lookup secrets from ansible credentials tacacs_pw: \u0026#34;{{ tacacs_pw }}\u0026#34; root_pw: \u0026#34;{{ root_pw }}\u0026#34; snmp_cred: \u0026#34;{{ snmp_cred }}\u0026#34; ssh_key: \u0026#34;{{ ssh_key }}\u0026#34; tasks: Creating Idempotent Secret Keys from Passwords We could load these secrets directly into our Jinja2 template. Junos would accept the rendered configuration and internally convert the passwords to encrypted keys in its running configuration. However, if we did this, then each subsequent run would register a change (i.e. not idempotent)\nHere\u0026rsquo;s how we can solve this problem:\nThe $9$ Key\nJuniper uses $9$ encoded keys (similar to cisco type 7) for shared secrets which run on services such as BGP, TACACS and SNMP. The user inputs a plain text password and the OS obfuscates it, when needed the OS is able to decrypt the key. Simple, lets find a python library that encodes the $9$ algorithm, input our snmpv3 plaintext password and call it a day. WRONG :( unfortunately, the OS accepts the key but snmpv3 authentication breaks: RFC3414 describes how snmpv3 creates a localized unique key per device. We need to hash the local engine ID + password + other. The below C code is provided as an example in the RFC. Luckily there\u0026rsquo;s a python library for that\u0026hellip; The snmpv3_hashgen library takes the input of your plain text pw + your engineID and outputs a key. Great, lets take this key and put it in our $9$ function. so localized key \u0026ndash;\u0026gt; $9$ function = $9$\u0026hellip;key to be used for junos config. The $9$ key is compatible and works in junos snmpv3 config block. However there\u0026rsquo;s still a problem, the key is different every time it\u0026rsquo;s generated. This breaks repeatability. We can fix this by setting a static salt in the $9$ generator function. In this case, a static salt does not make us less secure because each $9$ key is already unique per device (because each localized snmp key is unique due to the engineID being different). Now we have a working process. Lets put all this into a python filter and implement it. Custom python filter to be imported in the ansible playbook can be found here. Snippet of the function: def gen_snmp_9key(engine_id, snmp_pass): \u0026#34;\u0026#34;\u0026#34; Takes two inputs (engine_id and snmp password). Hashes the password and engine id together to create a localized key. Then encodes the key with the juniper $9$ algorithm to be used in junos configuration \u0026#34;\u0026#34;\u0026#34; hash = Hashgen.algs[\u0026#34;sha1\u0026#34;] Kul_auth = Hashgen.derive_msg(snmp_pass, engine_id, hash) Kul_priv = Hashgen.derive_msg(snmp_pass, engine_id, hash) localized_key = hash(Kul_auth) snmp_9key = encrypt(localized_key) return snmp_9key Results: first run to standardize the config. Registers a change (as expected): Subsequent runs. No change. But confirms we are in compliance and standardization: Other $9$ Keys (Not SNMP)\nFor other System $9$ keys (such as TACACS, RADIUS, etc). We can use the device hostname as our salt. Using the device hostname means we always generate a repeatable $9$ key per device but not the same key across all devices.\nIn the same python filter, we have some modified functions here which take two inputs\nDevice Hostname Password and spit out an encrypted unique but repeatable $9$ key:\n## Create a unique but repeatable decimal number based on string (i.e. hostname) ## Set range of decimal with modulo (i.e to get a number between 0 to 64 set modulo 65) ## Example: input hostname \u0026#39;switch-123\u0026#39; output: 56 def get_unique_decimal_from_string(str_to_hash,modulo): str_to_hash_hex = md5(str_to_hash.encode(\u0026#34;utf\u0026#34;)).hexdigest() str_to_hash_dec = int(str_to_hash_hex, 16) return str_to_hash_dec % modulo ## Create a unique, \u0026#39;random\u0026#39;, single character salt value from the string hostname. Repeatable. ## Example: input hostname \u0026#39;switch-123\u0026#39; output: \u0026#39;V\u0026#39; (this is a character from the FAMILY structure above) def get_deterministic_salt_from_hostname(hostname): # returns a number from 0-64 str_to_hash_dec = get_unique_decimal_from_string(hostname,65) # calls index of list NUM_ALPHA[0-64] and returns 1 character return NUM_ALPHA[str_to_hash_dec] ## Create a unique, \u0026#39;random\u0026#39;, 0-3 character bookend-salt value from the string hostname. Repeatable. ## Example: input hostname \u0026#39;switch-123\u0026#39; and salt \u0026#39;V\u0026#39; Output \u0026#39;9wY\u0026#39;. ## The length of the output is determined by the index of the salt in the EXTRA dictionary (i.e. EXTRA[salt]) def get_deterministic_bookend_from_hostname(hostname,salt): end_piece = \u0026#39;\u0026#39; for count in range(EXTRA[salt]): bookend_letter_dec = get_unique_decimal_from_string(hostname[count],65) end_piece += NUM_ALPHA[bookend_letter_dec] return end_piece def encrypt_with_hostname(value, hostname): salt = get_deterministic_salt_from_hostname(hostname) rand = get_deterministic_bookend_from_hostname(hostname,salt) position = 0 previous = salt crypted = MAGIC + salt + rand for x in value: encode = ENCODING[position % len(ENCODING)] crypted += __gap_encode(x, previous, encode) previous = crypted[-1] position += 1 return crypted $1$ - $5$ and $6$ Keys\nThese keys are MUCH simpler to implement. 1,5,6 keys use standard hashing algorithms.\nLegacy JUNOS devices use $1$ keys (Hash Algo md5) Modern JUNOS devices use $5$ keys (sha256) or $6$ (sha512) We can use the built in ansible module password_hash to generate these keys. However, to create an idempotent key, we need to use the device hostname as the random seed. Example below:\n- name: generate root login key per device set_fact: root_key: \u0026#34;{{ root_pw | password_hash(hash_algo, 65534 | random(seed=inventory_hostname) | string) }}\u0026#34; The root password is encrypted with the sha512 algorithm + device hostname as the random seed. The resulting string is a unique but repeatable $6$ key per device.\nJinja2 Template After all that work, we have our base config, variables and secret keys ready. We can mash them all together in a jinja2 template. Here is the template:\nreplace: system { root-authentication { encrypted-password \u0026#34;{{ root_key }}\u0026#34;; ## SECRET-DATA } authentication-order [ password tacplus ]; host-name {{ inventory_hostname }}; services { ssh { root-login deny; protocol-version v2; max-sessions-per-connection 32; {% if code_version|int \u0026gt;= 19 %} sftp-server; {% endif %} } netconf { ssh; } } domain-name {{ hostvars[inventory_hostname][\u0026#39;config_context\u0026#39;][0][\u0026#39;domain-name\u0026#39;] }}; time-zone {{ timezone }}; name-server { {% for item in hostvars[inventory_hostname][\u0026#39;config_context\u0026#39;][0][\u0026#39;name-servers\u0026#39;] %} {{ item }}; {% endfor %} } syslog { user * { any emergency; } {% for item in hostvars[inventory_hostname][\u0026#39;config_context\u0026#39;][0][\u0026#39;log-agg\u0026#39;] %} host {{ item }} { any any; } {% endfor %} file messages { any warning; authorization info; } source-address {{ ansible_host }}; } ntp { server {{ hostvars[inventory_hostname][\u0026#39;config_context\u0026#39;][0][\u0026#39;ntp-servers\u0026#39;][0] }} prefer; {% for item in hostvars[inventory_hostname][\u0026#39;config_context\u0026#39;][0][\u0026#39;ntp-servers\u0026#39;][1:] %} server {{ item }}; {% endfor %} source-address {{ ansible_host }}; } login { user ansible { class super-user; authentication { ssh-rsa \u0026#34;{{ ssh_pub_key }}\u0026#34;; ## SECRET-DATA } } tacplus-server { {{ hostvars[inventory_hostname][\u0026#39;config_context\u0026#39;][0][\u0026#39;tacplus-servers\u0026#39;][0] }} { port 49; secret \u0026#34;{{ tacacs_key }}\u0026#34;; ## SECRET-DATA source-address {{ ansible_host }}; } {{ hostvars[inventory_hostname][\u0026#39;config_context\u0026#39;][0][\u0026#39;tacplus-servers\u0026#39;][1] }} { port 49; secret \u0026#34;{{ tacacs_key }}\u0026#34;; ## SECRET-DATA source-address {{ ansible_host }}; } } accounting { events [ login interactive-commands ]; destination { tacplus { server { {{ hostvars[inventory_hostname][\u0026#39;config_context\u0026#39;][0][\u0026#39;tacplus-servers\u0026#39;][0] }} { port 49; secret \u0026#34;{{ tacacs_key }}\u0026#34;; ## SECRET-DATA timeout 1; source-address {{ ansible_host }}; } {{ hostvars[inventory_hostname][\u0026#39;config_context\u0026#39;][0][\u0026#39;tacplus-servers\u0026#39;][1] }} { port 49; secret \u0026#34;{{ tacacs_key }}\u0026#34;; ## SECRET-DATA timeout 1; source-address {{ ansible_host }}; } } } } } } replace: snmp { description {{ inventory_hostname }}; location {{ device_site }}; contact \u0026#34;email@email.com\u0026#34;; v3 { usm { local-engine { user usernameforsnmp { authentication-sha { authentication-key \u0026#34;{{ snmp_9key }}\u0026#34;; ## SECRET-DATA } privacy-aes128 { privacy-key \u0026#34;{{ snmp_9key }}\u0026#34;; ## SECRET-DATA } } } } vacm { security-to-group { security-model usm { security-name usernameforsnmp { group groupforsnmp; } } } access { group groupforsnmp { default-context-prefix { security-model usm { security-level privacy { read-view view-all; } } } } } } } view view-all { oid 1 include; } } Notice the python-style logic using IF and FOR Loops above. We can use conditional logic to add/subtract configuration.\nIn the above example, if running Junos version 19 or greater add the sftp line. For loops can also be used to implement multiple entries (like NTP servers).\nGenerating Config from Template\nThe Ansible module template can take the jinja2 text file and inject the variables to spit out a ready-to-go junos.conf file\n- name: Template Lookup and Config Generation # generate template of system params. Variables are filled in by Netbox (from hostvars pulled to inventory) template: src: \u0026#34;{{ template_name }}.j2\u0026#34; # create a temp candidate config file to be loaded in next task dest: \u0026#34;{{ inventory_hostname }}.conf\u0026#34; delegate_to: localhost # this task will always generate a change, don\u0026#39;t need to see it. changed_when: false Loading configuration using Junos_Config module:\n- name: Load Standard System Parameters to Juniper Device junipernetworks.junos.junos_config: # if TRUE this will be a DRY RUN (no changes) check_commit: \u0026#34;{{ check_commit }}\u0026#34; src_format: text # we use the replace flag to overwrite specific blocks of config update: replace # temp candidate config file created in previous task src: \u0026#34;{{ inventory_hostname }}.conf\u0026#34; register: result Juniper Commit_Check Feature In the above ansible task, notice the flag check_commit.\nThis is a junos-specific function that we can use to perform dry runs (when set to TRUE). During execution, we can inject the check_commit variable (True or False) to decide if our playbook should be running in enforcing mode or compliance mode.\nFirst Run and Recurring Enforcement A potential workflow could look like this:\nStandardize all configurations with the initial run of changes. Subsequent playbook runs are scheduled regularly in check_commit mode If something has changed (either in the field or in the Source-of-truth) which does not match the standard then fail the playbook and send a notification. Engineer manually assesses the situation and determines if the playbook should be run again in enforcing mode (revert the change) or update the source of truth. We can use a hack to fail the playbook and trigger a notification with this code:\n# if running in compliance mode, error out when change is detected and action is needed to be taken. i.e alert on non-compliance - debug: msg: \u0026#34;Change detected, action needed\u0026#34; when: - result.changed - check_commit failed_when: - result.changed - check_commit In AAP (or Ansible Tower), we can set notifications on failed playbooks via email, slack or other methods.\nIf running directly from ansible CLI yu can use callback plugins to send notifications.\nFinal Thoughts Standardizing configurations across a distributed network is an essential pillar of Network Automation. I hope this guide is helpful to anyone looking for a similar solution. My solution can be improved upon by adding more vendors to the mix (Cisco, Arista, etc) and adding additional config blocks such as BGP, Interface configurations, Prefix Maps etc\u0026hellip;.but that\u0026rsquo;s another post :)\nIf you have any questions or need clarifications feel free to leave a comment below or contact me. Thanks for reading!\n","date":"2022-11-15T00:00:00Z","image":"https://kaonbytes.com/p/golden-configuration-deployment-with-ansible-and-netbox/golden-config-front_hu3972f5f5b3bf12cabffe22f75756d981_1247419_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/golden-configuration-deployment-with-ansible-and-netbox/","title":"Golden Configuration Deployment with Ansible and Netbox"},{"content":"There are many commericial tools that can perform configuration backups of network gear (for example Solarwinds NCM). However, we can build an alternative tool using Ansible + a versioned Amazon AWS S3 Bucket + a daily scheduler.\nOverview In this guide, I will detail how we can build such a tool and meet the following requirments:\nDynamic Inventory Vendor neutral system Version Controlled Backups Notfication of successful and failed backups Health Check of the Backup Tool Currently Supported Vendors/OS: Opengear Aruba AOS Cisco ASA and IOS Fortinet Fortios Juniper Junos Citrix Netscaler Big-IP F5 Arista EOS A read-only recurring Ansible playbook, like the one we are discussing, is a great way to get started in network automation of your enterprise gear because:\nThe playbook does not make confuguration changes to your remote devices Your playbook has to touch every device in your network. That means Authentication, Connecticty, and Ansible Modules must all be correct on every run. Getting this all working the first time takes some effort but will pay off in future projects. Workflow Below is a diagram of the expected workflow operation of the ansible playbook.\nGrab inventory list from Netbox with devices tagged for backup. Call a specific Ansible task to perform system backup on each device Store all system backups locally and sync to a version controlled S3 Bucket Maintain a summary of failed or successful device backup actions and create report to Email Update local status.txt file to be polled by outside monitoring system The Solution Github Repo can be found here - Ansible Configuration Backup Manager\nRunning the playbook Here is an example output of running the playbook against 8 hosts each on a different network operating system. This playbook has been run on hundreds of network devices in production environments.\nCommand:\n[root@ncm]# ansible-playbook -i netbox_inventory.yml -e \u0026#34;var_hosts=aruba1:asa1:f51:nss1:fortios1:ios1:junos1:opengear1\u0026#34; ncm-engine.yml Output:\nPLAY [PLAY TO BACKUP NETWORK CONFIGURATIONS] ********** TASK [delete exisiting successful_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [delete exisiting failed_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [delete total_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [create successful_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [create failed_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [create total_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [update total_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [include_role : generate-backups] ********** TASK [generate-backups : include_tasks] ********** included: /root/ncm/roles/generate-backups/tasks/aos-backup-config.yml for aruba1 included: /root/ncm/roles/generate-backups/tasks/asa-backup-config.yml for asa1 included: /root/ncm/roles/generate-backups/tasks/big-ip-backup-config.yml for f51 included: /root/ncm/roles/generate-backups/tasks/citrix-backup-config.yml for nss1 included: /root/ncm/roles/generate-backups/tasks/fortios-backup-config.yml for fortios1 included: /root/ncm/roles/generate-backups/tasks/ios-backup-config.yml for ios1 included: /root/ncm/roles/generate-backups/tasks/junos-backup-config.yml for junos1 included: /root/ncm/roles/generate-backups/tasks/opengear-backup-config.yml for opengear1 TASK [generate-backups : grab and download aruba config] ********** ok: [aruba1] TASK [generate-backups : Save the backup information.] ********** changed: [aruba1 -\u0026gt; localhost] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [generate-backups : Backup ASA Device] ********** ok: [asa1] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [asa1 -\u0026gt; localhost] TASK [generate-backups : grab and download big-ip config] ********** ok: [f51 -\u0026gt; localhost] TASK [generate-backups : Save the backup information.] ********** changed: [f51 -\u0026gt; localhost] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [f51 -\u0026gt; localhost] TASK [generate-backups : grab and download citrix config] ********** ok: [nss1] TASK [generate-backups : Save the backup information.] ********** changed: [nss1 -\u0026gt; localhost] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [nss1 -\u0026gt; localhost] TASK [generate-backups : Backup Fortigate Device] ********** ok: [fortios1] TASK [generate-backups : Save the backup information.] ********** changed: [fortios1] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [fortios1 -\u0026gt; localhost] TASK [generate-backups : Backup IOS Device] ***************** ok: [ios1] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [ios1 -\u0026gt; localhost] TASK [generate-backups : grab and download junos config] ********** ok: [junos1] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [junos1 -\u0026gt; localhost] TASK [generate-backups : grab and download opengear config] ********** changed: [opengear1] TASK [generate-backups : Save the backup information.] ********** ok: [opengear1 -\u0026gt; localhost] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [opengear1 -\u0026gt; localhost] PLAY [SYNC NETWORK CONFIGURATIONS TO S3 BUCKET] ************* TASK [delete exisiting s3_sync file] ************************ changed: [localhost] TASK [create s3_sync file] ********************************** changed: [localhost] TASK [include_role : sync-to-s3] **************************** TASK [sync-to-s3 : Sync to S3] ****************************** changed: [localhost] TASK [sync-to-s3 : Add status to s3 state file] ************* changed: [localhost] PLAY [Build Email Template] ********************************* TASK [lookup file successful_hosts.txt] ********************* ok: [localhost] TASK [lookup file failed_hosts.txt] ************************* ok: [localhost] TASK [lookup file total_hosts.txt] ************************** ok: [localhost] TASK [lookup file failed_s3.txt] **************************** ok: [localhost] TASK [Generate Backup State File] *************************** changed: [localhost] TASK [send email] ******************************************* ok: [localhost] PLAY RECAP ************************************************** asa1 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 opengear1 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 nss1 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 fortios1 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 localhost : ok=10 changed=5 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 aruba1 : ok=11 changed=9 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ios1 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 junos1 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 f51 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 [root@ncm]# Email Summary Report The playbook generates an Email Template which gets sent out as the final task.\nThe Netbox Dynamic Inventory Ansible calls the netbox API and grabs a list of inventory-to-be-backed-up by filtering for a specified tag.\nAdditionally, we pass other important data to the inventory such as:\nansible_network_os ansible_connection netbox_inventory.yml Source\n--- plugin: netbox.netbox.nb_inventory api_endpoint: \u0026#34;\u0026lt;netbox-url\u0026gt;\u0026#34; token: token validate_certs: false config_context: false compose: ansible_network_os: platform.slug ansible_connection: custom_fields.ansible_connection device_query_filters: - status: \u0026#39;active\u0026#39; - tag: \u0026#39;ncm_backup\u0026#39; Example of Dynamic Inventory Pull [root@ncm]# ansible-inventory -i netbox_inventory.yml --host junos1 { \u0026#34;ansible_connection\u0026#34;: \u0026#34;netconf\u0026#34;, \u0026#34;ansible_host\u0026#34;: \u0026#34;10.10.10.10\u0026#34;, \u0026#34;ansible_network_os\u0026#34;: \u0026#34;junos\u0026#34;, \u0026#34;custom_fields\u0026#34;: { \u0026#34;ansible_connection\u0026#34;: \u0026#34;netconf\u0026#34;, \u0026#34;code_version\u0026#34;: \u0026#34;xxx\u0026#34; }, \u0026#34;device_roles\u0026#34;: [ \u0026#34;access_switch\u0026#34; ], \u0026#34;device_types\u0026#34;: [ \u0026#34;ex4300-48p\u0026#34; ], \u0026#34;is_virtual\u0026#34;: false, \u0026#34;local_context_data\u0026#34;: [ null ], \u0026#34;locations\u0026#34;: [], \u0026#34;manufacturers\u0026#34;: [ \u0026#34;juniper\u0026#34; ], \u0026#34;platforms\u0026#34;: [ \u0026#34;junos\u0026#34; ], \u0026#34;primary_ip4\u0026#34;: \u0026#34;10.10.10.10\u0026#34;, \u0026#34;regions\u0026#34;: [ \u0026#34;americas\u0026#34; ], \u0026#34;services\u0026#34;: [], \u0026#34;sites\u0026#34;: [ \u0026#34;xxx1\u0026#34; ], \u0026#34;status\u0026#34;: { \u0026#34;label\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;active\u0026#34; }, \u0026#34;tags\u0026#34;: [ \u0026#34;ncm_backup\u0026#34;, ] } The Main File source\n--- - name: \u0026#34;PLAY TO BACKUP NETWORK CONFIGURATIONS\u0026#34; hosts: \u0026#34;{{ var_hosts }}\u0026#34; roles: - role: arubanetworks.aos_wlan_role vars: network_backup_dir: \u0026#34;/root/configuration-backup-manager/config-backups/\u0026#34; net_backup_filename: \u0026#34;{{ inventory_hostname }}-{{ ansible_host }}-config.txt\u0026#34; tasks: - name: delete exisiting successful_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/successful_hosts.txt state: absent run_once: True delegate_to: localhost - name: delete exisiting failed_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/failed_hosts.txt state: absent run_once: True delegate_to: localhost - name: delete total_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/total_hosts.txt state: absent run_once: True delegate_to: localhost - name: create successful_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/successful_hosts.txt state: touch run_once: True delegate_to: localhost - name: create failed_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/failed_hosts.txt state: touch run_once: True delegate_to: localhost - name: create total_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/total_hosts.txt state: touch run_once: True delegate_to: localhost - name: update total_hosts file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/total_hosts.txt line: \u0026#34;{{ groups[\u0026#39;all\u0026#39;] | length }}\u0026#34; run_once: True delegate_to: localhost - include_role: name: generate-backups - name: \u0026#34;SYNC NETWORK CONFIGURATIONS TO S3 BUCKET\u0026#34; hosts: localhost vars: network_backup_dir: \u0026#34;/root/configuration-backup-manager/config-backups/\u0026#34; net_backup_filename: \u0026#34;{{ inventory_hostname }}-{{ ansible_host }}-config.txt\u0026#34; tasks: - name: delete exisiting s3_sync file ansible.builtin.file: path: /root/configuration-backup-manager/templates/failed_s3.txt state: absent run_once: True - name: create s3_sync file ansible.builtin.file: path: /root/configuration-backup-manager/templates/failed_s3.txt state: touch run_once: True - include_role: name: sync-to-s3 - name: \u0026#34;Build Email Template\u0026#34; hosts: localhost tasks: - name: \u0026#34;lookup file successful_hosts.txt\u0026#34; set_fact: success_data: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/configuration-backup-manager/templates/successful_hosts.txt\u0026#39;).splitlines() }}\u0026#34; - name: \u0026#34;lookup file failed_hosts.txt\u0026#34; set_fact: failed_data: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/configuration-backup-manager/templates/failed_hosts.txt\u0026#39;).splitlines() }}\u0026#34; - name: \u0026#34;lookup file total_hosts.txt\u0026#34; set_fact: total_data: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/configuration-backup-manager/templates/total_hosts.txt\u0026#39;) }}\u0026#34; - name: \u0026#34;lookup file failed_s3.txt\u0026#34; set_fact: s3_error: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/configuration-backup-manager/templates/failed_s3.txt\u0026#39;).splitlines() }}\u0026#34; - name: Generate Backup State File template: src: \u0026#34;/root/configuration-backup-manager/templates/backup_state.j2\u0026#34; dest: \u0026#34;/root/configuration-backup-manager/templates/backup_state.txt\u0026#34; - name: send email mail: host: localhost port: 25 sender: \u0026#39;\u0026lt;email\u0026gt;\u0026#39; to: \u0026#39;\u0026lt;email\u0026gt;\u0026#39; subject: \u0026#39;Ansible NCM Job Completion\u0026#39; body: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/configuration-backup-manager/templates/backup_state.txt\u0026#39;)}}\u0026#34; The above playbook performs the following actions:\nInitializes some local flat text files called successful_hosts \u0026ndash; failed_hosts \u0026ndash; total_hosts \u0026ndash; failed_s3.txt The playbook performs writes to these files to keep track of stats per host to generate a report later on Uses include_role to include the generate-backups folder of tasks. The main.yml file in the generate-backups is how we call each backup task based on vendor OS Sync the local config-backups directory to s3 using the community.aws.s3_sync library Builds and sends an email report using a jinja2 template and the previously mentioned text files as variables The Backup Tasks tasks/main.yml One line include_task which uses the vendor OS to call the desired backup task. Source can be found here\n--- - include_tasks: \u0026#34;{{ role_path }}/tasks/{{ ansible_network_os }}-backup-config.yml\u0026#34; Cisco IOS The below playbook calls the Cisco ios_config library to backup the device and register the output locally.\nWe use ansible_connection: network_cli (as defined in netbox inventory) and plain user and password which can be defined as an enviornment variable or through some more secure method (Ansible Vault, Hashicorp Vault, etc) if desired.\n--- - name: IOS CISCO block: - name: Backup IOS Device vars: ansible_user: \u0026#34;username\u0026#34; ansible_password: pwd ios_config: backup: yes backup_options: filename: \u0026#34;{{ net_backup_filename }}\u0026#34; dir_path: \u0026#34;{{ network_backup_dir }}\u0026#34; register: backupinfo - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: backupinfo is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Cisco ASA Similarly, the ASA Playbook uses the asa_config library.\n--- - name: ASA CISCO block: - name: Backup ASA Device vars: ansible_user: \u0026#34;username\u0026#34; ansible_password: pwd asa_config: backup: yes backup_options: filename: \u0026#34;{{ net_backup_filename }}\u0026#34; dir_path: \u0026#34;{{ network_backup_dir }}\u0026#34; register: backupinfo - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: backupinfo is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Juniper Junos For Juniper devices, we use the junipernetworks.junos library and the netconf ansible_connection on port 930\n--- - name: JUNOS block: - name: grab and download junos config vars: ansible_user: un ansible_ssh_private_key_file: key # ansible_connection: netconf junipernetworks.junos.junos_config: backup: yes backup_options: filename: \u0026#34;{{ net_backup_filename }}\u0026#34; dir_path: \u0026#34;{{ network_backup_dir }}\u0026#34; register: config_output - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: config_output is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Aruba AOS For Aruba AOS devices we use the HTTPAPI over port 4343\n--- - name: ARUBA block: - name: grab and download aruba config vars: ansible_httpapi_port: 4343 ansible_httpapi_validate_certs: False ansible_httpapi_use_ssl: True ansible_user: username ansible_password: pwd aos_show_command: command: show running-config register: aos_output - name: Save the backup information. copy: content: \u0026#39;{{ aos_output.msg._data[0] }}\u0026#39; dest: \u0026#34;{{ network_backup_dir }}/{{ net_backup_filename }}\u0026#34; delegate_to: localhost - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: aos_output is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Fortinet FortiOS Fortinet devices are also connected via the ansible_httpapi over port 443 using the fortinet.fortios library\n--- - name: FORTIOS block: - name: Backup Fortigate Device vars: ansible_httpapi_use_ssl: yes ansible_httpapi_validate_certs: no ansible_httpapi_port: 443 ansible_user: \u0026#34;username\u0026#34; ansible_password: pwd # ansible_connection: httpapi fortinet.fortios.fortios_monitor_fact: selector: \u0026#39;system_config_backup\u0026#39; vdom: \u0026#39;root\u0026#39; params: scope: \u0026#39;global\u0026#39; register: backupinfo - name: Save the backup information. copy: content: \u0026#39;{{ backupinfo.meta.raw }}\u0026#39; dest: \u0026#34;{{ network_backup_dir }}/{{ net_backup_filename }}\u0026#34; - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: backupinfo is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Citrix Netscaler For Citrix Netscaler devices we can use any basic cli_command library. In this case, I set the network_os to be vyos because I did not want to interfere with the cisco IOS devices.\n--- - name: CITRIX NETSCALER block: - name: grab and download citrix config vars: ansible_user: \u0026lt;un\u0026gt; ansible_password: \u0026lt;pwd\u0026gt; ansible_network_os: vyos cli_command: command: show ns runningConfig register: citrix_output - name: Save the backup information. copy: content: \u0026#39;{{ citrix_output.stdout_lines | to_nice_json }}\u0026#39; dest: \u0026#34;{{ network_backup_dir }}/{{ net_backup_filename }}\u0026#34; delegate_to: localhost - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: citrix_output is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Big IP F5 The BIG IP F5 task uses a legacy library running as ansible_connection local\n--- - name: F5 BIG-IP block: - name: grab and download big-ip config bigip_command: commands: - show running-config provider: server: \u0026#34;{{ ansible_host }}\u0026#34; password: pwd user: un validate_certs: no transport: cli register: bigip_result delegate_to: localhost - name: Save the backup information. copy: content: \u0026#39;{{ bigip_result.stdout_lines | to_nice_json }}\u0026#39; dest: \u0026#34;{{ network_backup_dir }}/{{ net_backup_filename }}\u0026#34; delegate_to: localhost - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: bigip_result is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 OpenGear For OpenGear devices we can run a raw SSH command and register the output.\n--- - name: OPENGEAR block: - name: grab and download opengear config vars: ansible_port: 22 ansible_user: un ansible_password: pwd raw: config -g config register: output_opengear_config - name: Save the backup information. copy: content: \u0026#39;{{ output_opengear_config.stdout_lines | to_nice_json }}\u0026#39; dest: \u0026#34;{{ network_backup_dir }}/{{ net_backup_filename }}\u0026#34; delegate_to: localhost - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: output_opengear_config delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Arista EOS Arista Devices use the arista.eos.eos_config library over httpapi port 443\n--- - name: ARISTA EOS block: - name: grab and download ARISTA config vars: ansible_httpapi_port: 443 ansible_httpapi_validate_certs: False ansible_httpapi_use_ssl: True ansible_become: True ansible_user: username ansible_password: pwd arista.eos.eos_config: backup: yes backup_options: filename: \u0026#34;{{ net_backup_filename }}\u0026#34; dir_path: \u0026#34;{{ network_backup_dir }}\u0026#34; register: arista_backupinfo - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: arista_backupinfo is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Syncing to S3 After all the backup files are saved in a local directory. We can sync the entire directory to an S3 bucket. The S3 bucket has versioning enbaled, so we can keep track of changes over a period of time.\n--- - name: block: - name: Sync to S3 community.aws.s3_sync: bucket: \u0026lt;bucket_name\u0026gt; key_prefix: config-backups file_root: /root/configuration-backup-manager/config-backups/ register: result_s3 - name: Add status to s3 state file ansible.builtin.lineinfile: path: templates/failed_s3.txt line: \u0026#34;S3 SYNC SUCCESSFUL\u0026#34; when: result_s3 is defined rescue: - name: Add error state to file ansible.builtin.lineinfile: path: templates/failed_s3.txt line: \u0026#34;ERROR S3 SYNC FAILED\u0026#34; Example of a backed up versioned config Notifications and Health Checking Jinja2 Email Template Jinja2 Template\nAnsible Configuration Backup Manager Date: \u0026#34;{{ lookup(\u0026#39;pipe\u0026#39;,\u0026#39;date\u0026#39;) }}\u0026#34; Errored Backups: {{ failed_data | length }} Successful Backups: {{ success_data | length }} Total Devices Tagged for Backup: {{ total_data }} S3 Sync Status: {{ s3_error }} ___________________________________________________________________________ Location of All Backups can be found on AWS S3: s3://\u0026lt;bucket-name\u0026gt;/config-backups/ List of Devices Tagged for Nightly Backup: https://\u0026lt;netbox-url\u0026gt;/dcim/devices/?tag=ncm_backup Execution Server Location: ssh://\u0026lt;server\u0026gt; Directory: /root/configuration-backup-manager/ncm-engine.yml GitHub Repo: https://github.com/\u0026lt;repo\u0026gt; PRTG Sensor Of Backup State: https://\u0026lt;url\u0026gt;/sensor.htm?id=... Documentation: https://\u0026lt;url\u0026gt; ___________________________________________________________________________ Failed Device Names: {# new line #} {{ failed_data | to_nice_yaml(indent=2) }} Successful Device Names: {# new line #} {{ success_data | to_nice_yaml(indent=2) }} Creating a Status Page We can use a python script to tell us if the backup process has worked as expected.\nCheck if the latest report is more than 24 hours old Check if error count is \u0026gt; 0 Check if successfull hosts + errored hosts matches total hosts If any of the above conditions fail, update a local web-hosted server file to be FALSE\nOur Network Monitoring System can poll this file and alert us if its in a FAILED state\nHere\u0026rsquo;s the code:\nimport os import datetime as dt import re backup_state_file_path = \u0026#39;/root/configuration-backup-manager/backup_state.txt\u0026#39; def is_file_current(filename): try: today = dt.datetime.now().date() filetime = dt.datetime.fromtimestamp(os.path.getmtime(filename)) if filetime.date() == today: return True else: return False except Exception as e: return False def is_no_errors_count(filename): try: pattern = re.compile(\u0026#34;Errored Backups: (\\\\d)\u0026#34;) for line in open(filename): for match in re.finditer(pattern, line): if match.group(1) == \u0026#39;0\u0026#39;: return True else: return False return False except Exception as e: return False def is_no_s3_errors(filename): try: pattern = re.compile(\u0026#34;(S3 SYNC SUCCESSFUL)\u0026#34;) for line in open(filename): for match in re.finditer(pattern, line): if match.group(1) == \u0026#39;S3 SYNC SUCCESSFUL\u0026#39;: return True else: return False return False except Exception as e: return False def is_total_match(filename): try: pattern1 = re.compile(\u0026#34;Errored Backups: (\\\\d)\u0026#34;) for line in open(filename): for match in re.finditer(pattern1, line): total_errors = match.group(1) pattern2 = re.compile(\u0026#34;Successful Backups: (\\\\d)\u0026#34;) for line in open(filename): for match in re.finditer(pattern2, line): total_success = match.group(1) pattern3 = re.compile(\u0026#34;Total Devices Tagged for Backup: (\\\\d)\u0026#34;) for line in open(filename): for match in re.finditer(pattern3, line): total_hosts = match.group(1) if int(total_errors) + int(total_hosts) == int(total_success): return True else: return False except Exception as e: return False def main(): backup_status = is_file_current(backup_state_file_path) and is_no_errors_count(backup_state_file_path) and is_no_s3_errors(backup_state_file_path) and is_total_match(backup_state_file_path) if os.path.exists(\u0026#34;/usr/share/nginx/html/backup_status.txt\u0026#34;): os.remove(\u0026#34;/usr/share/nginx/html/backup_status.txt\u0026#34;) f = open(\u0026#34;/usr/share/nginx/html/backup_status.txt\u0026#34;, \u0026#34;a\u0026#34;) f.write(str(backup_status)) f.close() if __name__ == \u0026#34;__main__\u0026#34;: main() Scheduling Daily schedules can be setup with a crontab job. The first task runs the ansible playbook and the 2nd task (1 hour later) runs the python code status check.\n[root@ncmncm]# crontab -l MAILTO=\u0026#34;kaon\u0026#34; 0 7 * * * /usr/local/bin/ansible-playbook -i /root/ncm/netbox_inventory.yml -e \u0026#34;var_hosts=all\u0026#34; /root/ncm/ncm-engine.yml 0 8 * * * /usr/bin/python3 /root/ncm/backup_status_nms.py Final Thoughts The system may seem complex, but hopefully I have broken it down into enough small chunks to be understandable. Feel free to contact me on Twitter with any questions. Thanks for reading.\n","date":"2022-08-23T00:00:00Z","image":"https://kaonbytes.com/p/ansible-configuration-backup-manager/ansible-network-config-front_hu12225e3f406182c91634608ad7cb2c64_1239508_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/ansible-configuration-backup-manager/","title":"Ansible Configuration Backup Manager"},{"content":"In this guide I will walk through how to deploy BGPAlerter as a service in Amazon (AWS) as an ECS/Fargate container using a Github, Terraform, Drone Pipeline\nWhat is BGPAlerter? BGPAlerter is a well-engineered and useful tool built by Massimo Candela which allows you to monitor, in real-time, important BGP updates to your desired list of Prefixes and ASNs on the public internet.\nThe BGPAlerter application connects to public BGP data from these sources.\nThe user edits prefix.yml and config.yml files to setup desired monitoring and alerting/notifications.\nHere is a Super High Level overview of the flow:\nLive RIPE Feed Example\nHere is an example of the Live RIPE Feed of BGP Updates Why should we use Infrastructure as Code? Network Engineers love tools, but oftentimes, the tools that Network Engineers deploy turn into bespoke pet projects that are hard to keep in sync, maintain or document.\nInfrastructure as code (IaC) helps alleviate these problems because it: Allows for version controlled configuration changes Self-documents (for the most part) Creates an easy-to-follow process for updating and maintaining the tool application and system environment The Solution High Level Diagram Drone Process The Code Github Repo of the Code can be found here This repo and blog post are not meant to be \u0026ldquo;plug-and-play\u0026rdquo; ready, there are some parts of the infrastructure omitted (such as the VPC, Route53 etc) The key learning takeaways of this guide are how to format the drone.yml, Dockerfile, ecr.tf and ecs.tf files Credit goes to the devops folks at my org from whom I borrowed much of the terraform code and made my own tweaks as needed prefix.yml The source file that BGPAlerter reads of which prefixes and ASNs to monitor\n1.1.1.0/23: description: ABCDEFG asn: - 111111 ignoreMorespecifics: false ignore: false group: noc 2.2.2.0/24: description: HIJKLMNOP asn: - 22222 ignoreMorespecifics: false ignore: false group: noc 3.3.3.0/21: description: QRSTUVWXYZ asn: - 33333 ignoreMorespecifics: false ignore: false group: noc options: monitorASns: \u0026#39;111111\u0026#39;: group: noc upstreams: - 55555 downstreams: [] generate: asnList: - \u0026#39;111111\u0026#39; - \u0026#39;22222\u0026#39; - \u0026#39;33333\u0026#39; exclude: [] excludeDelegated: false prefixes: null monitoredASes: true historical: true group: noc Dockerfile Instructions that Drone uses to build your docker image. This image is later pushed to AWS ECR\n### Pull latest bgpalerter container from nttgin docker hub FROM nttgin/bgpalerter:latest ### Copy our own prefix.yml file and config file to the docker image ### When drone builds this image and pushes it to ECR, it will contain our files (not the defaults) COPY bgpalerter/YOUR_ASN_PREFIX_LIST.yml /opt/bgpalerter/volume/YOUR_ASN_PREFIX_LIST.yml COPY bgpalerter/config.yml /opt/bgpalerter/volume/config.yml ### Slack webhook should not be committed in code. This script replaces the slackwebhook in config.yml with a tf var COPY bgpalerter/slack-token-replace.sh /opt/bgpalerter/volume/slack-token-replace.sh ### open port 8011 for status checks EXPOSE 8011 Drone File The Drone file are the steps that drone takes to build the infrastructure. This file consists of two sections\nFirst, build the docker image and push it to ECR Second, build the AWS Fargate infra from the terraform directory --- ### First pipeline creates the docker image from Dockerfile and pushes to your ECR repo kind: pipeline name: ecr workspace: path: XXX steps: ### Drone pushes to ECR/bgpalerter and tags as latest + adds commit_sha ### The commit_sha is important because its how we force Fargate to always rebuild/redeploy the container on changes - name: publish-image-local image: plugins/ecr settings: registry: xxx.dkr.ecr.us-east-1.amazonaws.com repo: bgpalerter dockerfile: Dockerfile tag: - latest-${DRONE_COMMIT_SHA} environment: SHARED_CREDENTIALS_FILE: XXX ### When main changes, drone runs this pipeline when: event: push branch: main --- ### The 2nd drone pipeline runs terraform code to build aws infrastructure kind: pipeline name: default workspace: path: XXX steps: ### Format syntax check terraform files - name: fmt-module image: hashicorp/terraform commands: - cd terraform/modules/fargate-infra - terraform fmt -check -diff=true - name: fmt-aws image: hashicorp/terraform commands: - cd terraform/aws - terraform fmt -check -diff=true ### Build terraform plan file and sleep30 seconds. Giving admin time to abort - name: prd-terraform-plan image: hashicorp/terraform commands: - cd terraform/aws - terraform init -input=false - terraform plan -out=PLANFILE -input=false -lock-timeout=5m - sleep 30 environment: SHARED_CREDENTIALS_FILE: XXX ### Pass commit_sha so we can use it in terraform file to call our docker image TF_VAR_image_tag_digest: ${DRONE_COMMIT_SHA} when: event: push branch: main ### Deploy infra with terraform apply - name: prd-terraform-apply image: hashicorp/terraform commands: - cd terraform/aws - terraform apply -input=false -lock-timeout=5m PLANFILE environment: SHARED_CREDENTIALS_FILE: XXX ### Pass commit_sha so we can use it in terraform file to call our docker image TF_VAR_image_tag_digest: ${DRONE_COMMIT_SHA} ### When main changes, drone runs this pipeline when: event: push branch: main Slack Webhook Script Slack Webhooks are sensitive, and should not be committed to Github. This script grabs the slackwebhook from a tf env variable and uses sed to load it into confi.yml. This is done before the BGPAlerter service is started on the container.\n### This script is whats started on the docker container. First we replace the slackwebhook then start bgpalerter service ### Call SLACK_API_TOKEN from tf env variable which is stored in AWS Param Store ESCAPED_REPLACE=$(printf \u0026#39;%s\\n\u0026#39; \u0026#34;$SLACK_API_TOKEN\u0026#34; | sed -e \u0026#39;s/[\\/\u0026amp;]/\\\\\u0026amp;/g\u0026#39;) /bin/sed -i \u0026#34;s/slacktokentobereplaced/$ESCAPED_REPLACE/g\u0026#34; /opt/bgpalerter/volume/config.yml /bin/cat /opt/bgpalerter/volume/config.yml /usr/local/bin/npm run serve -- --d /opt/bgpalerter/volume/ ECR Terraform File Builds the Elastic Container Registry in AWS\n###This file creates the ECR repository for the docker image. resource \u0026#34;aws_ecr_repository\u0026#34; \u0026#34;app_image\u0026#34; { name = \u0026#34;bgpalerter\u0026#34; image_tag_mutability = \u0026#34;MUTABLE\u0026#34; image_scanning_configuration { scan_on_push = true } } ECS Terraform File Builds the Elastic Container Service in AWS\n### ECS File creates the Fargate service for the docker container ### Sets CPU and RAM ### Creates the bgpalerter task definition and runs it resource \u0026#34;aws_ecs_cluster\u0026#34; \u0026#34;bgpalerter-fargate\u0026#34; { name = local.svc_name capacity_providers = [\u0026#34;FARGATE_SPOT\u0026#34;] default_capacity_provider_strategy { capacity_provider = \u0026#34;FARGATE_SPOT\u0026#34; } tags = var.common_tags } // ECS service resource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;bgpalerter-svc\u0026#34; { name = local.svc_name cluster = aws_ecs_cluster.bgpalerter-fargate.id task_definition = aws_ecs_task_definition.task_definition.arn desired_count = 1 launch_type = \u0026#34;FARGATE\u0026#34; network_configuration { subnets = xxx security_groups = [xxx] } load_balancer { target_group_arn = xxx container_name = xxx container_port = xxx } tags = xxx } ### Create BGPAlerter Task resource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;task_definition\u0026#34; { family = local.svc_name requires_compatibilities = [\u0026#34;FARGATE\u0026#34;] network_mode = \u0026#34;awsvpc\u0026#34; execution_role_arn = xxx task_role_arn = xxx ### CPU and Memory cpu = 512 memory = 1024 tags = xxx container_definitions = jsonencode([ { name = local.svc_name image = \u0026#34;${var.image_name}${var.image_tag_digest}\u0026#34; essential = true ### Call the shell script when the image starts. This replaces the slack webhook and starts the service entryPoint = [\u0026#34;/bin/sh\u0026#34;, \u0026#34;/opt/bgpalerter/volume/slack-token-replace.sh\u0026#34;] secrets = [ { name = \u0026#34;SLACK_API_TOKEN\u0026#34; valueFrom = xxx } ] portMappings = [ { containerPort = 8011 hostPort = 8011 } ] logConfiguration = { logDriver = \u0026#34;awslogs\u0026#34; options = { awslogs-group = local.svc_name awslogs-region = \u0026#34;us-east-1\u0026#34; awslogs-stream-prefix = \u0026#34;bgpalerter\u0026#34; } } } ]) } The Results Drone Build Progress Deployed Infrastructure - Elastic Container Registry Deployed Infrastructure - Elastic Container Service and Fargate BGPAlerter Running and Status Notifications The Alert email provides a link to BGPLay which lets you view the event in a graphed form\nFinal Thoughts As a Network Engineer, I often find it hard to grasp abstract concepts of software development. In my experience, the best way to learn these concepts is to port over something you know (like a networking tool) into a new system. Hopefully, this post will help others in their journey. Thanks for reading!\n","date":"2022-08-12T00:00:00Z","image":"https://kaonbytes.com/p/bgpalerter-as-code-using-a-terraform-pipeline/front_huaf53bc8ed3209b5b56820218227b86f5_395056_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/bgpalerter-as-code-using-a-terraform-pipeline/","title":"BGPAlerter As Code Using A Terraform Pipeline"},{"content":"The most important pillar of Network Automation is a Source of Truth (SoT)\nAn accurate and well maintained SoT should provide:\nA list of device inventory (hosts) Connection parameters (IP/Hostname, connection protocol/port) Global, Regional, or Site specific parameters that are required to maintain desired network state The Chicken or the Egg Problem When dealing with a brownfield enterprise environment, we run into The Chicken or The Egg Problem at the beginning of our Network Automation journey.\nThe problem is, we need an accurate and well maintained SoT but this requires manual work to gather data from all currently running devices and keep that data up to date as the network changes. Let\u0026rsquo;s automate this! But wait, how can we automate this if we don\u0026rsquo;t have a SoT?\nThe Solution The answer is a staged approach (i.e. bootstrapping):\nChoose your SoT platform, I like Netbox Perform some manual work to get a basic static device list as an ansible inventory file Run Ansible fact gathering accross all of your inventory Use the Netbox Ansible Collection to populate netbox with key information of each device such as name, ip, connection profile, site, etc For all future playbooks, use the Netbox Ansible Dynamic Plugin to populate your ansible inventory. Maintain accurate state by repeatedly running (scheduled) fact gathering and keep netbox up to date via Ansible Diagram The Code Bootstrap Playbook \u0026ndash;\u0026gt; netbox-ansible-junos-bootstrap.yml\n### Playbook to bootstrap netbox inventory with a list of juniper devices provided by static inventory file # Uses ansible gather_facts to grab net_version, serial number and net_model # Also perform a dig to get a FQDN which we can use as device name instead of the inventory_name --- - name: PB to Bootstrap Netbox Inventory hosts: junosinv gather_facts: True vars: ansible_user: ansible_ssh_private_key_file: netbox_url: netbox_token: platform: \u0026#34;{{ ansible_network_os }}\u0026#34; site: device_role: \u0026#34;access_switch\u0026#34; tasks: - name: \u0026#34;Check if net_version exists\u0026#34; ### If ansible_facts does not provide net_version we manually fill it in as 111 set_fact: net_version: \u0026#34;111\u0026#34; when: ansible_facts[\u0026#39;net_version\u0026#39;] is undefined - name: \u0026#34;Assign net version\u0026#34; set_fact: net_version: \u0026#34;{{ ansible_facts[\u0026#39;net_version\u0026#39;] }}\u0026#34; when: ansible_facts[\u0026#39;net_version\u0026#39;] is defined ### Optional - name: \u0026#34;Resolve FQDN Hostname - perform DIG\u0026#34; ### Perform linux DIG command to get the reverse DNS record for the IP. THis will be our new hostname for netbox raw: \u0026#34;dig -x {{ ansible_host }} +short | sed -e \u0026#39;s/.$//\u0026#39;\u0026#34; register: dig_result delegate_to: localhost ### Optional - name: \u0026#34;TASK 11: Assign dig result to fqdn var\u0026#34; ### If Reverse DNS exists, trim whhite spaces and assing to var set_fact: fqdn: \u0026#34;{{ dig_result.stdout_lines[0] | trim}}\u0026#34; when: dig_result.stdout_lines[0] is defined ### Optional - name: \u0026#34;TASK 12: If no dig result, assign placeholder fqdn value\u0026#34; ### If no reverse DNS, then set a inventory hostname and IP as the hostname set_fact: fqdn: \u0026#34;{{ inventory_hostname }}-no-dns-{{ ansible_host }}\u0026#34; when: dig_result.stdout_lines[0] is undefined - name: \u0026#34;Add Device to NetBox\u0026#34; netbox.netbox.netbox_device: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: name: \u0026#34;{{ fqdn }}\u0026#34; device_type: \u0026#34;{{ ansible_facts[\u0026#39;net_model\u0026#39;] }}\u0026#34; platform: \u0026#34;{{ platform }}\u0026#34; serial: \u0026#34;{{ ansible_facts[\u0026#39;net_serialnum\u0026#39;] }}\u0026#34; site: \u0026#34;{{ site }}\u0026#34; device_role: \u0026#34;{{ device_role }}\u0026#34; custom_fields: code_version: \u0026#34;{{ net_version }}\u0026#34; state: present validate_certs: no delegate_to: localhost - name: \u0026#34;Add a new Interface called management_interface to device\u0026#34; ### this interface will be used as the primary IP and interface for the device netbox.netbox.netbox_device_interface: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: device: \u0026#34;{{ fqdn }}\u0026#34; name: Management_Interface type: other state: present validate_certs: no delegate_to: localhost - name: \u0026#34;Add IP address of ansible host to IPAM\u0026#34; netbox.netbox.netbox_ip_address: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: family: 4 address: \u0026#34;{{ ansible_host }}/32\u0026#34; status: active assigned_object: name: Management_Interface device: \u0026#34;{{ fqdn }}\u0026#34; state: present validate_certs: no delegate_to: localhost - name: \u0026#34;Assign ansible_host IP as the primary interface for the device\u0026#34; netbox.netbox.netbox_device: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: name: \u0026#34;{{ fqdn }}\u0026#34; device_type: \u0026#34;{{ ansible_facts[\u0026#39;net_model\u0026#39;] }}\u0026#34; platform: \u0026#34;{{ platform }}\u0026#34; serial: \u0026#34;{{ ansible_facts[\u0026#39;net_serialnum\u0026#39;] }}\u0026#34; status: Active primary_ip4: \u0026#34;{{ ansible_host }}/32\u0026#34; state: present validate_certs: no delegate_to: localhost The Process Let\u0026rsquo;s go through the step by step.\nThe example below will use Juniper Devices. Junos provides some key features to help with automation:\nConfiguration Structure (versioning, commit confirm, rollback options are super valuable) Ability to load configuration as text blocks with replace, override and merge options Return JSON values (i.e. Display JSON) Building Static Inventory File Bootstrap Inventory \u0026ndash;\u0026gt; bootstrap_inventory.ini\n[junosinv] switch1 ansible_host=10.10.10.1 switch2 ansible_host=10.10.10.2 switch3 ansible_host=10.10.10.3 [junosinv:vars] ansible_connection=netconf ansible_network_os=junos Running the bootstrap playbook Now that we have a basic inventory. We can run the above Bootstrap Playbook \u0026ndash;\u0026gt; netbox-ansible-junos-bootstrap.yml\nThe playbook will do the following:\nConnect to the remote juniper host and gather system facts Connect to the Netbox server and add a new device with the below data: name device_type (model) platform (OS) Serial Number Site Device role Code Version Creates a new Netbox IP Address (IPAM) entry with the ansible_host IP Creates a new interface for the device named \u0026ldquo;management_interface\u0026rdquo; Assigns the IP address to the device\u0026rsquo;s management_interface Ansible Result:\n(venv) [root]# ansible-playbook -i bootstrap_inventory.ini netbox-ansible-junos-bootstrap.yml PLAY [PB to Bootstrap Netbox Inventory] ****** [WARNING]: Ignoring timeout(10) for junos_facts TASK [Gathering Facts] *********************** [WARNING]: default value for `gather_subset` will be changed to `min` from `!config` v2.11 onwards ok: [switch1] TASK [Assign net version] ******************** ok: [switch1] TASK [Resolve FQDN Hostname - perform DIG] *** changed: [switch1 -\u0026gt; localhost] TASK [TASK 11: Assign dig result to fqdn var] * ok: [switch1] TASK [Add Device to NetBox] ***************** changed: [switch1 -\u0026gt; localhost] TASK [Add a new Interface called management_interface to device] * changed: [switch1 -\u0026gt; localhost] TASK [Add IP address of ansible host to IPAM] ****** changed: [switch1 -\u0026gt; localhost] TASK [Assign ansible_host IP as the primary interface for the device] * changed: [switch1 -\u0026gt; localhost] PLAY RECAP ************************************** switch1 : ok=8 changed=5 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 Netbox Result:\nDynamic Inventory Now that we have populated Netbox, we can use Netbox as our ansible inventory\nThe Netbox Dynamic Inventory Plugin can be used by creating an inventory file as follows:\n## Ansible Plugin file for dynamic inventory through netbox --- plugin: netbox.netbox.nb_inventory api_endpoint: # token: \u0026#34;{{ lookup(\u0026#39;env\u0026#39;,\u0026#39;NETBOX_API_KEY\u0026#39;) }}\u0026#34; validate_certs: false config_context: true compose: ansible_network_os: platform.slug ansible_connection: custom_fields.ansible_connection device_query_filters: - status: \u0026#39;active\u0026#39; - tag: \u0026#39;some_tag\u0026#39; To check if dynamic inventory is working, run ansible-inventory:\n# ansible-inventory -i ../netbox/netbox_inventory.yml --host TEST01\nResult:\n{ \u0026#34;ansible_connection\u0026#34;: \u0026#34;netconf\u0026#34;, \u0026#34;ansible_host\u0026#34;: \u0026#34;10.X.X.X\u0026#34;, \u0026#34;ansible_network_os\u0026#34;: \u0026#34;junos\u0026#34;, \u0026#34;custom_fields\u0026#34;: { \u0026#34;ansible_connection\u0026#34;: \u0026#34;netconf\u0026#34;, \u0026#34;code_version\u0026#34;: \u0026#34;20\u0026#34;, }, \u0026#34;device_roles\u0026#34;: [ \u0026#34;access_switch\u0026#34; ], \u0026#34;device_site\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;device_types\u0026#34;: [ \u0026#34;ex4300-48p\u0026#34; ], \u0026#34;is_virtual\u0026#34;: false, \u0026#34;local_context_data\u0026#34;: [ null ], \u0026#34;locations\u0026#34;: [], \u0026#34;manufacturers\u0026#34;: [ \u0026#34;juniper\u0026#34; ], \u0026#34;platforms\u0026#34;: [ \u0026#34;junos\u0026#34; ], \u0026#34;primary_ip4\u0026#34;: \u0026#34;10.x.x.x\u0026#34;, \u0026#34;regions\u0026#34;: [ \u0026#34;americas\u0026#34; ], \u0026#34;services\u0026#34;: [], \u0026#34;sites\u0026#34;: [ \u0026#34;xxx\u0026#34; ], \u0026#34;status\u0026#34;: { \u0026#34;label\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;active\u0026#34; }, \u0026#34;tags\u0026#34;: [ \u0026#34;3\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;1\u0026#34; ] } Schedule Playbook to Maintain System State Let\u0026rsquo;s regularly run a modified version of the bootstrap playbook to maintain network state.\nModified snippet:\n- name: \u0026#34;CONFIRM DEVICE TO NETBOX\u0026#34; netbox.netbox.netbox_device: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: name: \u0026#34;{{ inventory_hostname }}\u0026#34; device_type: \u0026#34;{{ ansible_facts[\u0026#39;net_model\u0026#39;] }}\u0026#34; serial: \u0026#34;{{ ansible_facts[\u0026#39;net_serialnum\u0026#39;] }}\u0026#34; custom_fields: code_version: \u0026#34;{{ net_version }}\u0026#34; ansible_connection: \u0026#34;{{ custom_fields[\u0026#39;ansible_connection\u0026#39;] }}\u0026#34; state: present validate_certs: no delegate_to: localhost The above task will only make changes to Netbox if something changes on the field device, such as the:\nModel number OS version Serial number Use Ansible Tower/AWX to schedule playbook execution daily, weekly or monthly.\nWrap Up I hope this guide was helpful in understanding how we can use a Source of truth to dynamically populate an Ansible Inventory. Feel free to comment below with any questions.\n","date":"2022-05-13T00:00:00Z","image":"https://kaonbytes.com/p/netbox-dynamic-inventory-for-ansible-as-a-feedback-loop/netbox-ansible-feedback_hu91008c89e08c35be24c68bc187db22be_244905_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/netbox-dynamic-inventory-for-ansible-as-a-feedback-loop/","title":"Netbox Dynamic Inventory for Ansible as a feedback loop"},{"content":"A common IT Workflow exists where a Cloud provider\u0026rsquo;s public IP ranges are added to a static firewall object list. The reasons why this is done include: Network Address Translation (NAT), Access Control (ACL) or logging.\nOften times, the network engineer is unaware when the Cloud Provider updates their IP ranges and thus the IT Workflow is no longer in compliance.\nWe can automate some of this process by setting up a job to continually check the Cloud IP ranges and alert us if a change is required.\nOverview The below process is specifically for Google Cloud IP Ranges and Fortinet Firewall Objects. However, with a few tweaks can be ported to other services such as Fastly and AWS as well as other firewall vendors supported by ansible collections.\nAnsible Code compare-google-ips-check-against-fortinet.yml\n### This playbook will compare the list of known IPv4 Google Cloud IP addresses against ### the a deployed \u0026#39;address_grp\u0026#39; object in a Fortigate Firewall ### First we grab the json data from https://www.gstatic.com/ipranges/goog.json and extract ipv4 addresses to a list ### Next we grab the firewall_addrgrp called \u0026#39;google_cdn\u0026#39; on the firewall and put it into a list ### We run a difference of list1 vs list2. If there is a difference, the PB will throw an error and Tower sends email --- - name: PB to compare Google Cloud IPs vs current \u0026#39;google_cdn\u0026#39; object in firewall hosts: firewall_host1 gather_facts: false vars: ### Fortinet specific vars for Ansible to connect - https://galaxy.ansible.com/fortinet/fortios ansible_python_interpreter: /usr/bin/python3 ansible_user: user ansible_password: password ansible_connection: httpapi ansible_httpapi_use_ssl: yes ansible_httpapi_validate_certs: no ansible_httpapi_port: 443 ansible_network_os: fortinet.fortios.fortios vdom: \u0026#34;root\u0026#34; ### Variable to store list of \u0026#39;google_cdn\u0026#39; ips as a list from the user firewall google_ip_list: [] ### Variable to store list of Google Cloud IP addresses we retrieve from the internet google_cdn_ipv4_list: [] ### Variable to store the list difference result missing_google_ips: [] tasks: ### GET request to retrieve the current json data of Google Cloud IP Ranges - name: Get all google cloud ip ranges as json result uri: url: \u0026#34;https://www.gstatic.com/ipranges/goog.json\u0026#34; method: GET validate_certs: no register: google_web_json_result ### Ansible will register a change here, we can ignore it. changed_when: false delegate_to: localhost ### Extract only IPv4 Addresses and add to a flat list - set_fact: google_cdn_ipv4_list: \u0026#34;{{ google_cdn_ipv4_list + [ item[\u0026#39;ipv4Prefix\u0026#39;] ] }}\u0026#34; loop: \u0026#34;{{ google_web_json_result.json.prefixes }}\u0026#34; when: item[\u0026#39;ipv4Prefix\u0026#39;] is defined changed_when: false delegate_to: localhost ### Hit the firewall once here to retrieve the object. This object does not contain IP/Mask info only names - name: Get google_cdn list of objects from firewall fortinet.fortios.fortios_configuration_fact: vdom: \u0026#34;{{ vdom }}\u0026#34; selector: \u0026#34;firewall_addrgrp\u0026#34; params: name: \u0026#34;google_cdn\u0026#34; register: google_networks_objects changed_when: false ### For each name in \u0026#39;google_cdn\u0026#39; object we ask the firewall to give us back the IP/Mask info. Many API hits here. - name: Iterate through every Google object and extract subnet info fortinet.fortios.fortios_configuration_fact: vdom: \u0026#34;{{ vdom }}\u0026#34; selector: \u0026#34;firewall_address\u0026#34; params: name: \u0026#34;{{ item.name }}\u0026#34; register: google_item loop: \u0026#34;{{ google_networks_objects.meta.results[0][\u0026#39;member\u0026#39;] }}\u0026#34; changed_when: false ### The returned IP and Subnet info is in form 10.10.10.10 255.255.255.0. These Filters translate that to 10.10.10.10/24 ### List is populated with all \u0026#39;google_cdn\u0026#39; IP/Mask in correct format for comparison - set_fact: google_ip_list: \u0026#34;{{ google_ip_list + [ item.meta.results[0].subnet | replace(\u0026#39; \u0026#39;,\u0026#39;/\u0026#39;) | ansible.netcommon.ipaddr ] }}\u0026#34; loop: \u0026#34;{{ google_item.results }}\u0026#34; changed_when: false delegate_to: localhost ### Use ansible difference filter to compare list1 to list2. It shows items that are in list1 but not in list2 - name: Show the difference in lists set_fact: missing_google_ips: \u0026#34;{{ google_cdn_ipv4_list | difference(google_ip_list) }}\u0026#34; changed_when: false delegate_to: localhost ### If an IP Subnet exists in the google cdn ipv4 list but not on the firewall \u0026#39;google_cdn object\u0026#39; then ### we fail the PB and Tower will send an email with the list - debug: msg: \u0026#34;List of missing Google Subnets that need to be added to Firewall: {{ missing_google_ips }}\u0026#34; failed_when: missing_google_ips | length\u0026gt;0 Results No Difference Detected: Difference Detected: Detailed Step-by-Step Let\u0026rsquo;s dive into the step-by-step\nGet Google Cloud IP Ranges Browsing to https://www.gstatic.com/ipranges/goog.json returns a JSON result of IP Prefixes.\nWe can programmatically access the JSON result and store all IPv4 prefixes in an ansible list called google_cdn_ipv4_list:\n### GET request to retrieve the current json data of Google Cloud IP Ranges - name: Get all google cloud ip ranges as json result uri: url: \u0026#34;https://www.gstatic.com/ipranges/goog.json\u0026#34; method: GET validate_certs: no register: google_web_json_result ### Ansible will register a change here, we can ignore it. changed_when: false delegate_to: localhost ### Extract only IPv4 Addresses and add to a flat list - set_fact: google_cdn_ipv4_list: \u0026#34;{{ google_cdn_ipv4_list + [ item[\u0026#39;ipv4Prefix\u0026#39;] ] }}\u0026#34; loop: \u0026#34;{{ google_web_json_result.json.prefixes }}\u0026#34; when: item[\u0026#39;ipv4Prefix\u0026#39;] is defined changed_when: false delegate_to: localhost We can run the above tasks and print the result. Below is a printout of each dataset: https://www.gstatic.com/ipranges/goog.json and google_cdn_ipv4_list\nHTTP Site Ansible GET Result Get Address Group from Fortigate We can use the ansible collection provided by Fortinet to gather facts.\nIn the below task we will retrieve the firewall address group named test_group_1\n- name: Get Fortigate Address Group fortinet.fortios.fortios_configuration_fact: vdom: \u0026#34;root\u0026#34; selector: \u0026#34;firewall_addrgrp\u0026#34; params: name: \u0026#34;test_group_1\u0026#34; register: google_networks_objects - debug: var: google_networks_objects.meta.results[0] Here is a comparison of the Address Group in the Fortigate GUI and the Ansible Result:\nHTTP Site Ansible Result:\nTASK [debug] ********************* ok: [localhost] =\u0026gt; { \u0026#34;google_networks_objects.meta.results[0]\u0026#34;: { \u0026#34;allow-routing\u0026#34;: \u0026#34;disable\u0026#34;, \u0026#34;color\u0026#34;: 0, \u0026#34;comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;exclude\u0026#34;: \u0026#34;disable\u0026#34;, \u0026#34;exclude-member\u0026#34;: [], \u0026#34;member\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;test_net_1\u0026#34;, \u0026#34;q_origin_key\u0026#34;: \u0026#34;test_net_1\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;test_net_2\u0026#34;, \u0026#34;q_origin_key\u0026#34;: \u0026#34;test_net_2\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;test_net_3\u0026#34;, \u0026#34;q_origin_key\u0026#34;: \u0026#34;test_net_3\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;test_group_1\u0026#34;, \u0026#34;q_origin_key\u0026#34;: \u0026#34;test_group_1\u0026#34;, \u0026#34;tagging\u0026#34;: [], \u0026#34;uuid\u0026#34;: \u0026#34;3ff6c462-c7f1-51ec-f405-2a6b59ee9591\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;enable\u0026#34; } } Get Each Member of Group The above ansible result gives us a list of address group member names. We have to query the firewall again for the IP address and subnet mask of each member.\nThis task will loop through each group member:\n- name: Iterate through every Google object and extract subnet info fortinet.fortios.fortios_configuration_fact: vdom: \u0026#34;{{ vdom }}\u0026#34; selector: \u0026#34;firewall_address\u0026#34; params: name: \u0026#34;{{ item.name }}\u0026#34; register: google_item loop: \u0026#34;{{ google_networks_objects.meta.results[0][\u0026#39;member\u0026#39;] }}\u0026#34; - debug: var: google_item Extract Subnet Info to List The returned results need to be parsed. The subnet field should be normalized to IP/MASK notation so that we can more easily run a diff later against the exisiting google cdn list that we grabbed earlier from the web.\nWe can parse each subnet field and transfrom it to IP/MASK notation with this task:\n- set_fact: google_ip_list: \u0026#34;{{ google_ip_list + [ item.meta.results[0].subnet | replace(\u0026#39; \u0026#39;,\u0026#39;/\u0026#39;) | ansible.netcommon.ipaddr ] }}\u0026#34; loop: \u0026#34;{{ google_item.results }}\u0026#34; changed_when: false delegate_to: localhost Now we have a list of all IPs from the Fortigate in a normalized view (Example Data):\nTASK [debug] ***************************** ok: [localhost] =\u0026gt; { \u0026#34;google_ip_list\u0026#34;: [ \u0026#34;1.1.1.0/24\u0026#34;, \u0026#34;2.2.0.0/22\u0026#34;, \u0026#34;3.3.3.0/30\u0026#34; ] } Data Comparison We use the built-in ansible difference filter to compare the two lists that we have extracted.\nThis is a one-way comparison, meaning we compare Google list from the web \u0026ndash;\u0026gt; Google List from the Fortigate.\nIf we wanted a full comparison, we would run this task again in reverse.\n### Use ansible difference filter to compare list1 to list2. It shows items that are in list1 but not in list2 - name: Show the difference in lists set_fact: missing_google_ips: \u0026#34;{{ google_cdn_ipv4_list | difference(google_ip_list) }}\u0026#34; changed_when: false delegate_to: localhost ### If an IP Subnet exists in the google cdn ipv4 list but not on the firewall \u0026#39;google_cdn object\u0026#39; then ### we fail the PB and Tower will send an email with the list - debug: msg: \u0026#34;List of missing Google Subnets that need to be added to Firewall: {{ missing_google_ips }}\u0026#34; failed_when: missing_google_ips | length\u0026gt;0 Alerting and Scheduling We can use Ansible Tower/AWX to schedule playbook execution and alerting.\nThe playbook is set to fail if there is an IP in the web list that does not exist in the Fortigate list.\nWe do this so that Ansible Tower can alert on failure. More info on setting up Notifications here\nSimilarly, job scheduling can also be setup so the task runs daily\nWrap Up I hope this tutorial was useful to anyone looking to automate a common operational workflow. This playbook performs read actions, its a good way to get started on your automation journey without worrying about making breaking changes.\nThe next logical iteration for this process is to automatically update the firewall object without manual intervention.\nFeel free to comment below or contact me on Twitter if you have any questions.\nThanks!\n","date":"2022-04-28T00:00:00Z","image":"https://kaonbytes.com/p/use-ansible-to-compare-cloud-ip-ranges-against-firewall-object/cloud-ip-checks-mini_hudd0ab147682fbdd31b8bb756e5cdd221_351688_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/use-ansible-to-compare-cloud-ip-ranges-against-firewall-object/","title":"Use Ansible to compare Cloud IP Ranges against Firewall object"},{"content":"The best way for a Network Engineer to grasp automation is to begin by coding a simple problem that they encounter. We often use IPERF to measure the bandwidth performance of a network path. With a few lines of python code, we can automate this task and graph the data via DataDog for historical reference.\nThe End Result Nobody likes to read through pages of text and images to get to the money shot. So here it is\u0026hellip;\nDataDog Graph Graph of hourly iperf3 tests:\nPython Code iperf-dd-metrics.py\n# Script to run iperf3 test to measure bandwidth to remote site. # Runs in reverse mode to measure both ingress and egress bandwidth # Sends avg bandwidth metric to Datadog as a custom gauge metric # Its suggested you run this script as a cron job on a regular hourly interval from datadog import initialize, statsd import time import iperf3 import os # Set vars # Remote iperf server IP remote_site = \u0026#39;\u0026lt;enter remote host IP here\u0026gt;\u0026#39; # Datadog API Key api_key = \u0026#39;\u0026lt;enter dd api key here\u0026gt;\u0026#39; # How long to run iperf3 test in seconds test_duration = 20 # Set DD options for statsd init options = { \u0026#39;statsd_host\u0026#39;: \u0026#39;127.0.0.1\u0026#39;, \u0026#39;statsd_port\u0026#39;: 8125, \u0026#39;api_key\u0026#39;: api_key } initialize(**options) # Set Iperf Client Options # Run 10 parallel streams on port 5201 for duration w/ reverse client = iperf3.Client() client.server_hostname = remote_site client.zerocopy = True client.verbose = False client.reverse = True client.port = 5201 client.num_streams = 10 client.duration = int(test_duration) client.bandwidth = 1000000000 # Run iperf3 test result = client.run() # extract relevant data sent_mbps = int(result.sent_Mbps) received_mbps = int(result.received_Mbps) # send Metrics to DD and add some tags for classification in DD GUI # send bandwidth metric - egress mbps statsd.gauge(\u0026#39;iperf3.test.mbps.egress\u0026#39;, sent_mbps, tags=[\u0026#34;team_name:your_team\u0026#34;, \u0026#34;team_app:iperf\u0026#34;]) # send bandwidth metric - ingress mbps statsd.gauge(\u0026#39;iperf3.test.mbps.ingress\u0026#39;, received_mbps, tags=[\u0026#34;team_name:your_team\u0026#34;, \u0026#34;team_app:iperf\u0026#34;]) The Process If you\u0026rsquo;re still here, let\u0026rsquo;s get into the details\u0026hellip;\nSetup Spin up two Linux Hosts (HostA and HostB) Install Python3 \u0026ndash;\u0026gt; yum install python3 Install IPERF3 \u0026ndash;\u0026gt; yum install iperf3 Install iPerf3 Python Wrapper \u0026ndash;\u0026gt; pip install iperf3 Setup a DataDog Account and API Key Graphing the metrics is optional. Alternatively, we can save results to local disk. iperf3 Server Choose one of your hosts to be your always-on iperf3 server and setup an iperf3 service in systemd\nFor Centos Linux: cd /etc/systemd/system/ Create a file called iperf3.service Contents: # Centos Server file to run iperf3 service on startup. # This server acts as the iperf \u0026#39;receiver\u0026#39; for speed testing. # /etc/systemd/system/iperf3.service # User service: $HOME/.config/systemd/user/iperf3.service [Unit] Description=iperf3 server After=syslog.target network.target auditd.service [Service] ExecStart=/usr/bin/iperf3 -s [Install] WantedBy=multi-user.target enable the service to run at startup: systemctl enable iperf3.service start the iperf3 service: systemctl start iperf3.service verify the service is running: systemctl status iperf3.service or ps aux | grep iperf iperf3 Client Begin testing the iperf3 service by creating a simple python script on the Client HostA\nCreate iperf-dd-metrics-test.py: import iperf3 # Set vars # Remote iperf server IP remote_site = \u0026#39;ip of server goes here\u0026#39; # How long to run iperf3 test in seconds test_duration = 10 # Set Iperf Client Options # Run 10 parallel streams on port 5201 for duration w/ reverse client = iperf3.Client() client.server_hostname = remote_site client.zerocopy = True client.verbose = False client.reverse = True client.port = 5201 client.num_streams = 10 client.duration = int(test_duration) client.bandwidth = 1000000000 # Run iperf3 test result = client.run() # extract relevant data sent_mbps = int(result.sent_Mbps) received_mbps = int(result.received_Mbps) print(\u0026#39;sent_mbps: \u0026#39;) print(sent_mbps) print(\u0026#39;received_mbps: \u0026#39;) print(received_mbps) run the file: # python3 iperf-dd-metrics-test.py: results: sent_mbps: 966 received_mbps: 959 Now we have a fully functioning iperf3 client and server setup.\nSending Metrics to DataDog DataDog has a python library to allow us to send the speed test results up to DD\nImport the library and initialize options: from datadog import initialize, statsd # Datadog API Key api_key = os.getenv(\u0026#39;DD_API_KEY\u0026#39;) # Set DD options for statsd init options = { \u0026#39;statsd_host\u0026#39;: \u0026#39;127.0.0.1\u0026#39;, \u0026#39;statsd_port\u0026#39;: 8125, \u0026#39;api_key\u0026#39;: api_key } initialize(**options) # send Metrics to DD and add some tags for classification in DD GUI # send bandwidth metric - egress mbps statsd.gauge(\u0026#39;iperf3.test.mbps.egress\u0026#39;, sent_mbps, tags=[\u0026#34;team_name:your_team\u0026#34;, \u0026#34;team_app:iperf\u0026#34;]) # send bandwidth metric - ingress mbps statsd.gauge(\u0026#39;iperf3.test.mbps.ingress\u0026#39;, received_mbps, tags=[\u0026#34;team_name:your_team\u0026#34;, \u0026#34;team_app:iperf\u0026#34;]) Full Script and Crontab Put the two pieces of the script together and you get the end result \u0026mdash; iperf-dd-metrics.py\nUse crontab to continuously run the script in periodic intervals.\n# Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed 20 * * * * root /usr/bin/python3 /home/iperf-python/iperf-dd-metrics.py The above job will run every hour on the 20th minute of the hour forever\nVisualizing the Data The metrics that are being sent to DataDog need to be graphed on a dashboard\nLogon to DataDog and go to Metrics \u0026ndash;\u0026gt; Explorer Search iperf3 Find you metrics Create a timeseries graph Graph both Egress and Ingress Metrics as A and B respectively Set your Y-Axis to desired MAX setting (999 in my case) Give your graph a title Final Thoughts This scenario is a good way to get started with network automation. However, it can be iterated and improved by:\nAdding more metrics: Jitter Packet Loss UDP Testing Add exception handling Feel free to contact me on twitter or comment below if you need help. Cheers!\n","date":"2022-04-14T00:00:00Z","image":"https://kaonbytes.com/p/automate-network-bandwidth-testing-with-python-and-iperf/python-iperf-datadog-mini2_hu779ac677055344eb65e39a4877c2e8ce_279163_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/automate-network-bandwidth-testing-with-python-and-iperf/","title":"Automate Network Bandwidth Testing with Python and Iperf"}]