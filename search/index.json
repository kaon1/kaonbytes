[{"content":"There are many commericial tools that can perform configuration backups of network gear (for example Solarwinds NCM). However, we can build an alternative tool using Ansible + a versioned Amazon AWS S3 Bucket + a daily scheduler.\nOverview In this guide, I will detail how we can build such a tool and meet the following requirments:\nDynamic Inventory Vendor neutral system Version Controlled Backups Notfication of successful and failed backups Health Check of the Backup Tool Currently Supported Vendors/OS: Opengear Aruba AOS Cisco ASA and IOS Fortinet Fortios Juniper Junos Citrix Netscaler Big-IP F5 Arista EOS A read-only recurring Ansible playbook, like the one we are discussing, is a great way to get started in network automation of your enterprise gear because:\nThe playbook does not make confuguration changes to your remote devices Your playbook has to touch every device in your network. That means Authentication, Connecticty, and Ansible Modules must all be correct on every run. Getting this all working the first time takes some effort but will pay off in future projects. Workflow Below is a diagram of the expected workflow operation of the ansible playbook.\nGrab inventory list from Netbox with devices tagged for backup. Call a specific Ansible task to perform system backup on each device Store all system backups locally and sync to a version controlled S3 Bucket Maintain a summary of failed or successful device backup actions and create report to Email Update local status.txt file to be polled by outside monitoring system The Solution Github Repo can be found here - Ansible Configuration Backup Manager\nRunning the playbook Here is an example output of running the playbook against 8 hosts each on a different network operating system. This playbook has been run on hundreds of network devices in production environments.\nCommand:\n[root@ncm]# ansible-playbook -i netbox_inventory.yml -e \u0026#34;var_hosts=aruba1:asa1:f51:nss1:fortios1:ios1:junos1:opengear1\u0026#34; ncm-engine.yml Output:\nPLAY [PLAY TO BACKUP NETWORK CONFIGURATIONS] ********** TASK [delete exisiting successful_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [delete exisiting failed_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [delete total_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [create successful_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [create failed_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [create total_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [update total_hosts file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [include_role : generate-backups] ********** TASK [generate-backups : include_tasks] ********** included: /root/ncm/roles/generate-backups/tasks/aos-backup-config.yml for aruba1 included: /root/ncm/roles/generate-backups/tasks/asa-backup-config.yml for asa1 included: /root/ncm/roles/generate-backups/tasks/big-ip-backup-config.yml for f51 included: /root/ncm/roles/generate-backups/tasks/citrix-backup-config.yml for nss1 included: /root/ncm/roles/generate-backups/tasks/fortios-backup-config.yml for fortios1 included: /root/ncm/roles/generate-backups/tasks/ios-backup-config.yml for ios1 included: /root/ncm/roles/generate-backups/tasks/junos-backup-config.yml for junos1 included: /root/ncm/roles/generate-backups/tasks/opengear-backup-config.yml for opengear1 TASK [generate-backups : grab and download aruba config] ********** ok: [aruba1] TASK [generate-backups : Save the backup information.] ********** changed: [aruba1 -\u0026gt; localhost] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [aruba1 -\u0026gt; localhost] TASK [generate-backups : Backup ASA Device] ********** ok: [asa1] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [asa1 -\u0026gt; localhost] TASK [generate-backups : grab and download big-ip config] ********** ok: [f51 -\u0026gt; localhost] TASK [generate-backups : Save the backup information.] ********** changed: [f51 -\u0026gt; localhost] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [f51 -\u0026gt; localhost] TASK [generate-backups : grab and download citrix config] ********** ok: [nss1] TASK [generate-backups : Save the backup information.] ********** changed: [nss1 -\u0026gt; localhost] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [nss1 -\u0026gt; localhost] TASK [generate-backups : Backup Fortigate Device] ********** ok: [fortios1] TASK [generate-backups : Save the backup information.] ********** changed: [fortios1] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [fortios1 -\u0026gt; localhost] TASK [generate-backups : Backup IOS Device] ***************** ok: [ios1] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [ios1 -\u0026gt; localhost] TASK [generate-backups : grab and download junos config] ********** ok: [junos1] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [junos1 -\u0026gt; localhost] TASK [generate-backups : grab and download opengear config] ********** changed: [opengear1] TASK [generate-backups : Save the backup information.] ********** ok: [opengear1 -\u0026gt; localhost] TASK [generate-backups : Add SUCCESS line to file] ********** changed: [opengear1 -\u0026gt; localhost] PLAY [SYNC NETWORK CONFIGURATIONS TO S3 BUCKET] ************* TASK [delete exisiting s3_sync file] ************************ changed: [localhost] TASK [create s3_sync file] ********************************** changed: [localhost] TASK [include_role : sync-to-s3] **************************** TASK [sync-to-s3 : Sync to S3] ****************************** changed: [localhost] TASK [sync-to-s3 : Add status to s3 state file] ************* changed: [localhost] PLAY [Build Email Template] ********************************* TASK [lookup file successful_hosts.txt] ********************* ok: [localhost] TASK [lookup file failed_hosts.txt] ************************* ok: [localhost] TASK [lookup file total_hosts.txt] ************************** ok: [localhost] TASK [lookup file failed_s3.txt] **************************** ok: [localhost] TASK [Generate Backup State File] *************************** changed: [localhost] TASK [send email] ******************************************* ok: [localhost] PLAY RECAP ************************************************** asa1 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 opengear1 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 nss1 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 fortios1 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 localhost : ok=10 changed=5 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 aruba1 : ok=11 changed=9 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ios1 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 junos1 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 f51 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 [root@ncm]# Email Summary Report The playbook generates an Email Template which gets sent out as the final task.\nThe Netbox Dynamic Inventory Ansible calls the netbox API and grabs a list of inventory-to-be-backed-up by filtering for a specified tag.\nAdditionally, we pass other important data to the inventory such as:\nansible_network_os ansible_connection netbox_inventory.yml Source\n--- plugin: netbox.netbox.nb_inventory api_endpoint: \u0026#34;\u0026lt;netbox-url\u0026gt;\u0026#34; token: token validate_certs: false config_context: false compose: ansible_network_os: platform.slug ansible_connection: custom_fields.ansible_connection device_query_filters: - status: \u0026#39;active\u0026#39; - tag: \u0026#39;ncm_backup\u0026#39; Example of Dynamic Inventory Pull [root@ncm]# ansible-inventory -i netbox_inventory.yml --host junos1 { \u0026#34;ansible_connection\u0026#34;: \u0026#34;netconf\u0026#34;, \u0026#34;ansible_host\u0026#34;: \u0026#34;10.10.10.10\u0026#34;, \u0026#34;ansible_network_os\u0026#34;: \u0026#34;junos\u0026#34;, \u0026#34;custom_fields\u0026#34;: { \u0026#34;ansible_connection\u0026#34;: \u0026#34;netconf\u0026#34;, \u0026#34;code_version\u0026#34;: \u0026#34;xxx\u0026#34; }, \u0026#34;device_roles\u0026#34;: [ \u0026#34;access_switch\u0026#34; ], \u0026#34;device_types\u0026#34;: [ \u0026#34;ex4300-48p\u0026#34; ], \u0026#34;is_virtual\u0026#34;: false, \u0026#34;local_context_data\u0026#34;: [ null ], \u0026#34;locations\u0026#34;: [], \u0026#34;manufacturers\u0026#34;: [ \u0026#34;juniper\u0026#34; ], \u0026#34;platforms\u0026#34;: [ \u0026#34;junos\u0026#34; ], \u0026#34;primary_ip4\u0026#34;: \u0026#34;10.10.10.10\u0026#34;, \u0026#34;regions\u0026#34;: [ \u0026#34;americas\u0026#34; ], \u0026#34;services\u0026#34;: [], \u0026#34;sites\u0026#34;: [ \u0026#34;xxx1\u0026#34; ], \u0026#34;status\u0026#34;: { \u0026#34;label\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;active\u0026#34; }, \u0026#34;tags\u0026#34;: [ \u0026#34;ncm_backup\u0026#34;, ] } The Main File source\n--- - name: \u0026#34;PLAY TO BACKUP NETWORK CONFIGURATIONS\u0026#34; hosts: \u0026#34;{{ var_hosts }}\u0026#34; roles: - role: arubanetworks.aos_wlan_role vars: network_backup_dir: \u0026#34;/root/configuration-backup-manager/config-backups/\u0026#34; net_backup_filename: \u0026#34;{{ inventory_hostname }}-{{ ansible_host }}-config.txt\u0026#34; tasks: - name: delete exisiting successful_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/successful_hosts.txt state: absent run_once: True delegate_to: localhost - name: delete exisiting failed_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/failed_hosts.txt state: absent run_once: True delegate_to: localhost - name: delete total_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/total_hosts.txt state: absent run_once: True delegate_to: localhost - name: create successful_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/successful_hosts.txt state: touch run_once: True delegate_to: localhost - name: create failed_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/failed_hosts.txt state: touch run_once: True delegate_to: localhost - name: create total_hosts file ansible.builtin.file: path: /root/configuration-backup-manager/templates/total_hosts.txt state: touch run_once: True delegate_to: localhost - name: update total_hosts file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/total_hosts.txt line: \u0026#34;{{ groups[\u0026#39;all\u0026#39;] | length }}\u0026#34; run_once: True delegate_to: localhost - include_role: name: generate-backups - name: \u0026#34;SYNC NETWORK CONFIGURATIONS TO S3 BUCKET\u0026#34; hosts: localhost vars: network_backup_dir: \u0026#34;/root/configuration-backup-manager/config-backups/\u0026#34; net_backup_filename: \u0026#34;{{ inventory_hostname }}-{{ ansible_host }}-config.txt\u0026#34; tasks: - name: delete exisiting s3_sync file ansible.builtin.file: path: /root/configuration-backup-manager/templates/failed_s3.txt state: absent run_once: True - name: create s3_sync file ansible.builtin.file: path: /root/configuration-backup-manager/templates/failed_s3.txt state: touch run_once: True - include_role: name: sync-to-s3 - name: \u0026#34;Build Email Template\u0026#34; hosts: localhost tasks: - name: \u0026#34;lookup file successful_hosts.txt\u0026#34; set_fact: success_data: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/configuration-backup-manager/templates/successful_hosts.txt\u0026#39;).splitlines() }}\u0026#34; - name: \u0026#34;lookup file failed_hosts.txt\u0026#34; set_fact: failed_data: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/configuration-backup-manager/templates/failed_hosts.txt\u0026#39;).splitlines() }}\u0026#34; - name: \u0026#34;lookup file total_hosts.txt\u0026#34; set_fact: total_data: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/configuration-backup-manager/templates/total_hosts.txt\u0026#39;) }}\u0026#34; - name: \u0026#34;lookup file failed_s3.txt\u0026#34; set_fact: s3_error: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/configuration-backup-manager/templates/failed_s3.txt\u0026#39;).splitlines() }}\u0026#34; - name: Generate Backup State File template: src: \u0026#34;/root/configuration-backup-manager/templates/backup_state.j2\u0026#34; dest: \u0026#34;/root/configuration-backup-manager/templates/backup_state.txt\u0026#34; - name: send email mail: host: localhost port: 25 sender: \u0026#39;\u0026lt;email\u0026gt;\u0026#39; to: \u0026#39;\u0026lt;email\u0026gt;\u0026#39; subject: \u0026#39;Ansible NCM Job Completion\u0026#39; body: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/configuration-backup-manager/templates/backup_state.txt\u0026#39;)}}\u0026#34; The above playbook performs the following actions:\nInitializes some local flat text files called successful_hosts \u0026ndash; failed_hosts \u0026ndash; total_hosts \u0026ndash; failed_s3.txt The playbook performs writes to these files to keep track of stats per host to generate a report later on Uses include_role to include the generate-backups folder of tasks. The main.yml file in the generate-backups is how we call each backup task based on vendor OS Sync the local config-backups directory to s3 using the community.aws.s3_sync library Builds and sends an email report using a jinja2 template and the previously mentioned text files as variables The Backup Tasks tasks/main.yml One line include_task which uses the vendor OS to call the desired backup task. Source can be found here\n--- - include_tasks: \u0026#34;{{ role_path }}/tasks/{{ ansible_network_os }}-backup-config.yml\u0026#34; Cisco IOS The below playbook calls the Cisco ios_config library to backup the device and register the output locally.\nWe use ansible_connection: network_cli (as defined in netbox inventory) and plain user and password which can be defined as an enviornment variable or through some more secure method (Ansible Vault, Hashicorp Vault, etc) if desired.\n--- - name: IOS CISCO block: - name: Backup IOS Device vars: ansible_user: \u0026#34;username\u0026#34; ansible_password: pwd ios_config: backup: yes backup_options: filename: \u0026#34;{{ net_backup_filename }}\u0026#34; dir_path: \u0026#34;{{ network_backup_dir }}\u0026#34; register: backupinfo - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: backupinfo is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Cisco ASA Similarly, the ASA Playbook uses the asa_config library.\n--- - name: ASA CISCO block: - name: Backup ASA Device vars: ansible_user: \u0026#34;username\u0026#34; ansible_password: pwd asa_config: backup: yes backup_options: filename: \u0026#34;{{ net_backup_filename }}\u0026#34; dir_path: \u0026#34;{{ network_backup_dir }}\u0026#34; register: backupinfo - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: backupinfo is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Juniper Junos For Juniper devices, we use the junipernetworks.junos library and the netconf ansible_connection on port 930\n--- - name: JUNOS block: - name: grab and download junos config vars: ansible_user: un ansible_ssh_private_key_file: key # ansible_connection: netconf junipernetworks.junos.junos_config: backup: yes backup_options: filename: \u0026#34;{{ net_backup_filename }}\u0026#34; dir_path: \u0026#34;{{ network_backup_dir }}\u0026#34; register: config_output - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: config_output is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Aruba AOS For Aruba AOS devices we use the HTTPAPI over port 4343\n--- - name: ARUBA block: - name: grab and download aruba config vars: ansible_httpapi_port: 4343 ansible_httpapi_validate_certs: False ansible_httpapi_use_ssl: True ansible_user: username ansible_password: pwd aos_show_command: command: show running-config register: aos_output - name: Save the backup information. copy: content: \u0026#39;{{ aos_output.msg._data[0] }}\u0026#39; dest: \u0026#34;{{ network_backup_dir }}/{{ net_backup_filename }}\u0026#34; delegate_to: localhost - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: aos_output is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Fortinet FortiOS Fortinet devices are also connected via the ansible_httpapi over port 443 using the fortinet.fortios library\n--- - name: FORTIOS block: - name: Backup Fortigate Device vars: ansible_httpapi_use_ssl: yes ansible_httpapi_validate_certs: no ansible_httpapi_port: 443 ansible_user: \u0026#34;username\u0026#34; ansible_password: pwd # ansible_connection: httpapi fortinet.fortios.fortios_monitor_fact: selector: \u0026#39;system_config_backup\u0026#39; vdom: \u0026#39;root\u0026#39; params: scope: \u0026#39;global\u0026#39; register: backupinfo - name: Save the backup information. copy: content: \u0026#39;{{ backupinfo.meta.raw }}\u0026#39; dest: \u0026#34;{{ network_backup_dir }}/{{ net_backup_filename }}\u0026#34; - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: backupinfo is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Citrix Netscaler For Citrix Netscaler devices we can use any basic cli_command library. In this case, I set the network_os to be vyos because I did not want to interfere with the cisco IOS devices.\n--- - name: CITRIX NETSCALER block: - name: grab and download citrix config vars: ansible_user: \u0026lt;un\u0026gt; ansible_password: \u0026lt;pwd\u0026gt; ansible_network_os: vyos cli_command: command: show ns runningConfig register: citrix_output - name: Save the backup information. copy: content: \u0026#39;{{ citrix_output.stdout_lines | to_nice_json }}\u0026#39; dest: \u0026#34;{{ network_backup_dir }}/{{ net_backup_filename }}\u0026#34; delegate_to: localhost - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: citrix_output is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Big IP F5 The BIG IP F5 task uses a legacy library running as ansible_connection local\n--- - name: F5 BIG-IP block: - name: grab and download big-ip config bigip_command: commands: - show running-config provider: server: \u0026#34;{{ ansible_host }}\u0026#34; password: pwd user: un validate_certs: no transport: cli register: bigip_result delegate_to: localhost - name: Save the backup information. copy: content: \u0026#39;{{ bigip_result.stdout_lines | to_nice_json }}\u0026#39; dest: \u0026#34;{{ network_backup_dir }}/{{ net_backup_filename }}\u0026#34; delegate_to: localhost - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: bigip_result is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 OpenGear For OpenGear devices we can run a raw SSH command and register the output.\n--- - name: OPENGEAR block: - name: grab and download opengear config vars: ansible_port: 22 ansible_user: un ansible_password: pwd raw: config -g config register: output_opengear_config - name: Save the backup information. copy: content: \u0026#39;{{ output_opengear_config.stdout_lines | to_nice_json }}\u0026#39; dest: \u0026#34;{{ network_backup_dir }}/{{ net_backup_filename }}\u0026#34; delegate_to: localhost - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: output_opengear_config delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Arista EOS Arista Devices use the arista.eos.eos_config library over httpapi port 443\n--- - name: ARISTA EOS block: - name: grab and download ARISTA config vars: ansible_httpapi_port: 443 ansible_httpapi_validate_certs: False ansible_httpapi_use_ssl: True ansible_become: True ansible_user: username ansible_password: pwd arista.eos.eos_config: backup: yes backup_options: filename: \u0026#34;{{ net_backup_filename }}\u0026#34; dir_path: \u0026#34;{{ network_backup_dir }}\u0026#34; register: arista_backupinfo - name: Add SUCCESS line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/successful_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; when: arista_backupinfo is defined delegate_to: localhost throttle: 1 rescue: - name: Add ERROR line to file ansible.builtin.lineinfile: path: /root/configuration-backup-manager/templates/failed_hosts.txt line: \u0026#34;{{ inventory_hostname }}\u0026#34; delegate_to: localhost throttle: 1 Syncing to S3 After all the backup files are saved in a local directory. We can sync the entire directory to an S3 bucket. The S3 bucket has versioning enbaled, so we can keep track of changes over a period of time.\n--- - name: block: - name: Sync to S3 community.aws.s3_sync: bucket: \u0026lt;bucket_name\u0026gt; key_prefix: config-backups file_root: /root/configuration-backup-manager/config-backups/ register: result_s3 - name: Add status to s3 state file ansible.builtin.lineinfile: path: templates/failed_s3.txt line: \u0026#34;S3 SYNC SUCCESSFUL\u0026#34; when: result_s3 is defined rescue: - name: Add error state to file ansible.builtin.lineinfile: path: templates/failed_s3.txt line: \u0026#34;ERROR S3 SYNC FAILED\u0026#34; Example of a backed up versioned config Notifications and Health Checking Jinja2 Email Template Jinja2 Template\nAnsible Configuration Backup Manager Date: \u0026#34;{{ lookup(\u0026#39;pipe\u0026#39;,\u0026#39;date\u0026#39;) }}\u0026#34; Errored Backups: {{ failed_data | length }} Successful Backups: {{ success_data | length }} Total Devices Tagged for Backup: {{ total_data }} S3 Sync Status: {{ s3_error }} ___________________________________________________________________________ Location of All Backups can be found on AWS S3: s3://\u0026lt;bucket-name\u0026gt;/config-backups/ List of Devices Tagged for Nightly Backup: https://\u0026lt;netbox-url\u0026gt;/dcim/devices/?tag=ncm_backup Execution Server Location: ssh://\u0026lt;server\u0026gt; Directory: /root/configuration-backup-manager/ncm-engine.yml GitHub Repo: https://github.com/\u0026lt;repo\u0026gt; PRTG Sensor Of Backup State: https://\u0026lt;url\u0026gt;/sensor.htm?id=... Documentation: https://\u0026lt;url\u0026gt; ___________________________________________________________________________ Failed Device Names: {# new line #} {{ failed_data | to_nice_yaml(indent=2) }} Successful Device Names: {# new line #} {{ success_data | to_nice_yaml(indent=2) }} Creating a Status Page We can use a python script to tell us if the backup process has worked as expected.\nCheck if the latest report is more than 24 hours old Check if error count is \u0026gt; 0 Check if successfull hosts + errored hosts matches total hosts If any of the above conditions fail, update a local web-hosted server file to be FALSE\nOur Network Monitoring System can poll this file and alert us if its in a FAILED state\nHere\u0026rsquo;s the code:\nimport os import datetime as dt import re backup_state_file_path = \u0026#39;/root/configuration-backup-manager/backup_state.txt\u0026#39; def is_file_current(filename): try: today = dt.datetime.now().date() filetime = dt.datetime.fromtimestamp(os.path.getmtime(filename)) if filetime.date() == today: return True else: return False except Exception as e: return False def is_no_errors_count(filename): try: pattern = re.compile(\u0026#34;Errored Backups: (\\\\d)\u0026#34;) for line in open(filename): for match in re.finditer(pattern, line): if match.group(1) == \u0026#39;0\u0026#39;: return True else: return False return False except Exception as e: return False def is_no_s3_errors(filename): try: pattern = re.compile(\u0026#34;(S3 SYNC SUCCESSFUL)\u0026#34;) for line in open(filename): for match in re.finditer(pattern, line): if match.group(1) == \u0026#39;S3 SYNC SUCCESSFUL\u0026#39;: return True else: return False return False except Exception as e: return False def is_total_match(filename): try: pattern1 = re.compile(\u0026#34;Errored Backups: (\\\\d)\u0026#34;) for line in open(filename): for match in re.finditer(pattern1, line): total_errors = match.group(1) pattern2 = re.compile(\u0026#34;Successful Backups: (\\\\d)\u0026#34;) for line in open(filename): for match in re.finditer(pattern2, line): total_success = match.group(1) pattern3 = re.compile(\u0026#34;Total Devices Tagged for Backup: (\\\\d)\u0026#34;) for line in open(filename): for match in re.finditer(pattern3, line): total_hosts = match.group(1) if int(total_errors) + int(total_hosts) == int(total_success): return True else: return False except Exception as e: return False def main(): backup_status = is_file_current(backup_state_file_path) and is_no_errors_count(backup_state_file_path) and is_no_s3_errors(backup_state_file_path) and is_total_match(backup_state_file_path) if os.path.exists(\u0026#34;/usr/share/nginx/html/backup_status.txt\u0026#34;): os.remove(\u0026#34;/usr/share/nginx/html/backup_status.txt\u0026#34;) f = open(\u0026#34;/usr/share/nginx/html/backup_status.txt\u0026#34;, \u0026#34;a\u0026#34;) f.write(str(backup_status)) f.close() if __name__ == \u0026#34;__main__\u0026#34;: main() Scheduling Daily schedules can be setup with a crontab job. The first task runs the ansible playbook and the 2nd task (1 hour later) runs the python code status check.\n[root@ncmncm]# crontab -l MAILTO=\u0026#34;kaon\u0026#34; 0 7 * * * /usr/local/bin/ansible-playbook -i /root/ncm/netbox_inventory.yml -e \u0026#34;var_hosts=all\u0026#34; /root/ncm/ncm-engine.yml 0 8 * * * /usr/bin/python3 /root/ncm/backup_status_nms.py Final Thoughts The system may seem complex, but hopefully I have broken it down into enough small chunks to be understandable. Feel free to contact me on Twitter with any questions. Thanks for reading.\n","date":"2022-08-23T00:00:00Z","image":"https://kaonbytes.com/p/ansible-configuration-backup-manager/ansible-network-config-front_hu12225e3f406182c91634608ad7cb2c64_1239508_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/ansible-configuration-backup-manager/","title":"Ansible Configuration Backup Manager"},{"content":"In this guide I will walk through how to deploy BGPAlerter as a service in Amazon (AWS) as an ECS/Fargate container using a Github, Terraform, Drone Pipeline\nWhat is BGPAlerter? BGPAlerter is a well-engineered and useful tool built by Massimo Candela which allows you to monitor, in real-time, important BGP updates to your desired list of Prefixes and ASNs on the public internet.\nThe BGPAlerter application connects to public BGP data from these sources.\nThe user edits prefix.yml and config.yml files to setup desired monitoring and alerting/notifications.\nHere is a Super High Level overview of the flow:\nLive RIPE Feed Example\nHere is an example of the Live RIPE Feed of BGP Updates Why should we use Infrastructure as Code? Network Engineers love tools, but oftentimes, the tools that Network Engineers deploy turn into bespoke pet projects that are hard to keep in sync, maintain or document.\nInfrastructure as code (IaC) helps alleviate these problems because it: Allows for version controlled configuration changes Self-documents (for the most part) Creates an easy-to-follow process for updating and maintaining the tool application and system environment The Solution High Level Diagram Drone Process The Code Github Repo of the Code can be found here This repo and blog post are not meant to be \u0026ldquo;plug-and-play\u0026rdquo; ready, there are some parts of the infrastructure omitted (such as the VPC, Route53 etc) The key learning takeaways of this guide are how to format the drone.yml, Dockerfile, ecr.tf and ecs.tf files Credit goes to the devops folks at my org from whom I borrowed much of the terraform code and made my own tweaks as needed prefix.yml The source file that BGPAlerter reads of which prefixes and ASNs to monitor\n1.1.1.0/23: description: ABCDEFG asn: - 111111 ignoreMorespecifics: false ignore: false group: noc 2.2.2.0/24: description: HIJKLMNOP asn: - 22222 ignoreMorespecifics: false ignore: false group: noc 3.3.3.0/21: description: QRSTUVWXYZ asn: - 33333 ignoreMorespecifics: false ignore: false group: noc options: monitorASns: \u0026#39;111111\u0026#39;: group: noc upstreams: - 55555 downstreams: [] generate: asnList: - \u0026#39;111111\u0026#39; - \u0026#39;22222\u0026#39; - \u0026#39;33333\u0026#39; exclude: [] excludeDelegated: false prefixes: null monitoredASes: true historical: true group: noc Dockerfile Instructions that Drone uses to build your docker image. This image is later pushed to AWS ECR\n### Pull latest bgpalerter container from nttgin docker hub FROM nttgin/bgpalerter:latest ### Copy our own prefix.yml file and config file to the docker image ### When drone builds this image and pushes it to ECR, it will contain our files (not the defaults) COPY bgpalerter/YOUR_ASN_PREFIX_LIST.yml /opt/bgpalerter/volume/YOUR_ASN_PREFIX_LIST.yml COPY bgpalerter/config.yml /opt/bgpalerter/volume/config.yml ### Slack webhook should not be committed in code. This script replaces the slackwebhook in config.yml with a tf var COPY bgpalerter/slack-token-replace.sh /opt/bgpalerter/volume/slack-token-replace.sh ### open port 8011 for status checks EXPOSE 8011 Drone File The Drone file are the steps that drone takes to build the infrastructure. This file consists of two sections\nFirst, build the docker image and push it to ECR Second, build the AWS Fargate infra from the terraform directory --- ### First pipeline creates the docker image from Dockerfile and pushes to your ECR repo kind: pipeline name: ecr workspace: path: XXX steps: ### Drone pushes to ECR/bgpalerter and tags as latest + adds commit_sha ### The commit_sha is important because its how we force Fargate to always rebuild/redeploy the container on changes - name: publish-image-local image: plugins/ecr settings: registry: xxx.dkr.ecr.us-east-1.amazonaws.com repo: bgpalerter dockerfile: Dockerfile tag: - latest-${DRONE_COMMIT_SHA} environment: SHARED_CREDENTIALS_FILE: XXX ### When main changes, drone runs this pipeline when: event: push branch: main --- ### The 2nd drone pipeline runs terraform code to build aws infrastructure kind: pipeline name: default workspace: path: XXX steps: ### Format syntax check terraform files - name: fmt-module image: hashicorp/terraform commands: - cd terraform/modules/fargate-infra - terraform fmt -check -diff=true - name: fmt-aws image: hashicorp/terraform commands: - cd terraform/aws - terraform fmt -check -diff=true ### Build terraform plan file and sleep30 seconds. Giving admin time to abort - name: prd-terraform-plan image: hashicorp/terraform commands: - cd terraform/aws - terraform init -input=false - terraform plan -out=PLANFILE -input=false -lock-timeout=5m - sleep 30 environment: SHARED_CREDENTIALS_FILE: XXX ### Pass commit_sha so we can use it in terraform file to call our docker image TF_VAR_image_tag_digest: ${DRONE_COMMIT_SHA} when: event: push branch: main ### Deploy infra with terraform apply - name: prd-terraform-apply image: hashicorp/terraform commands: - cd terraform/aws - terraform apply -input=false -lock-timeout=5m PLANFILE environment: SHARED_CREDENTIALS_FILE: XXX ### Pass commit_sha so we can use it in terraform file to call our docker image TF_VAR_image_tag_digest: ${DRONE_COMMIT_SHA} ### When main changes, drone runs this pipeline when: event: push branch: main Slack Webhook Script Slack Webhooks are sensitive, and should not be committed to Github. This script grabs the slackwebhook from a tf env variable and uses sed to load it into confi.yml. This is done before the BGPAlerter service is started on the container.\n### This script is whats started on the docker container. First we replace the slackwebhook then start bgpalerter service ### Call SLACK_API_TOKEN from tf env variable which is stored in AWS Param Store ESCAPED_REPLACE=$(printf \u0026#39;%s\\n\u0026#39; \u0026#34;$SLACK_API_TOKEN\u0026#34; | sed -e \u0026#39;s/[\\/\u0026amp;]/\\\\\u0026amp;/g\u0026#39;) /bin/sed -i \u0026#34;s/slacktokentobereplaced/$ESCAPED_REPLACE/g\u0026#34; /opt/bgpalerter/volume/config.yml /bin/cat /opt/bgpalerter/volume/config.yml /usr/local/bin/npm run serve -- --d /opt/bgpalerter/volume/ ECR Terraform File Builds the Elastic Container Registry in AWS\n###This file creates the ECR repository for the docker image. resource \u0026#34;aws_ecr_repository\u0026#34; \u0026#34;app_image\u0026#34; { name = \u0026#34;bgpalerter\u0026#34; image_tag_mutability = \u0026#34;MUTABLE\u0026#34; image_scanning_configuration { scan_on_push = true } } ECS Terraform File Builds the Elastic Container Service in AWS\n### ECS File creates the Fargate service for the docker container ### Sets CPU and RAM ### Creates the bgpalerter task definition and runs it resource \u0026#34;aws_ecs_cluster\u0026#34; \u0026#34;bgpalerter-fargate\u0026#34; { name = local.svc_name capacity_providers = [\u0026#34;FARGATE_SPOT\u0026#34;] default_capacity_provider_strategy { capacity_provider = \u0026#34;FARGATE_SPOT\u0026#34; } tags = var.common_tags } // ECS service resource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;bgpalerter-svc\u0026#34; { name = local.svc_name cluster = aws_ecs_cluster.bgpalerter-fargate.id task_definition = aws_ecs_task_definition.task_definition.arn desired_count = 1 launch_type = \u0026#34;FARGATE\u0026#34; network_configuration { subnets = xxx security_groups = [xxx] } load_balancer { target_group_arn = xxx container_name = xxx container_port = xxx } tags = xxx } ### Create BGPAlerter Task resource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;task_definition\u0026#34; { family = local.svc_name requires_compatibilities = [\u0026#34;FARGATE\u0026#34;] network_mode = \u0026#34;awsvpc\u0026#34; execution_role_arn = xxx task_role_arn = xxx ### CPU and Memory cpu = 512 memory = 1024 tags = xxx container_definitions = jsonencode([ { name = local.svc_name image = \u0026#34;${var.image_name}${var.image_tag_digest}\u0026#34; essential = true ### Call the shell script when the image starts. This replaces the slack webhook and starts the service entryPoint = [\u0026#34;/bin/sh\u0026#34;, \u0026#34;/opt/bgpalerter/volume/slack-token-replace.sh\u0026#34;] secrets = [ { name = \u0026#34;SLACK_API_TOKEN\u0026#34; valueFrom = xxx } ] portMappings = [ { containerPort = 8011 hostPort = 8011 } ] logConfiguration = { logDriver = \u0026#34;awslogs\u0026#34; options = { awslogs-group = local.svc_name awslogs-region = \u0026#34;us-east-1\u0026#34; awslogs-stream-prefix = \u0026#34;bgpalerter\u0026#34; } } } ]) } The Results Drone Build Progress Deployed Infrastructure - Elastic Container Registry Deployed Infrastructure - Elastic Container Service and Fargate BGPAlerter Running and Status Notifications The Alert email provides a link to BGPLay which lets you view the event in a graphed form\nFinal Thoughts As a Network Engineer, I often find it hard to grasp abstract concepts of software development. In my experience, the best way to learn these concepts is to port over something you know (like a networking tool) into a new system. Hopefully, this post will help others in their journey. Thanks for reading!\n","date":"2022-08-12T00:00:00Z","image":"https://kaonbytes.com/p/bgpalerter-as-code-using-a-terraform-pipeline/front_huaf53bc8ed3209b5b56820218227b86f5_395056_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/bgpalerter-as-code-using-a-terraform-pipeline/","title":"BGPAlerter As Code Using A Terraform Pipeline"},{"content":"The most important pillar of Network Automation is a Source of Truth (SoT)\nAn accurate and well maintained SoT should provide:\nA list of device inventory (hosts) Connection parameters (IP/Hostname, connection protocol/port) Global, Regional, or Site specific parameters that are required to maintain desired network state The Chicken or the Egg Problem When dealing with a brownfield enterprise environment, we run into The Chicken or The Egg Problem at the beginning of our Network Automation journey.\nThe problem is, we need an accurate and well maintained SoT but this requires manual work to gather data from all currently running devices and keep that data up to date as the network changes. Let\u0026rsquo;s automate this! But wait, how can we automate this if we don\u0026rsquo;t have a SoT?\nThe Solution The answer is a staged approach (i.e. bootstrapping):\nChoose your SoT platform, I like Netbox Perform some manual work to get a basic static device list as an ansible inventory file Run Ansible fact gathering accross all of your inventory Use the Netbox Ansible Collection to populate netbox with key information of each device such as name, ip, connection profile, site, etc For all future playbooks, use the Netbox Ansible Dynamic Plugin to populate your ansible inventory. Maintain accurate state by repeatedly running (scheduled) fact gathering and keep netbox up to date via Ansible Diagram The Code Bootstrap Playbook \u0026ndash;\u0026gt; netbox-ansible-junos-bootstrap.yml\n### Playbook to bootstrap netbox inventory with a list of juniper devices provided by static inventory file # Uses ansible gather_facts to grab net_version, serial number and net_model # Also perform a dig to get a FQDN which we can use as device name instead of the inventory_name --- - name: PB to Bootstrap Netbox Inventory hosts: junosinv gather_facts: True vars: ansible_user: ansible_ssh_private_key_file: netbox_url: netbox_token: platform: \u0026#34;{{ ansible_network_os }}\u0026#34; site: device_role: \u0026#34;access_switch\u0026#34; tasks: - name: \u0026#34;Check if net_version exists\u0026#34; ### If ansible_facts does not provide net_version we manually fill it in as 111 set_fact: net_version: \u0026#34;111\u0026#34; when: ansible_facts[\u0026#39;net_version\u0026#39;] is undefined - name: \u0026#34;Assign net version\u0026#34; set_fact: net_version: \u0026#34;{{ ansible_facts[\u0026#39;net_version\u0026#39;] }}\u0026#34; when: ansible_facts[\u0026#39;net_version\u0026#39;] is defined ### Optional - name: \u0026#34;Resolve FQDN Hostname - perform DIG\u0026#34; ### Perform linux DIG command to get the reverse DNS record for the IP. THis will be our new hostname for netbox raw: \u0026#34;dig -x {{ ansible_host }} +short | sed -e \u0026#39;s/.$//\u0026#39;\u0026#34; register: dig_result delegate_to: localhost ### Optional - name: \u0026#34;TASK 11: Assign dig result to fqdn var\u0026#34; ### If Reverse DNS exists, trim whhite spaces and assing to var set_fact: fqdn: \u0026#34;{{ dig_result.stdout_lines[0] | trim}}\u0026#34; when: dig_result.stdout_lines[0] is defined ### Optional - name: \u0026#34;TASK 12: If no dig result, assign placeholder fqdn value\u0026#34; ### If no reverse DNS, then set a inventory hostname and IP as the hostname set_fact: fqdn: \u0026#34;{{ inventory_hostname }}-no-dns-{{ ansible_host }}\u0026#34; when: dig_result.stdout_lines[0] is undefined - name: \u0026#34;Add Device to NetBox\u0026#34; netbox.netbox.netbox_device: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: name: \u0026#34;{{ fqdn }}\u0026#34; device_type: \u0026#34;{{ ansible_facts[\u0026#39;net_model\u0026#39;] }}\u0026#34; platform: \u0026#34;{{ platform }}\u0026#34; serial: \u0026#34;{{ ansible_facts[\u0026#39;net_serialnum\u0026#39;] }}\u0026#34; site: \u0026#34;{{ site }}\u0026#34; device_role: \u0026#34;{{ device_role }}\u0026#34; custom_fields: code_version: \u0026#34;{{ net_version }}\u0026#34; state: present validate_certs: no delegate_to: localhost - name: \u0026#34;Add a new Interface called management_interface to device\u0026#34; ### this interface will be used as the primary IP and interface for the device netbox.netbox.netbox_device_interface: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: device: \u0026#34;{{ fqdn }}\u0026#34; name: Management_Interface type: other state: present validate_certs: no delegate_to: localhost - name: \u0026#34;Add IP address of ansible host to IPAM\u0026#34; netbox.netbox.netbox_ip_address: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: family: 4 address: \u0026#34;{{ ansible_host }}/32\u0026#34; status: active assigned_object: name: Management_Interface device: \u0026#34;{{ fqdn }}\u0026#34; state: present validate_certs: no delegate_to: localhost - name: \u0026#34;Assign ansible_host IP as the primary interface for the device\u0026#34; netbox.netbox.netbox_device: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: name: \u0026#34;{{ fqdn }}\u0026#34; device_type: \u0026#34;{{ ansible_facts[\u0026#39;net_model\u0026#39;] }}\u0026#34; platform: \u0026#34;{{ platform }}\u0026#34; serial: \u0026#34;{{ ansible_facts[\u0026#39;net_serialnum\u0026#39;] }}\u0026#34; status: Active primary_ip4: \u0026#34;{{ ansible_host }}/32\u0026#34; state: present validate_certs: no delegate_to: localhost The Process Let\u0026rsquo;s go through the step by step.\nThe example below will use Juniper Devices. Junos provides some key features to help with automation:\nConfiguration Structure (versioning, commit confirm, rollback options are super valuable) Ability to load configuration as text blocks with replace, override and merge options Return JSON values (i.e. Display JSON) Building Static Inventory File Bootstrap Inventory \u0026ndash;\u0026gt; bootstrap_inventory.ini\n[junosinv] switch1 ansible_host=10.10.10.1 switch2 ansible_host=10.10.10.2 switch3 ansible_host=10.10.10.3 [junosinv:vars] ansible_connection=netconf ansible_network_os=junos Running the bootstrap playbook Now that we have a basic inventory. We can run the above Bootstrap Playbook \u0026ndash;\u0026gt; netbox-ansible-junos-bootstrap.yml\nThe playbook will do the following:\nConnect to the remote juniper host and gather system facts Connect to the Netbox server and add a new device with the below data: name device_type (model) platform (OS) Serial Number Site Device role Code Version Creates a new Netbox IP Address (IPAM) entry with the ansible_host IP Creates a new interface for the device named \u0026ldquo;management_interface\u0026rdquo; Assigns the IP address to the device\u0026rsquo;s management_interface Ansible Result:\n(venv) [root]# ansible-playbook -i bootstrap_inventory.ini netbox-ansible-junos-bootstrap.yml PLAY [PB to Bootstrap Netbox Inventory] ****** [WARNING]: Ignoring timeout(10) for junos_facts TASK [Gathering Facts] *********************** [WARNING]: default value for `gather_subset` will be changed to `min` from `!config` v2.11 onwards ok: [switch1] TASK [Assign net version] ******************** ok: [switch1] TASK [Resolve FQDN Hostname - perform DIG] *** changed: [switch1 -\u0026gt; localhost] TASK [TASK 11: Assign dig result to fqdn var] * ok: [switch1] TASK [Add Device to NetBox] ***************** changed: [switch1 -\u0026gt; localhost] TASK [Add a new Interface called management_interface to device] * changed: [switch1 -\u0026gt; localhost] TASK [Add IP address of ansible host to IPAM] ****** changed: [switch1 -\u0026gt; localhost] TASK [Assign ansible_host IP as the primary interface for the device] * changed: [switch1 -\u0026gt; localhost] PLAY RECAP ************************************** switch1 : ok=8 changed=5 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 Netbox Result:\nDynamic Inventory Now that we have populated Netbox, we can use Netbox as our ansible inventory\nThe Netbox Dynamic Inventory Plugin can be used by creating an inventory file as follows:\n## Ansible Plugin file for dynamic inventory through netbox --- plugin: netbox.netbox.nb_inventory api_endpoint: # token: \u0026#34;{{ lookup(\u0026#39;env\u0026#39;,\u0026#39;NETBOX_API_KEY\u0026#39;) }}\u0026#34; validate_certs: false config_context: true compose: ansible_network_os: platform.slug ansible_connection: custom_fields.ansible_connection device_query_filters: - status: \u0026#39;active\u0026#39; - tag: \u0026#39;some_tag\u0026#39; To check if dynamic inventory is working, run ansible-inventory:\n# ansible-inventory -i ../netbox/netbox_inventory.yml --host TEST01\nResult:\n{ \u0026#34;ansible_connection\u0026#34;: \u0026#34;netconf\u0026#34;, \u0026#34;ansible_host\u0026#34;: \u0026#34;10.X.X.X\u0026#34;, \u0026#34;ansible_network_os\u0026#34;: \u0026#34;junos\u0026#34;, \u0026#34;custom_fields\u0026#34;: { \u0026#34;ansible_connection\u0026#34;: \u0026#34;netconf\u0026#34;, \u0026#34;code_version\u0026#34;: \u0026#34;20\u0026#34;, }, \u0026#34;device_roles\u0026#34;: [ \u0026#34;access_switch\u0026#34; ], \u0026#34;device_site\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;device_types\u0026#34;: [ \u0026#34;ex4300-48p\u0026#34; ], \u0026#34;is_virtual\u0026#34;: false, \u0026#34;local_context_data\u0026#34;: [ null ], \u0026#34;locations\u0026#34;: [], \u0026#34;manufacturers\u0026#34;: [ \u0026#34;juniper\u0026#34; ], \u0026#34;platforms\u0026#34;: [ \u0026#34;junos\u0026#34; ], \u0026#34;primary_ip4\u0026#34;: \u0026#34;10.x.x.x\u0026#34;, \u0026#34;regions\u0026#34;: [ \u0026#34;americas\u0026#34; ], \u0026#34;services\u0026#34;: [], \u0026#34;sites\u0026#34;: [ \u0026#34;xxx\u0026#34; ], \u0026#34;status\u0026#34;: { \u0026#34;label\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;active\u0026#34; }, \u0026#34;tags\u0026#34;: [ \u0026#34;3\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;1\u0026#34; ] } Schedule Playbook to Maintain System State Let\u0026rsquo;s regularly run a modified version of the bootstrap playbook to maintain network state.\nModified snippet:\n- name: \u0026#34;CONFIRM DEVICE TO NETBOX\u0026#34; netbox.netbox.netbox_device: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: name: \u0026#34;{{ inventory_hostname }}\u0026#34; device_type: \u0026#34;{{ ansible_facts[\u0026#39;net_model\u0026#39;] }}\u0026#34; serial: \u0026#34;{{ ansible_facts[\u0026#39;net_serialnum\u0026#39;] }}\u0026#34; custom_fields: code_version: \u0026#34;{{ net_version }}\u0026#34; ansible_connection: \u0026#34;{{ custom_fields[\u0026#39;ansible_connection\u0026#39;] }}\u0026#34; state: present validate_certs: no delegate_to: localhost The above task will only make changes to Netbox if something changes on the field device, such as the:\nModel number OS version Serial number Use Ansible Tower/AWX to schedule playbook execution daily, weekly or monthly.\nWrap Up I hope this guide was helpful in understanding how we can use a Source of truth to dynamically populate an Ansible Inventory. Feel free to comment below with any questions.\n","date":"2022-05-13T00:00:00Z","image":"https://kaonbytes.com/p/netbox-dynamic-inventory-for-ansible-as-a-feedback-loop/netbox-ansible-feedback_hu91008c89e08c35be24c68bc187db22be_244905_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/netbox-dynamic-inventory-for-ansible-as-a-feedback-loop/","title":"Netbox Dynamic Inventory for Ansible as a feedback loop"},{"content":"A common IT Workflow exists where a Cloud provider\u0026rsquo;s public IP ranges are added to a static firewall object list. The reasons why this is done include: Network Address Translation (NAT), Access Control (ACL) or logging.\nOften times, the network engineer is unaware when the Cloud Provider updates their IP ranges and thus the IT Workflow is no longer in compliance.\nWe can automate some of this process by setting up a job to continually check the Cloud IP ranges and alert us if a change is required.\nOverview The below process is specifically for Google Cloud IP Ranges and Fortinet Firewall Objects. However, with a few tweaks can be ported to other services such as Fastly and AWS as well as other firewall vendors supported by ansible collections.\nAnsible Code compare-google-ips-check-against-fortinet.yml\n### This playbook will compare the list of known IPv4 Google Cloud IP addresses against ### the a deployed \u0026#39;address_grp\u0026#39; object in a Fortigate Firewall ### First we grab the json data from https://www.gstatic.com/ipranges/goog.json and extract ipv4 addresses to a list ### Next we grab the firewall_addrgrp called \u0026#39;google_cdn\u0026#39; on the firewall and put it into a list ### We run a difference of list1 vs list2. If there is a difference, the PB will throw an error and Tower sends email --- - name: PB to compare Google Cloud IPs vs current \u0026#39;google_cdn\u0026#39; object in firewall hosts: firewall_host1 gather_facts: false vars: ### Fortinet specific vars for Ansible to connect - https://galaxy.ansible.com/fortinet/fortios ansible_python_interpreter: /usr/bin/python3 ansible_user: user ansible_password: password ansible_connection: httpapi ansible_httpapi_use_ssl: yes ansible_httpapi_validate_certs: no ansible_httpapi_port: 443 ansible_network_os: fortinet.fortios.fortios vdom: \u0026#34;root\u0026#34; ### Variable to store list of \u0026#39;google_cdn\u0026#39; ips as a list from the user firewall google_ip_list: [] ### Variable to store list of Google Cloud IP addresses we retrieve from the internet google_cdn_ipv4_list: [] ### Variable to store the list difference result missing_google_ips: [] tasks: ### GET request to retrieve the current json data of Google Cloud IP Ranges - name: Get all google cloud ip ranges as json result uri: url: \u0026#34;https://www.gstatic.com/ipranges/goog.json\u0026#34; method: GET validate_certs: no register: google_web_json_result ### Ansible will register a change here, we can ignore it. changed_when: false delegate_to: localhost ### Extract only IPv4 Addresses and add to a flat list - set_fact: google_cdn_ipv4_list: \u0026#34;{{ google_cdn_ipv4_list + [ item[\u0026#39;ipv4Prefix\u0026#39;] ] }}\u0026#34; loop: \u0026#34;{{ google_web_json_result.json.prefixes }}\u0026#34; when: item[\u0026#39;ipv4Prefix\u0026#39;] is defined changed_when: false delegate_to: localhost ### Hit the firewall once here to retrieve the object. This object does not contain IP/Mask info only names - name: Get google_cdn list of objects from firewall fortinet.fortios.fortios_configuration_fact: vdom: \u0026#34;{{ vdom }}\u0026#34; selector: \u0026#34;firewall_addrgrp\u0026#34; params: name: \u0026#34;google_cdn\u0026#34; register: google_networks_objects changed_when: false ### For each name in \u0026#39;google_cdn\u0026#39; object we ask the firewall to give us back the IP/Mask info. Many API hits here. - name: Iterate through every Google object and extract subnet info fortinet.fortios.fortios_configuration_fact: vdom: \u0026#34;{{ vdom }}\u0026#34; selector: \u0026#34;firewall_address\u0026#34; params: name: \u0026#34;{{ item.name }}\u0026#34; register: google_item loop: \u0026#34;{{ google_networks_objects.meta.results[0][\u0026#39;member\u0026#39;] }}\u0026#34; changed_when: false ### The returned IP and Subnet info is in form 10.10.10.10 255.255.255.0. These Filters translate that to 10.10.10.10/24 ### List is populated with all \u0026#39;google_cdn\u0026#39; IP/Mask in correct format for comparison - set_fact: google_ip_list: \u0026#34;{{ google_ip_list + [ item.meta.results[0].subnet | replace(\u0026#39; \u0026#39;,\u0026#39;/\u0026#39;) | ansible.netcommon.ipaddr ] }}\u0026#34; loop: \u0026#34;{{ google_item.results }}\u0026#34; changed_when: false delegate_to: localhost ### Use ansible difference filter to compare list1 to list2. It shows items that are in list1 but not in list2 - name: Show the difference in lists set_fact: missing_google_ips: \u0026#34;{{ google_cdn_ipv4_list | difference(google_ip_list) }}\u0026#34; changed_when: false delegate_to: localhost ### If an IP Subnet exists in the google cdn ipv4 list but not on the firewall \u0026#39;google_cdn object\u0026#39; then ### we fail the PB and Tower will send an email with the list - debug: msg: \u0026#34;List of missing Google Subnets that need to be added to Firewall: {{ missing_google_ips }}\u0026#34; failed_when: missing_google_ips | length\u0026gt;0 Results No Difference Detected: Difference Detected: Detailed Step-by-Step Let\u0026rsquo;s dive into the step-by-step\nGet Google Cloud IP Ranges Browsing to https://www.gstatic.com/ipranges/goog.json returns a JSON result of IP Prefixes.\nWe can programmatically access the JSON result and store all IPv4 prefixes in an ansible list called google_cdn_ipv4_list:\n### GET request to retrieve the current json data of Google Cloud IP Ranges - name: Get all google cloud ip ranges as json result uri: url: \u0026#34;https://www.gstatic.com/ipranges/goog.json\u0026#34; method: GET validate_certs: no register: google_web_json_result ### Ansible will register a change here, we can ignore it. changed_when: false delegate_to: localhost ### Extract only IPv4 Addresses and add to a flat list - set_fact: google_cdn_ipv4_list: \u0026#34;{{ google_cdn_ipv4_list + [ item[\u0026#39;ipv4Prefix\u0026#39;] ] }}\u0026#34; loop: \u0026#34;{{ google_web_json_result.json.prefixes }}\u0026#34; when: item[\u0026#39;ipv4Prefix\u0026#39;] is defined changed_when: false delegate_to: localhost We can run the above tasks and print the result. Below is a printout of each dataset: https://www.gstatic.com/ipranges/goog.json and google_cdn_ipv4_list\nHTTP Site Ansible GET Result Get Address Group from Fortigate We can use the ansible collection provided by Fortinet to gather facts.\nIn the below task we will retrieve the firewall address group named test_group_1\n- name: Get Fortigate Address Group fortinet.fortios.fortios_configuration_fact: vdom: \u0026#34;root\u0026#34; selector: \u0026#34;firewall_addrgrp\u0026#34; params: name: \u0026#34;test_group_1\u0026#34; register: google_networks_objects - debug: var: google_networks_objects.meta.results[0] Here is a comparison of the Address Group in the Fortigate GUI and the Ansible Result:\nHTTP Site Ansible Result:\nTASK [debug] ********************* ok: [localhost] =\u0026gt; { \u0026#34;google_networks_objects.meta.results[0]\u0026#34;: { \u0026#34;allow-routing\u0026#34;: \u0026#34;disable\u0026#34;, \u0026#34;color\u0026#34;: 0, \u0026#34;comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;exclude\u0026#34;: \u0026#34;disable\u0026#34;, \u0026#34;exclude-member\u0026#34;: [], \u0026#34;member\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;test_net_1\u0026#34;, \u0026#34;q_origin_key\u0026#34;: \u0026#34;test_net_1\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;test_net_2\u0026#34;, \u0026#34;q_origin_key\u0026#34;: \u0026#34;test_net_2\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;test_net_3\u0026#34;, \u0026#34;q_origin_key\u0026#34;: \u0026#34;test_net_3\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;test_group_1\u0026#34;, \u0026#34;q_origin_key\u0026#34;: \u0026#34;test_group_1\u0026#34;, \u0026#34;tagging\u0026#34;: [], \u0026#34;uuid\u0026#34;: \u0026#34;3ff6c462-c7f1-51ec-f405-2a6b59ee9591\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;enable\u0026#34; } } Get Each Member of Group The above ansible result gives us a list of address group member names. We have to query the firewall again for the IP address and subnet mask of each member.\nThis task will loop through each group member:\n- name: Iterate through every Google object and extract subnet info fortinet.fortios.fortios_configuration_fact: vdom: \u0026#34;{{ vdom }}\u0026#34; selector: \u0026#34;firewall_address\u0026#34; params: name: \u0026#34;{{ item.name }}\u0026#34; register: google_item loop: \u0026#34;{{ google_networks_objects.meta.results[0][\u0026#39;member\u0026#39;] }}\u0026#34; - debug: var: google_item Extract Subnet Info to List The returned results need to be parsed. The subnet field should be normalized to IP/MASK notation so that we can more easily run a diff later against the exisiting google cdn list that we grabbed earlier from the web.\nWe can parse each subnet field and transfrom it to IP/MASK notation with this task:\n- set_fact: google_ip_list: \u0026#34;{{ google_ip_list + [ item.meta.results[0].subnet | replace(\u0026#39; \u0026#39;,\u0026#39;/\u0026#39;) | ansible.netcommon.ipaddr ] }}\u0026#34; loop: \u0026#34;{{ google_item.results }}\u0026#34; changed_when: false delegate_to: localhost Now we have a list of all IPs from the Fortigate in a normalized view (Example Data):\nTASK [debug] ***************************** ok: [localhost] =\u0026gt; { \u0026#34;google_ip_list\u0026#34;: [ \u0026#34;1.1.1.0/24\u0026#34;, \u0026#34;2.2.0.0/22\u0026#34;, \u0026#34;3.3.3.0/30\u0026#34; ] } Data Comparison We use the built-in ansible difference filter to compare the two lists that we have extracted.\nThis is a one-way comparison, meaning we compare Google list from the web \u0026ndash;\u0026gt; Google List from the Fortigate.\nIf we wanted a full comparison, we would run this task again in reverse.\n### Use ansible difference filter to compare list1 to list2. It shows items that are in list1 but not in list2 - name: Show the difference in lists set_fact: missing_google_ips: \u0026#34;{{ google_cdn_ipv4_list | difference(google_ip_list) }}\u0026#34; changed_when: false delegate_to: localhost ### If an IP Subnet exists in the google cdn ipv4 list but not on the firewall \u0026#39;google_cdn object\u0026#39; then ### we fail the PB and Tower will send an email with the list - debug: msg: \u0026#34;List of missing Google Subnets that need to be added to Firewall: {{ missing_google_ips }}\u0026#34; failed_when: missing_google_ips | length\u0026gt;0 Alerting and Scheduling We can use Ansible Tower/AWX to schedule playbook execution and alerting.\nThe playbook is set to fail if there is an IP in the web list that does not exist in the Fortigate list.\nWe do this so that Ansible Tower can alert on failure. More info on setting up Notifications here\nSimilarly, job scheduling can also be setup so the task runs daily\nWrap Up I hope this tutorial was useful to anyone looking to automate a common operational workflow. This playbook performs read actions, its a good way to get started on your automation journey without worrying about making breaking changes.\nThe next logical iteration for this process is to automatically update the firewall object without manual intervention.\nFeel free to comment below or contact me on Twitter if you have any questions.\nThanks!\n","date":"2022-04-28T00:00:00Z","image":"https://kaonbytes.com/p/use-ansible-to-compare-cloud-ip-ranges-against-firewall-object/cloud-ip-checks-mini_hudd0ab147682fbdd31b8bb756e5cdd221_351688_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/use-ansible-to-compare-cloud-ip-ranges-against-firewall-object/","title":"Use Ansible to compare Cloud IP Ranges against Firewall object"},{"content":"The best way for a Network Engineer to grasp automation is to begin by coding a simple problem that they encounter. We often use IPERF to measure the bandwidth performance of a network path. With a few lines of python code, we can automate this task and graph the data via DataDog for historical reference.\nThe End Result Nobody likes to read through pages of text and images to get to the money shot. So here it is\u0026hellip;\nDataDog Graph Graph of hourly iperf3 tests:\nPython Code iperf-dd-metrics.py\n# Script to run iperf3 test to measure bandwidth to remote site. # Runs in reverse mode to measure both ingress and egress bandwidth # Sends avg bandwidth metric to Datadog as a custom gauge metric # Its suggested you run this script as a cron job on a regular hourly interval from datadog import initialize, statsd import time import iperf3 import os # Set vars # Remote iperf server IP remote_site = \u0026#39;\u0026lt;enter remote host IP here\u0026gt;\u0026#39; # Datadog API Key api_key = \u0026#39;\u0026lt;enter dd api key here\u0026gt;\u0026#39; # How long to run iperf3 test in seconds test_duration = 20 # Set DD options for statsd init options = { \u0026#39;statsd_host\u0026#39;: \u0026#39;127.0.0.1\u0026#39;, \u0026#39;statsd_port\u0026#39;: 8125, \u0026#39;api_key\u0026#39;: api_key } initialize(**options) # Set Iperf Client Options # Run 10 parallel streams on port 5201 for duration w/ reverse client = iperf3.Client() client.server_hostname = remote_site client.zerocopy = True client.verbose = False client.reverse = True client.port = 5201 client.num_streams = 10 client.duration = int(test_duration) client.bandwidth = 1000000000 # Run iperf3 test result = client.run() # extract relevant data sent_mbps = int(result.sent_Mbps) received_mbps = int(result.received_Mbps) # send Metrics to DD and add some tags for classification in DD GUI # send bandwidth metric - egress mbps statsd.gauge(\u0026#39;iperf3.test.mbps.egress\u0026#39;, sent_mbps, tags=[\u0026#34;team_name:your_team\u0026#34;, \u0026#34;team_app:iperf\u0026#34;]) # send bandwidth metric - ingress mbps statsd.gauge(\u0026#39;iperf3.test.mbps.ingress\u0026#39;, received_mbps, tags=[\u0026#34;team_name:your_team\u0026#34;, \u0026#34;team_app:iperf\u0026#34;]) The Process If you\u0026rsquo;re still here, let\u0026rsquo;s get into the details\u0026hellip;\nSetup Spin up two Linux Hosts (HostA and HostB) Install Python3 \u0026ndash;\u0026gt; yum install python3 Install IPERF3 \u0026ndash;\u0026gt; yum install iperf3 Install iPerf3 Python Wrapper \u0026ndash;\u0026gt; pip install iperf3 Setup a DataDog Account and API Key Graphing the metrics is optional. Alternatively, we can save results to local disk. iperf3 Server Choose one of your hosts to be your always-on iperf3 server and setup an iperf3 service in systemd\nFor Centos Linux: cd /etc/systemd/system/ Create a file called iperf3.service Contents: # Centos Server file to run iperf3 service on startup. # This server acts as the iperf \u0026#39;receiver\u0026#39; for speed testing. # /etc/systemd/system/iperf3.service # User service: $HOME/.config/systemd/user/iperf3.service [Unit] Description=iperf3 server After=syslog.target network.target auditd.service [Service] ExecStart=/usr/bin/iperf3 -s [Install] WantedBy=multi-user.target enable the service to run at startup: systemctl enable iperf3.service start the iperf3 service: systemctl start iperf3.service verify the service is running: systemctl status iperf3.service or ps aux | grep iperf iperf3 Client Begin testing the iperf3 service by creating a simple python script on the Client HostA\nCreate iperf-dd-metrics-test.py: import iperf3 # Set vars # Remote iperf server IP remote_site = \u0026#39;ip of server goes here\u0026#39; # How long to run iperf3 test in seconds test_duration = 10 # Set Iperf Client Options # Run 10 parallel streams on port 5201 for duration w/ reverse client = iperf3.Client() client.server_hostname = remote_site client.zerocopy = True client.verbose = False client.reverse = True client.port = 5201 client.num_streams = 10 client.duration = int(test_duration) client.bandwidth = 1000000000 # Run iperf3 test result = client.run() # extract relevant data sent_mbps = int(result.sent_Mbps) received_mbps = int(result.received_Mbps) print(\u0026#39;sent_mbps: \u0026#39;) print(sent_mbps) print(\u0026#39;received_mbps: \u0026#39;) print(received_mbps) run the file: # python3 iperf-dd-metrics-test.py: results: sent_mbps: 966 received_mbps: 959 Now we have a fully functioning iperf3 client and server setup.\nSending Metrics to DataDog DataDog has a python library to allow us to send the speed test results up to DD\nImport the library and initialize options: from datadog import initialize, statsd # Datadog API Key api_key = os.getenv(\u0026#39;DD_API_KEY\u0026#39;) # Set DD options for statsd init options = { \u0026#39;statsd_host\u0026#39;: \u0026#39;127.0.0.1\u0026#39;, \u0026#39;statsd_port\u0026#39;: 8125, \u0026#39;api_key\u0026#39;: api_key } initialize(**options) # send Metrics to DD and add some tags for classification in DD GUI # send bandwidth metric - egress mbps statsd.gauge(\u0026#39;iperf3.test.mbps.egress\u0026#39;, sent_mbps, tags=[\u0026#34;team_name:your_team\u0026#34;, \u0026#34;team_app:iperf\u0026#34;]) # send bandwidth metric - ingress mbps statsd.gauge(\u0026#39;iperf3.test.mbps.ingress\u0026#39;, received_mbps, tags=[\u0026#34;team_name:your_team\u0026#34;, \u0026#34;team_app:iperf\u0026#34;]) Full Script and Crontab Put the two pieces of the script together and you get the end result \u0026mdash; iperf-dd-metrics.py\nUse crontab to continuously run the script in periodic intervals.\n# Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed 20 * * * * root /usr/bin/python3 /home/iperf-python/iperf-dd-metrics.py The above job will run every hour on the 20th minute of the hour forever\nVisualizing the Data The metrics that are being sent to DataDog need to be graphed on a dashboard\nLogon to DataDog and go to Metrics \u0026ndash;\u0026gt; Explorer Search iperf3 Find you metrics Create a timeseries graph Graph both Egress and Ingress Metrics as A and B respectively Set your Y-Axis to desired MAX setting (999 in my case) Give your graph a title Final Thoughts This scenario is a good way to get started with network automation. However, it can be iterated and improved by:\nAdding more metrics: Jitter Packet Loss UDP Testing Add exception handling Feel free to contact me on twitter or comment below if you need help. Cheers!\n","date":"2022-04-14T00:00:00Z","image":"https://kaonbytes.com/p/automate-network-bandwidth-testing-with-python-and-iperf/python-iperf-datadog-mini2_hu779ac677055344eb65e39a4877c2e8ce_279163_120x120_fill_box_smart1_3.png","permalink":"https://kaonbytes.com/p/automate-network-bandwidth-testing-with-python-and-iperf/","title":"Automate Network Bandwidth Testing with Python and Iperf"}]