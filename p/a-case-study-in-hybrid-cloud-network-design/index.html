<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content='The challenges of interconnecting the big 3 cloud providers to form a cohesive solution'><title>A Case Study in Hybrid Cloud Network Design</title>
<link rel=canonical href=https://kaonbytes.com/p/a-case-study-in-hybrid-cloud-network-design/><link rel=stylesheet href=/scss/style.min.450926226e724574a6b936335ea06111f8aeb253d932c86cb2cc807341cd2889.css><meta property='og:title' content='A Case Study in Hybrid Cloud Network Design'><meta property='og:description' content='The challenges of interconnecting the big 3 cloud providers to form a cohesive solution'><meta property='og:url' content='https://kaonbytes.com/p/a-case-study-in-hybrid-cloud-network-design/'><meta property='og:site_name' content='KaonBytes'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2024-05-17T00:00:00+00:00'><meta property='article:modified_time' content='2024-05-17T00:00:00+00:00'><meta property='og:image' content='https://kaonbytes.com/p/a-case-study-in-hybrid-cloud-network-design/images/front2.png'><meta name=twitter:site content="@kaon_123"><meta name=twitter:creator content="@kaon_123"><meta name=twitter:title content="A Case Study in Hybrid Cloud Network Design"><meta name=twitter:description content="The challenges of interconnecting the big 3 cloud providers to form a cohesive solution"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://kaonbytes.com/p/a-case-study-in-hybrid-cloud-network-design/images/front2.png'><link rel="shortcut icon" href=favicon.ico><script async src="https://www.googletagmanager.com/gtag/js?id=G-LE0JVZ0N09"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LE0JVZ0N09")}</script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/profile-new_hu0424c52cde2ba37c8617ad2cd064c5ee_88533_300x0_resize_q75_box.jpg width=300 height=450 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>KaonBytes</a></h1><h2 class=site-description>Welcome to my technical blog on all things Networking, Security and Automation</h2></div></header><ol class=social-menu><li><a href=https://github.com/kaon1 target=_blank title=GitHub><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/kaon1/ target=_blank title=LinkedIn><svg role="img" xmlns="http://www.w3.org/2000/svg" width="1e3mm" height="1e3mm" viewBox="0 0 1e3 1e3" style="max-width:1.6em;height:auto"><path id="path" style="opacity:1;vector-effect:none;fill:#000;fill-opacity:1" d="M336 332s0 457 0 457-125 0-125 0 0-457 0-457 125 0 125 0m10-126s0 0 0 0c0 41-33 74-73 74s-72-33-72-74c0-40 32-73 72-73s73 33 73 73M834 508s0 281 0 281-125 0-125 0 0-233 0-233c0-140-166-129-166 0V789s-124 0-124 0 0-457 0-457 124 0 124 0 0 74 0 74c58-108 291-116 291 102" transform=""/></svg></a></li><li><a href=https://twitter.com/kaonthana target=_blank title=Twitter><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/_index.zh-cn/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/learning-links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Learning Links</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://kaonbytes.com/ selected>English</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/a-case-study-in-hybrid-cloud-network-design/><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/front2_hu9bcde3317be3c40344c69cb562d5c034_33248_800x0_resize_box_3.png srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/front2_hu9bcde3317be3c40344c69cb562d5c034_33248_800x0_resize_box_3.png 800w, /p/a-case-study-in-hybrid-cloud-network-design/images/front2_hu9bcde3317be3c40344c69cb562d5c034_33248_1600x0_resize_box_3.png 1600w" width=800 height=290 loading=lazy alt="Featured image of post A Case Study in Hybrid Cloud Network Design"></a></div><div class=article-details><header class=article-category><a href=/categories/cloud/>Cloud
</a><a href=/categories/observability/>Observability
</a><a href=/categories/netdevops/>Netdevops
</a><a href=/categories/bgp/>Bgp</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/a-case-study-in-hybrid-cloud-network-design/>A Case Study in Hybrid Cloud Network Design</a></h2><h3 class=article-subtitle>The challenges of interconnecting the big 3 cloud providers to form a cohesive solution</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>May 17, 2024</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>15 minute read</time></div></footer></div></header><section class=article-content><h2 id=about>About</h2><p>A case study in network design for the <strong>hybrid network engineer</strong>. A walkthrough of the year-long journey to interconnect <strong>public cloud workloads</strong> from all three major CSPs (<a class=link href=https://azure.microsoft.com/en-us target=_blank rel=noopener>Azure</a>, <a class=link href=https://aws.amazon.com/ target=_blank rel=noopener>AWS</a>, <a class=link href="https://cloud.google.com/?hl=en" target=_blank rel=noopener>GCP</a>) and on-premises <strong>Enterprise Networks</strong> to provide a robust and highly available solution for the application teams. I will discuss the architectural strategy, lessons learned, pitfalls and wins of the <strong>overall solution</strong>.</p><h2 id=problem-statement>Problem Statement</h2><p>In many organizations, there may exist scenarios where some <strong>applications</strong> are built in one cloud provider (such as GCP) and another application or supporting system run by a different team is built in a different environment (such as AWS). This may be due to specific <strong>cloud offerings</strong> of one provider over another, developer preferences/experience, historical reasons, cost optimization etc. The <strong>why</strong> doesn&rsquo;t really matter - but when these workloads want to communicate - now this becomes a <strong>connectivity puzzle</strong>.</p><p>The answer for many teams is to route this traffic over the <strong>public internet</strong>. Like so:</p><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/f.png width=3525 height=1238 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/f_hua4742719b4fc54e9723e3c78390010a6_54398_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/f_hua4742719b4fc54e9723e3c78390010a6_54398_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=284 data-flex-basis=683px></p><p>This solution works and for most orgs thats usually <strong>good enough</strong>. There are some quick fixes we could implement instead of the above solution, such as to create <strong>site-to-site VPN tunnels</strong> or more recently CSPs are now offering cloud-to-cloud interconnects. However, when you are dealing with dozens of <strong>cloud accounts</strong> (tenants), large amounts of <strong>traffic</strong> and on-prem <strong>data center</strong> traffic these fixes may not scale. Let&rsquo;s step back and take a look at the whole picture.</p><p>Some key <strong>info</strong> to take note of:</p><ul><li>Most of the cloud-to-cloud traffic is happening in the <strong>US-East</strong> region. We can focus our efforts here.</li><li>The traffic pattern rates will not exceed <strong>10Gbps</strong> (for now :D)</li></ul><p>Can we <strong>architect</strong> a solution that:</p><ol><li>Decreases cloud egress <strong>costs</strong></li><li>Improves <strong>security posture</strong> by reducing the amount of public endpoints that don&rsquo;t need to be exposed</li><li>Integrates with the existing on premises <strong>data center network</strong></li><li>Improves network <strong>latency</strong></li></ol><p>While still mainting the performance, reliability and <strong>agility</strong> of hosting workloads in the cloud&mldr;</p><h2 id=strategy---weighing-the-options>Strategy - Weighing the Options</h2><h3 id=option-1---hairpin>Option 1 - Hairpin</h3><p>In my case, I have a production data center already built out and running in <strong>New York</strong>. I could leverage the existing Headquarters data center to provision new circuits to each Cloud Provider and integrate them into the existing <strong>WAN</strong>. Utilize <strong>BGP</strong> routing to bounce traffic back and forth as needed.</p><p>Like So:</p><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/g.png width=1069 height=1088 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/g_hu36a76c68b8d930323f835840566196d1_18438_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/g_hu36a76c68b8d930323f835840566196d1_18438_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=98 data-flex-basis=235px></p><h4 id=pros>Pros</h4><ul><li>Relatively <strong>short lead time</strong> to complete this option.</li><li>The <strong>network</strong> already exists (minus the new circuits)</li></ul><h4 id=cons>Cons</h4><ul><li><strong>Hair-pinning</strong> traffic from US-East1 (Virginia) to NY back to Virginia</li><li>May require hardware refresh depending on <strong>speeds/feeds</strong> available of current switches in my data center. Additional lead-time to buy new gear and set it up.</li><li>Introduces on-prem <strong>SLA factors</strong> (network uptime, upgrades, power work, maintenance windows etc) to <strong>cloud-to-cloud</strong> apps.</li></ul><h3 id=option-2---greenfield>Option 2 - Greenfield</h3><p>Rent out space at two co-location providers near Ashburn. Purchase new layer3 switches/routers and order new circuits for the connections.
i.e. <strong>Build it all yourself</strong></p><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/c.png width=1614 height=1251 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/c_hu258ccb9f6138a2ce8149908049aec0fb_30412_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/c_hu258ccb9f6138a2ce8149908049aec0fb_30412_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=129 data-flex-basis=309px></p><h4 id=pros-1>Pros</h4><ul><li>As a network engineer who loves new toys this would have been great, a <strong>new greenfield deployment</strong></li><li>Full <strong>control</strong> of the environment</li></ul><h4 id=cons-1>Cons</h4><ul><li>New Colo Vendor research, approval and onboarding <strong>process</strong> would take some time</li><li>Upfront <strong>capital</strong> costs</li><li>Vendor lead-time on new switches was still high at the time of this design | 1 year+ (post-covid <strong>supply chain issues</strong>)</li></ul><h3 id=option-3---cross-cloud>Option 3 - Cross Cloud</h3><p>Utilize the new <strong>cross cloud</strong> interconnect services provided by GCP</p><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/d.png width=1614 height=563 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/d_hu7d3b1ab6d507c9a77969a8836705d17c_11133_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/d_hu7d3b1ab6d507c9a77969a8836705d17c_11133_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=286 data-flex-basis=688px></p><h4 id=pros-2>Pros</h4><ul><li>Cloud <strong>Native</strong> Solution</li><li>Simple to setup and <strong>quickly</strong> implement</li></ul><h4 id=cons-2>Cons</h4><ul><li>Good for GCP to a CSP but doesn&rsquo;t help with other <strong>traffic patterns</strong> for different cloud providers or on-prem workloads</li><li>Cost savings is not <strong>maximized</strong></li></ul><h3 id=option-4---partner-interconnect>Option 4 - Partner Interconnect</h3><p>Onboard a <strong>3rd party interconnect</strong> vendor to essentially “rent a router” in Virginia at a monthly cost. Multiple companies exist who provide this <strong>service</strong> such as Megaport, Equinix, PacketFabric and <a class=link href=https://cloud.google.com/network-connectivity/docs/interconnect/concepts/service-providers target=_blank rel=noopener>others</a>.</p><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/h.png width=1575 height=1168 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/h_hu0caa2ee5970f394305fe071ef88a79de_28673_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/h_hu0caa2ee5970f394305fe071ef88a79de_28673_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=134 data-flex-basis=323px></p><h4 id=pros-3>Pros</h4><ul><li><strong>Flexibility</strong> in choosing regions or zones to deploy partner interconnect routers as needed</li><li>Ephemeral solution that could be <strong>scaled up/down</strong> quickly</li><li>Easy to compare monthly costs vs <strong>monthly savings</strong></li><li>Follows the overall <strong>cloud-first</strong> mindset of my technology organization</li></ul><h4 id=cons-3>Cons</h4><ul><li><strong>New vendor</strong> research, onboarding and learning</li><li>Losing some <strong>control</strong> of the network to another party - i.e nerd knobs, visibility</li><li>Introducing a new vendor into your critical path of traffic - high availability <strong>system design</strong> is crucial</li></ul><h2 id=design-decisions>Design Decisions</h2><p>We decided on <strong>Option 4</strong>. Using a 3rd party interconnect provider would give us the most flexibility and allow us the option to <strong>dynamically</strong> spin up and down resources/circuits as needed.
After a few weeks of going back and forth with a couple of <strong>providers</strong>, we chose one of them based on community feedback, price and availability of resources.</p><p>Additionally, the architecture should:</p><ul><li>Follow the concept of <strong>least privileged access</strong> - meaning don&rsquo;t open up the routing for <strong>ALL</strong> cloud teams to be able to talk to <strong>ALL</strong> other cloud accounts in perpetuity. Narrow the scope down for specific <strong>account-to-account</strong> workloads.</li><li>Implement <strong>99.99% availability</strong> for each cloud provider interconnect (following each CSP published best practice guides).</li><li>Use <strong>devops principles</strong> to spin up resources as code via Terraform, Ansible, Python APIs etc with well defined pipelines. Make the work visible for the entire tech organization and limit institutional knowledge.</li></ul><h3 id=gcp-architecture>GCP Architecture</h3><ul><li>Create a <strong>hub and spoke</strong> type model</li><li>Centralized <strong>networking hub</strong> project to host the GCP Routers + interconnects to partner routers</li><li>Spoke projects will <strong>VPC peer</strong> to hub project for access as needed</li><li>Following the <a class=link href=https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/partner-creating-9999-availability target=_blank rel=noopener>GCP Best Practices guide</a>, we can design an architecture as shown below:</li></ul><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/j.png width=5663 height=2588 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/j_hubf78e894bd9e11b886ecbab040456921_276547_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/j_hubf78e894bd9e11b886ecbab040456921_276547_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=218 data-flex-basis=525px></p><h3 id=aws-architecture>AWS Architecture</h3><ul><li>Create a centralized <strong>networking account</strong> to host an AWS Transit Gateway which attaches to other spoke accounts.</li><li>Following the <a class=link href=https://docs.aws.amazon.com/directconnect/latest/UserGuide/high_resiliency.html target=_blank rel=noopener>AWS High Resiliency Guide</a>, it should look something like this:</li></ul><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/k.png width=4313 height=2550 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/k_hu9f17a32d6847471562998032093e4eba_249672_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/k_hu9f17a32d6847471562998032093e4eba_249672_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=169 data-flex-basis=405px></p><h3 id=azure-architecture>Azure Architecture</h3><ul><li>Follow Azure Hub-Spoke network topology for peering <a class=link href="https://learn.microsoft.com/en-us/azure/architecture/networking/architecture/hub-spoke?tabs=cli" target=_blank rel=noopener>VNETs together</a></li><li>Design <a class=link href=https://learn.microsoft.com/en-us/azure/expressroute/designing-for-high-availability-with-expressroute target=_blank rel=noopener>high availability express route</a></li></ul><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/l.png width=5599 height=1800 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/l_huad54ff3ce60de5f1bf68ef92f33664b8_193677_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/l_huad54ff3ce60de5f1bf68ef92f33664b8_193677_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=311 data-flex-basis=746px></p><h3 id=partner-router-architecture>Partner Router Architecture</h3><ul><li>Following high availability patterns, create <strong>two virtual routers</strong> in two different availability zones in Virginia.</li><li>Create <strong>virtual cross connects</strong> for each desired CSP path</li><li>Provision new on-prem <strong>circuits</strong> (physical paths) to partner locations</li></ul><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/m.png width=2025 height=2550 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/m_huceba59e1d29dcc0287631f91e843cc49_59375_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/m_huceba59e1d29dcc0287631f91e843cc49_59375_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=79 data-flex-basis=190px></p><h2 id=implementation>Implementation</h2><h3 id=challenges>Challenges</h3><p>As with most projects, you can <strong>plan</strong> and design all day long. But once you start <strong>building</strong>, something unexpected always comes up. We ran into some <strong>challenges</strong> along the way but were able to find solutions and push through. Here&rsquo;s some key ones&mldr;</p><h4 id=gcp-network-peering>GCP Network Peering</h4><p>A <a class=link href=https://cloud.google.com/vpc/docs/vpc-peering target=_blank rel=noopener>key requirement</a> for GCP Network Peering is that IP network space cannot overlap. When I originally audited the candidate GCP accounts to peer, I only looked at the <strong>primary ranges</strong> - I took note that there were no collisions and moved ahead.</p><p>However, when it came time to actually peer the networks we immediately found a problem. There were some <strong>secondary ranges</strong> which shared similar space in the <code>100.X.X.X</code> space. These secondary ranges are used by the project&rsquo;s <strong>kubernetes</strong> clusters.</p><p>Some possible solutions to fix this:</p><ol><li>Re-IP these secondary ranges - wasn&rsquo;t a fan of this option as it would cause more work and intrusion on the <strong>application owners</strong> side.</li><li><strong>Implement</strong> Google&rsquo;s New Product offering called <a class=link href="https://cloud.google.com/network-connectivity-center?hl=en" target=_blank rel=noopener>Network Connectivity Center</a> (it was in Beta at the time)<ul><li>The key feature that could help here was the ability to <strong>prefix filter</strong> routes from peering</li><li>Unfortunately, even with NCC enabled we quickly ran into <a class=link href=https://cloud.google.com/network-connectivity/docs/network-connectivity-center/concepts/vpc-spokes-overview target=_blank rel=noopener>another blocker</a> <code>IPv4 static and dynamic routes exchange across VPC spokes are not supported.</code></li><li>This meant we could not <strong>exchange</strong> dynamically learned routes from the partner interconnect to the spoke accounts. This was a no-go.</li></ul></li></ol><p>With NCC Peering off the table, we went back to VPC Peering. But this time the decision was to select a few <strong>high value</strong> GCP Projects that made up the majority of the traffic load. Re-IP those secondary kubernetes <strong>ranges</strong> (if needed) and move on.</p><p>Another <strong>issue</strong> that arose: Google Cloud Routers connected to the interconnects would not automatically export the <a class=link href=https://cloud.google.com/vpc/docs/vpc-peering#custom-route-exchange target=_blank rel=noopener>learned VPC Peering routes</a>:</p><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/q.png width=1088 height=440 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/q_hu68c57a763b01c83f0d0100bdc61c0c94_157946_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/q_hu68c57a763b01c83f0d0100bdc61c0c94_157946_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=247 data-flex-basis=593px></p><p>To solve this <strong>problem</strong>, we have to install custom advertisements on the google cloud routers (i.e. maintain an <strong>IP Prefix List</strong>). This adds complexity to the project, but could be solved <strong>programmatically</strong> during our pipeline build which will be shared later on in this post.</p><h4 id=aws---account-to-account-routing-options>AWS - Account-to-Account Routing Options</h4><p>The original design assumed to use the existing <strong>Transit Gateway</strong> connections to route the newly introduced traffic to each account. However, after performing a cost reduction <a class=link href=https://calculator.aws/#/ target=_blank rel=noopener>analysis</a> we realized that the Transit Gateway <strong>transfer costs</strong> were still high, making the effort of the entire project less appealing. Another option would be to create new connections with Virtual Private Gateways (<strong>VGW attachments</strong>).</p><p>As an example exercise, if we assume <strong>1000TB</strong> of monthly data transfers:</p><ul><li>Using the <strong>TGW</strong> architecture would save up to <strong>30%</strong> of data transfer costs.</li><li>Using the <strong>VGW</strong> architecture would save up to <strong>60%</strong> of data transfer costs.</li></ul><p>So the tradeoff we made was to select a few <strong>heavy hitter</strong> accounts to peer directly with the hub network account using a <strong>VGW Architecture</strong>.</p><p>To do this, I needed to create dedicated VGWs in each <strong>heavy hitter</strong> account and attach it to a new DXGW instead of using the existing Transit Gateway <strong>architecture</strong>. For the <strong>rest of the accounts</strong> connectivity would still be established via the TGW.</p><p>Additionally, this meant we had to create 2 additional <strong>virtual cross connects</strong> to our partner routers and also pay close attention to the hard limit quotas of <a class=link href=https://docs.aws.amazon.com/directconnect/latest/UserGuide/limits.html target=_blank rel=noopener>AWS Direct Connect</a>. Although the cost of additional cross connects doubles our intended budget, it still made sense</p><p>The new AWS <strong>Architecture</strong> would be as follows:</p><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/n.png width=4388 height=2588 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/n_hua852a7e80331dd5f0bf274031099b9d0_316918_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/n_hua852a7e80331dd5f0bf274031099b9d0_316918_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=169 data-flex-basis=406px></p><h4 id=azure-quirks>Azure Quirks</h4><p>Prior to this project, I had almost no <strong>experience</strong> building anything in Azure. The interface felt foreign to me and the <strong>naming conventions and iconography</strong> also took some time to get used to&mldr;but in general, there were no major hangups in <strong>Azure</strong> except for one issue:</p><ul><li>Prior to this work, an Azure <strong>site-to-site VPN</strong> had already been setup in one VNET to an on-prem resource.</li><li>Due to this, I was unable to peer this azure VNET to the new <strong>hub vnet</strong></li><li>To work around this problem, we had to relocate the <strong>site-to-site VPN</strong> to the hub vnet and pay close attention to the routing. For this specific case I used <strong>less specific routes</strong> to <a class=link href=https://learn.microsoft.com/en-us/azure/expressroute/designing-for-disaster-recovery-with-expressroute-privatepeering target=_blank rel=noopener>prefer</a> the express route connection over the existing site-to-site VPN</li></ul><h4 id=differences-of-networking-concepts-across-csps>Differences of Networking Concepts across CSPs</h4><p>In general understanding the <strong>different concepts</strong> of VPCs vs VNETs - Projects vs Accounts vs Subscriptions - Global route tables vs regional route tables etc&mldr; could be a <strong>whole book</strong> (that I would not be qualified to write). One example that comes to mind which took me by surprise:</p><ul><li>In GCP - US <strong>East1</strong> is in South Carolina but in AWS US East1 is in <strong>Virginia</strong>. Something to keep in mind when thinking about traffic latency and regional disaster recovery scenarios.</li></ul><h3 id=high-availability>High Availability</h3><p>As I mentioned previously in the <strong>design</strong> section, implementing a highly available solution is crucial. We don&rsquo;t want to introduce additional <strong>failure events</strong> that do not typically occurr in cloud deployments (i.e. limit the blame on the network :D )</p><p>To Summarize the best practice documentation from each provider environment:</p><ul><li><p>In <a class=link href=https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/partner-creating-9999-availability target=_blank rel=noopener>GCP</a>, high availability requires <strong>4</strong> partner interconnects across <strong>2</strong> Google Cloud Routers in <strong>2</strong> different regions</p></li><li><p>In <a class=link href=https://docs.aws.amazon.com/directconnect/latest/UserGuide/high_resiliency.html target=_blank rel=noopener>AWS</a>, high resiliency can be achieved with <strong>2</strong> single connections in <strong>2</strong> different direct connect locations</p></li><li><p><a class=link href=https://learn.microsoft.com/en-us/azure/expressroute/designing-for-high-availability-with-expressroute target=_blank rel=noopener>Azure ExpressRoutes</a> requires <strong>2</strong> express route circuits in zone redundant virtual gateways</p></li><li><p>3rd Party Interconnect Routers require <strong>2</strong> virtual routers and the virtual cross connects should be in distinct <strong>A/B Availability Zones</strong></p></li></ul><h3 id=routing-decisions>Routing Decisions</h3><p>This is where <strong>network engineering</strong> chops matter. The standard routing protocol for all Cloud Service Providers is <a class=link href=https://datatracker.ietf.org/doc/html/rfc4271 target=_blank rel=noopener>Border Gateway Protocol - BGP</a></p><p>What are some BGP decisions we have to make to design a <strong>reliable and fast network</strong>?</p><ol><li><p>Use <a class=link href=https://datatracker.ietf.org/doc/html/rfc5880 target=_blank rel=noopener>Bidirectional Forwarding Detection - BFD</a> for fast BGP <strong>Failover</strong></p><ul><li>Keep in mind that different CSPs may have different <strong>BFD supported values</strong>, for example:<ul><li>In GCP we <strong>must</strong> use the values:<div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Transmit Interval - 1000 ms
</span></span><span class=line><span class=cl>Receive Interval  - 1000 ms
</span></span><span class=line><span class=cl>Multiplier        - 5
</span></span></code></pre></div></li><li>However, AWS supports faster values:<div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Transmit Interval - 300 ms
</span></span><span class=line><span class=cl>Receive Interval  - 300 ms
</span></span><span class=line><span class=cl>Multiplier        - 3
</span></span></code></pre></div></li></ul></li><li>When I performed failover testing of these circuits, it took almost <strong>5 seconds</strong> for the GCP traffic to recover as opposed to the AWS failure took under <strong>1 second</strong> to recover.</li></ul></li><li><p>Understand Route Manipulations and Priorities in Different Cloud <strong>Environments</strong></p><ul><li>For example, in AWS its best practice to <strong>influence</strong> traffic using <a class=link href=https://aws.amazon.com/blogs/networking-and-content-delivery/influencing-traffic-over-hybrid-networks-using-longest-prefix-match/ target=_blank rel=noopener>longest prefix match</a></li><li>We can also influence routing <strong>policies</strong> with <a class=link href=https://docs.aws.amazon.com/directconnect/latest/UserGuide/routing-and-bgp.html target=_blank rel=noopener>BGP Communities</a></li><li>Additionally, using AS PATH Prepending or MED values is another <strong>option</strong></li><li>Also keep in mind, Route Priority of <a class=link href=https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html#route-table-priority-propagated-routes target=_blank rel=noopener>Propagated Routes</a></li></ul></li><li><p>Should we use Equal Cost Multipathing (<strong>i.e. ECMP Load Sharing</strong>) across multiple links?</p></li></ol><ul><li><p>Possible <strong>negatives</strong> of ECMP:</p><ul><li><strong>Non-deterministic</strong> route paths - hard to &ldquo;know&rdquo; which way your traffic is flowing at all times.</li><li><strong>Gray outages</strong> where one path is not working optimally and causes intermittent issues making it hard to troubleshoot.</li><li>If a <strong>stateful firewall</strong> is in line inspecting traffic and return routes take a different path, it may get dropped</li><li>Load balancing across different <strong>geographical locations</strong> may cause latency variations</li></ul></li><li><p><strong>Benefits</strong>:</p><ul><li><strong>Maximize</strong> your provisioned circuits. i.e more available <strong>bandwidth</strong></li><li>More resilient to <strong>failover</strong> - if a link fails all of your traffic does not go down waiting for BFD to kick in.</li></ul></li><li><p>I decided to use ECMP - but I understand why other&rsquo;s may choose not to.</p></li></ul><h3 id=visibility-and-operations>Visibility and Operations</h3><p>In my view, a project is not <strong>done</strong> just because traffic is flowing from point A to point B. I believe it is just as important to devote equal planning, time and energy towards <strong>supporting systems</strong></p><h4 id=observability>Observability</h4><p>How do we <strong>monitor</strong> these new partner virtual routers? The typical network construct of pointing your <strong>SNMP poller</strong> to a router and graphing bandwidth does not apply here.</p><ul><li>Answer: Use <strong>APIs</strong> to gather info and relay it back to a centralized observability platform. In my case, our tech org has standardized on a single <strong>SaaS observability platform</strong> so I wrote some glue scripts to do this. I will share below.</li></ul><p><a class=link href=https://github.com/kaon1/python-misc/blob/master/observability-metrics/megaport-status-checks-for-resources.py target=_blank rel=noopener>Python Script to Send Metrics</a></p><p>The <strong>script</strong> does the following:</p><ol><li>Gathers a <strong>status check</strong> of the Virtual Routers and Cross Connects</li><li>Sends the status to the observability platform <strong>periodically</strong></li><li>In the platform we create <strong>dashboards and monitors</strong> to alert on interesting values or missing data (i.e a resource is no longer reporting)</li></ol><ul><li>Another script <a class=link href=https://github.com/kaon1/python-misc/blob/master/observability-metrics/megaport-mcr-bw-to-dd.py target=_blank rel=noopener>here polls bandwidth usage every 5 minutes</a> and sends the metric up to the observability platform for <strong>graphing</strong>.</li></ul><p>All of this info <strong>could</strong> be gathered directly from the Cloud Provider&rsquo;s portal - an operator would just need to login and click around to find it. But, in keeping with the principles listed above of <strong>limiting institutional knowledge</strong> its important to make this data available and visible to all teams within the organization.</p><h4 id=testing>Testing</h4><p>It is important to establish baseline tests of &ldquo;what the network <strong>should</strong> look like at any given time&rdquo; - so that if something goes wrong we have <strong>historical data</strong> to refer back to.</p><p>We should:</p><ul><li>Use <strong>synthetic testing</strong> to measure ICMP latency, HTTP Response Time, Packet Loss and Hop Count between two endpoints on each end of the <strong>dedicated private links</strong></li><li>Generate traffic using <strong>iperf</strong> across the links and <strong>measure performance</strong></li><li>There&rsquo;s a handy <a class=link href=https://iperf3-python.readthedocs.io/en/latest/ target=_blank rel=noopener>iperf3 library</a> which can be used to script these tests and send the results up to your <strong>observability platform</strong>.</li></ul><p>Here&rsquo;s an <a class=link href=https://github.com/kaon1/python-misc/blob/master/observability-metrics/iperf-megaport-dd.py target=_blank rel=noopener>example script</a> which takes in some <strong>user input</strong> (like iperf destination, number of streams to send, duration of test and bandwidth) and sends the metrics up for graphing.</p><p>An example way to run this script would be with a 5 minute <strong>cronjob</strong> from a server in each Cloud Environment.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=o>*/</span><span class=mi>5</span> <span class=o>*</span> <span class=o>*</span> <span class=o>*</span> <span class=o>*</span> <span class=n>root</span> <span class=n>python3</span> <span class=n>iperf</span><span class=o>-</span><span class=n>dd</span><span class=o>.</span><span class=n>py</span> <span class=o>--</span><span class=n>direction</span> <span class=n>upload</span> <span class=o>--</span><span class=n>dest_name</span> <span class=n>gcpuseast1</span> <span class=o>--</span><span class=n>dest_ip</span> <span class=mf>10.</span><span class=n>x</span><span class=o>.</span><span class=n>x</span><span class=o>.</span><span class=n>x</span> <span class=o>--</span><span class=n>dest_port</span> <span class=mi>5201</span> <span class=o>--</span><span class=n>numofstreams</span> <span class=mi>1</span> <span class=o>--</span><span class=n>duration</span> <span class=mi>30</span> <span class=o>--</span><span class=n>bandwidth</span> <span class=mi>1000000000</span>
</span></span><span class=line><span class=cl><span class=o>*/</span><span class=mi>5</span> <span class=o>*</span> <span class=o>*</span> <span class=o>*</span> <span class=o>*</span> <span class=n>root</span> <span class=n>python3</span> <span class=n>iperf</span><span class=o>-</span><span class=n>dd</span><span class=o>.</span><span class=n>py</span> <span class=o>--</span><span class=n>direction</span> <span class=n>upload</span> <span class=o>--</span><span class=n>dest_name</span> <span class=n>gcpuscentral1</span> <span class=o>--</span><span class=n>dest_ip</span> <span class=mf>10.</span><span class=n>x</span><span class=o>.</span><span class=n>x</span><span class=o>.</span><span class=n>x</span> <span class=o>--</span><span class=n>dest_port</span> <span class=mi>5203</span> <span class=o>--</span><span class=n>numofstreams</span> <span class=mi>1</span> <span class=o>--</span><span class=n>duration</span> <span class=mi>30</span> <span class=o>--</span><span class=n>bandwidth</span> <span class=mi>1000000000</span>
</span></span></code></pre></div><h4 id=pipelines-for-deploying-changes>Pipelines for Deploying Changes</h4><p>As I mentioned in the <strong>design</strong> portion of this post, it is important to use devops principles wherever possible for provisioning and changing these resources. I have multiple blog posts about creating <strong>terraform pipelines</strong>, so I won&rsquo;t go into detail here. But the general idea is this:</p><ol><li>Create a <strong>shared repository</strong> for these new cloud resources - give access to other teams to suggest changes (via PRs)</li><li>Build <strong>pipelines</strong> that perform dry runs or plans of changes</li><li>Execute new changes on <strong>merges to main</strong></li></ol><p>Here is an <a class=link href=https://github.com/kaon1/python-misc/blob/master/scripts/aws-vgw-to-dxgw-example.tf target=_blank rel=noopener>example terraform file</a> that can be installed on a per account basis to create the VGWs and also attach them to the DXGW.</p><p><strong>Why</strong> is this important?</p><ul><li>Provides <strong>visibility</strong> to app teams of the network connectivity (its no longer just a black box)</li><li>Tracks changes, self <strong>documents</strong> the network</li><li>Makes it easier to maintain complex, repeatable objects such as <strong>IP Prefix Lists</strong></li></ul><p>As mentioned above, we have to use IP Prefix lists on the GCRs, but Prefix Lists also give you more <strong>control</strong> of the routing and allows the ability to enforce the concept of <strong>least access</strong> (explicit permit). But these prefix lists can get <strong>complex</strong> with multiple virtual routers and cross connects. One way to keep these in <strong>sync</strong> is with a a <a class=link href=https://github.com/kaon1/python-misc/blob/master/scripts/prefix-sync-megaport.py target=_blank rel=noopener>pipeline script</a>.</p><p>The script eases the onboarding process of a new <strong>workload/account</strong>. If a new workload is onboarding to this <strong>architecture</strong>, we update the shared repository and add the <strong>new prefixes</strong>. The <strong>pipeline</strong> runs and updates the necessary prefix lists to allow communication.</p><h2 id=overall-wins>Overall Wins</h2><h3 id=lowering-monthly-cloud-traffic-costs>Lowering Monthly Cloud Traffic Costs</h3><ul><li>It takes a <strong>PHD</strong> to understand the complexity of cloud costs - and I do not have one. But I have been told its <strong>working</strong>&mldr;so that&rsquo;s good enough for me :D</li></ul><h3 id=improved-security-posture-and-private-connectivity>Improved Security Posture and Private Connectivity</h3><ul><li>Able to reduce the amount of <strong>public endpoints</strong> which do not need to be exposed</li><li>More <strong>control</strong> of cloud account routing</li></ul><h3 id=better-network-performance>Better Network Performance</h3><ul><li><strong>ICMP</strong> Latency Improvement: Public ~21 ms | <strong>Private</strong> ~13 ms</li></ul><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/o.png width=1635 height=551 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/o_huff130c524d64d0dd95b135e7047060b5_78413_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/o_huff130c524d64d0dd95b135e7047060b5_78413_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=296 data-flex-basis=712px></p><ul><li><strong>HTTP</strong> Latency Improvement: Public ~55 ms | <strong>Private</strong> ~ 45ms</li></ul><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/p.png width=1643 height=560 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/p_hu0e319c65338e1299109bb5f64accaae3_113857_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/p_hu0e319c65338e1299109bb5f64accaae3_113857_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=293 data-flex-basis=704px></p><h3 id=team-collaboration-across-different-missions>Team Collaboration Across Different Missions</h3><p>Not long ago, I traveled to Las Vegas for <strong>AWS Re:Invent</strong>. There I ran into some of my fellow work colleagues and talked shop we hung out for multiple days and got to know each other - it was the first time I ever interacted with them&mldr; <strong>DESPITE WORKING ON THE SAME FLOOR</strong>.</p><p>That being said, projects like this are a great way to break down those corporate silos and build up some <strong>cross team collaboration</strong>.</p><h2 id=end>End</h2><p>A final cohesive <strong>architecture</strong> example that we can use looks like this:</p><p><img src=/p/a-case-study-in-hybrid-cloud-network-design/images/r.png width=6563 height=4988 srcset="/p/a-case-study-in-hybrid-cloud-network-design/images/r_hu7085ff46d7f6515504a927472a639a57_223557_480x0_resize_box_3.png 480w, /p/a-case-study-in-hybrid-cloud-network-design/images/r_hu7085ff46d7f6515504a927472a639a57_223557_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=131 data-flex-basis=315px></p></section><footer class=article-footer></footer></article><aside class=related-contents--wrapper><h2 class=section-title>Related contents</h2><div class=related-contents><div class="flex article-list--tile"><article class=has-image><a href=/p/bgpalerter-as-code-using-a-terraform-pipeline/><div class=article-image><img src=/p/bgpalerter-as-code-using-a-terraform-pipeline/front.fbc61346389f4ec97dce763988d6ce13_huaf53bc8ed3209b5b56820218227b86f5_395056_250x150_fill_box_smart1_3.png width=250 height=150 loading=lazy alt="Featured image of post BGPAlerter As Code Using A Terraform Pipeline" data-hash="md5-+8YTRjifTsl9znY5iNbOEw=="></div><div class=article-details><h2 class=article-title>BGPAlerter As Code Using A Terraform Pipeline</h2></div></a></article><article class=has-image><a href=/p/slack-ipam-helper/><div class=article-image><img src=/p/slack-ipam-helper/images/ipam-helper-front.059c631bacc91c912d4cb0a7cbb4037f_hu0b4c396984346b48c89ef05d3f4ab66d_78125_250x150_fill_box_smart1_3.png width=250 height=150 loading=lazy alt="Featured image of post Slack IPAM Helper" data-hash="md5-BZxjG6zJHJEtTLCny7QDfw=="></div><div class=article-details><h2 class=article-title>Slack IPAM Helper</h2></div></a></article><article class=has-image><a href=/p/ansible-configuration-backup-manager/><div class=article-image><img src=/p/ansible-configuration-backup-manager/_hu12225e3f406182c91634608ad7cb2c64_1239508_1af7a87b45fb3b73a9c1c716fe9eb0f9.png width=250 height=150 loading=lazy alt="Featured image of post Ansible Configuration Backup Manager" data-hash="md5-+TQ8PJ8mkUygm6V0xYsxfQ=="></div><div class=article-details><h2 class=article-title>Ansible Configuration Backup Manager</h2></div></a></article><article class=has-image><a href=/p/building-a-datacenter-as-code-with-arista-cloudvision/><div class=article-image><img src=/p/building-a-datacenter-as-code-with-arista-cloudvision/images/dcac-front.e17c503945054eca9922bcf6fd46b4e7_hu9ac709360588aaaee1a03ffb9aefd712_150968_250x150_fill_box_smart1_3.png width=250 height=150 loading=lazy alt="Featured image of post Building a DataCenter As Code with Arista CloudVision" data-hash="md5-4XxQOUUFTsqZIrz2/Ua05w=="></div><div class=article-details><h2 class=article-title>Building a DataCenter As Code with Arista CloudVision</h2></div></a></article><article class=has-image><a href=/p/self-service-containerlab-deployment-with-ansible-tower/><div class=article-image><img src=/p/self-service-containerlab-deployment-with-ansible-tower/images/clab-ansible.ba505db6de1ab9014b43ae3976868a7e_hu900e690d7d00d8033c3afbeaf3ab65f6_51076_250x150_fill_box_smart1_3.png width=250 height=150 loading=lazy alt="Featured image of post Self-Service ContainerLab Deployment with Ansible Tower" data-hash="md5-ulBdtt4auQFLQ645doaKfg=="></div><div class=article-details><h2 class=article-title>Self-Service ContainerLab Deployment with Ansible Tower</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kaonbytes-com.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{DISQUS&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2022 -
2024 KaonBytes</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.11.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#about>About</a></li><li><a href=#problem-statement>Problem Statement</a></li><li><a href=#strategy---weighing-the-options>Strategy - Weighing the Options</a><ol><li><a href=#option-1---hairpin>Option 1 - Hairpin</a><ol><li><a href=#pros>Pros</a></li><li><a href=#cons>Cons</a></li></ol></li><li><a href=#option-2---greenfield>Option 2 - Greenfield</a><ol><li><a href=#pros-1>Pros</a></li><li><a href=#cons-1>Cons</a></li></ol></li><li><a href=#option-3---cross-cloud>Option 3 - Cross Cloud</a><ol><li><a href=#pros-2>Pros</a></li><li><a href=#cons-2>Cons</a></li></ol></li><li><a href=#option-4---partner-interconnect>Option 4 - Partner Interconnect</a><ol><li><a href=#pros-3>Pros</a></li><li><a href=#cons-3>Cons</a></li></ol></li></ol></li><li><a href=#design-decisions>Design Decisions</a><ol><li><a href=#gcp-architecture>GCP Architecture</a></li><li><a href=#aws-architecture>AWS Architecture</a></li><li><a href=#azure-architecture>Azure Architecture</a></li><li><a href=#partner-router-architecture>Partner Router Architecture</a></li></ol></li><li><a href=#implementation>Implementation</a><ol><li><a href=#challenges>Challenges</a><ol><li><a href=#gcp-network-peering>GCP Network Peering</a></li><li><a href=#aws---account-to-account-routing-options>AWS - Account-to-Account Routing Options</a></li><li><a href=#azure-quirks>Azure Quirks</a></li><li><a href=#differences-of-networking-concepts-across-csps>Differences of Networking Concepts across CSPs</a></li></ol></li><li><a href=#high-availability>High Availability</a></li><li><a href=#routing-decisions>Routing Decisions</a></li><li><a href=#visibility-and-operations>Visibility and Operations</a><ol><li><a href=#observability>Observability</a></li><li><a href=#testing>Testing</a></li><li><a href=#pipelines-for-deploying-changes>Pipelines for Deploying Changes</a></li></ol></li></ol></li><li><a href=#overall-wins>Overall Wins</a><ol><li><a href=#lowering-monthly-cloud-traffic-costs>Lowering Monthly Cloud Traffic Costs</a></li><li><a href=#improved-security-posture-and-private-connectivity>Improved Security Posture and Private Connectivity</a></li><li><a href=#better-network-performance>Better Network Performance</a></li><li><a href=#team-collaboration-across-different-missions>Team Collaboration Across Different Missions</a></li></ol></li><li><a href=#end>End</a></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>